{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9884dd7e",
        "outputId": "002281f8-676b-4483-c812-81aecd0c84e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "\n",
        "!pip install pypdfium2 PyMuPDF pandas tqdm requests"
      ],
      "metadata": {
        "id": "CMXxGY4ijF_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION\n",
        "\n",
        "PDF_FOLDER = \"/content/drive/MyDrive/CapstoneProject/Capstone/papers\"\n",
        "TEXT_OUTPUT = \"/content/drive/MyDrive/CapstoneProject/Capstone/text_extraction\"\n",
        "METADATA_CSV = \"/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new.csv\"\n",
        "\n",
        "# Source files for category assignment\n",
        "LIVER_SOURCE = \"/content/drive/MyDrive/CapstoneProject/Capstone/download_log_liver_transplant.csv\"\n",
        "LUNG_SOURCE = \"/content/drive/MyDrive/CapstoneProject/Capstone/download_log_lung_transplant.csv\"\n",
        "HEART_SOURCE = \"/content/drive/MyDrive/CapstoneProject/Capstone/download_log_heart_transplant.csv\"\n",
        "KIDNEY_SOURCE = \"/content/drive/MyDrive/CapstoneProject/Capstone/download_log_kidney_transplant.csv\"\n",
        "\n",
        "# Your email for API requests\n",
        "YOUR_EMAIL = \"tn2463@nyu.edu\""
      ],
      "metadata": {
        "id": "S6iPErJtjK9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "Path(TEXT_OUTPUT).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output folder ready: {TEXT_OUTPUT}\")"
      ],
      "metadata": {
        "id": "PNIgqp6cjcD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pypdfium2 as pdfium\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "all_pdfs = list(Path(PDF_FOLDER).glob('**/*.pdf'))\n",
        "\n",
        "print(\"TEXT EXTRACTION FROM PDFs\")\n",
        "print(f\"Total PDFs: {len(all_pdfs)}\\n\")\n",
        "\n",
        "def extract_doi_from_filename(filename):\n",
        "    name = filename.replace('.pdf', '')\n",
        "    doi = name.replace('_', '/')\n",
        "    return doi\n",
        "\n",
        "def extract_title_from_text(text):\n",
        "    lines = text.strip().split('\\n')\n",
        "    for line in lines[:10]:\n",
        "        line = line.strip()\n",
        "        if len(line) > 20 and len(line) < 200:\n",
        "            return line\n",
        "    return \"Unknown\"\n",
        "\n",
        "metadata_records = []\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, pdf_path in enumerate(tqdm(all_pdfs, desc=\"Extracting\", unit=\"pdf\")):\n",
        "    doi = extract_doi_from_filename(pdf_path.name)\n",
        "    txt_file = Path(TEXT_OUTPUT) / f\"{doi.replace('/', '_')}.txt\"\n",
        "\n",
        "    record = {\n",
        "        'pdf_title': 'Unknown',\n",
        "        'doi': doi,\n",
        "        'file_size_mb': round(pdf_path.stat().st_size / (1024 * 1024), 2),\n",
        "        'text_length': 0,\n",
        "        'is_scanned': False,\n",
        "        'needs_ocr': False,\n",
        "        'extraction_method': 'none',\n",
        "        'year': None,\n",
        "        'citation_count': None,\n",
        "        'publication': None,\n",
        "        'category': 'other'\n",
        "    }\n",
        "\n",
        "    if txt_file.exists():\n",
        "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        record['text_length'] = len(text.strip())\n",
        "        record['pdf_title'] = extract_title_from_text(text)\n",
        "        if record['text_length'] < 500:\n",
        "            record['is_scanned'] = True\n",
        "            record['needs_ocr'] = True\n",
        "            record['extraction_method'] = 'ocr'\n",
        "        else:\n",
        "            record['extraction_method'] = 'text_extraction'\n",
        "    else:\n",
        "        try:\n",
        "            pdf = pdfium.PdfDocument(pdf_path)\n",
        "            text = \"\"\n",
        "            for i in range(len(pdf)):\n",
        "                page = pdf[i]\n",
        "                textpage = page.get_textpage()\n",
        "                page_text = textpage.get_text_range()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\\n\"\n",
        "            pdf.close()\n",
        "\n",
        "            with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(text)\n",
        "\n",
        "            record['text_length'] = len(text.strip())\n",
        "            record['pdf_title'] = extract_title_from_text(text)\n",
        "\n",
        "            if record['text_length'] < 500:\n",
        "                record['is_scanned'] = True\n",
        "                record['needs_ocr'] = True\n",
        "                record['extraction_method'] = 'ocr'\n",
        "            else:\n",
        "                record['extraction_method'] = 'text_extraction'\n",
        "\n",
        "        except Exception as e:\n",
        "            record['extraction_method'] = 'failed'\n",
        "            record['needs_ocr'] = True\n",
        "            record['is_scanned'] = True\n",
        "\n",
        "    metadata_records.append(record)\n",
        "\n",
        "metadata_df = pd.DataFrame(metadata_records)\n",
        "metadata_df.to_csv(METADATA_CSV, index=False)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "needs_ocr_count = len(metadata_df[metadata_df['needs_ocr'] == True])\n",
        "\n",
        "print(\"EXTRACTION COMPLETE\")\n",
        "print(f\"Total PDFs: {len(metadata_df)}\")\n",
        "print(f\"Need OCR: {needs_ocr_count}\")\n",
        "print(f\"Time: {elapsed_time/60:.1f} minutes\")\n",
        "print(f\"\\nExtraction method distribution:\")\n",
        "print(metadata_df['extraction_method'].value_counts())"
      ],
      "metadata": {
        "id": "QpE2lG6gjcHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RE-EXTRACT FAILED PDFS WITH PYMUPDF\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "df = pd.read_csv(METADATA_CSV)\n",
        "missing_df = df[df['extraction_method'] == 'failed'].copy()\n",
        "\n",
        "print(\"RE-EXTRACTING FAILED PDFs WITH PyMuPDF\")\n",
        "print(f\"Failed PDFs to re-extract: {len(missing_df)}\\n\")\n",
        "\n",
        "success = 0\n",
        "failed = 0\n",
        "\n",
        "for idx, row in tqdm(missing_df.iterrows(), total=len(missing_df), desc=\"Extracting\"):\n",
        "    doi = row['doi']\n",
        "    pdf_filename = doi.replace('/', '_') + '.pdf'\n",
        "    txt_file = Path(TEXT_OUTPUT) / f\"{doi.replace('/', '_')}.txt\"\n",
        "    pdf_path = Path(PDF_FOLDER) / pdf_filename\n",
        "\n",
        "    if not pdf_path.exists():\n",
        "        failed += 1\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(str(pdf_path))\n",
        "        text = \"\"\n",
        "        for page_num in range(len(doc)):\n",
        "            try:\n",
        "                page = doc[page_num]\n",
        "                page_text = page.get_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\\n\"\n",
        "            except:\n",
        "                continue\n",
        "        doc.close()\n",
        "\n",
        "        with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "\n",
        "        text_length = len(text.strip())\n",
        "        df.at[idx, 'text_length'] = text_length\n",
        "        df.at[idx, 'pdf_title'] = extract_title_from_text(text)\n",
        "\n",
        "        if text_length < 500:\n",
        "            df.at[idx, 'extraction_method'] = 'ocr'\n",
        "            df.at[idx, 'needs_ocr'] = True\n",
        "            df.at[idx, 'is_scanned'] = True\n",
        "        else:\n",
        "            df.at[idx, 'extraction_method'] = 'text_extraction'\n",
        "            df.at[idx, 'needs_ocr'] = False\n",
        "            df.at[idx, 'is_scanned'] = False\n",
        "\n",
        "        success += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        failed += 1\n",
        "\n",
        "df.to_csv(METADATA_CSV, index=False)\n",
        "\n",
        "print(\"RE-EXTRACTION COMPLETE\")\n",
        "print(f\"Successfully extracted: {success}\")\n",
        "print(f\"Still failed: {failed}\")\n",
        "print(f\"\\nExtraction method distribution:\")\n",
        "print(df['extraction_method'].value_counts())"
      ],
      "metadata": {
        "id": "-z280ncIjcKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXTRACT METADATA FROM TEXT FILES\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "print(\"EXTRACTING METADATA FROM TEXT FILES\")\n",
        "\n",
        "df = pd.read_csv(METADATA_CSV)\n",
        "print(f\"Loaded {len(df)} records\\n\")\n",
        "\n",
        "def extract_year_from_text(text, doi):\n",
        "    if not text or len(text) < 50:\n",
        "        return None\n",
        "\n",
        "    header = text[:10000]\n",
        "    year_candidates = []\n",
        "\n",
        "    patterns = [\n",
        "        (r'(?:published|publication\\s+year)[:\\s]+(\\d{4})', 3),\n",
        "        (r'(?:copyright|©|\\(c\\))[:\\s]*(\\d{4})', 3),\n",
        "        (r'(?:received|accepted)[:\\s]+\\w+[,\\s]+(\\d{4})', 2),\n",
        "        (r'\\b(\\d{4})\\s*;\\s*\\d+', 2),\n",
        "    ]\n",
        "\n",
        "    for pattern, weight in patterns:\n",
        "        matches = re.findall(pattern, header, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            try:\n",
        "                year = int(match)\n",
        "                if 1950 <= year <= 2025:\n",
        "                    year_candidates.extend([year] * weight)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    if year_candidates:\n",
        "        return Counter(year_candidates).most_common(1)[0][0]\n",
        "    return None\n",
        "\n",
        "def extract_publication_from_text(text):\n",
        "    if not text or len(text) < 50:\n",
        "        return None\n",
        "\n",
        "    header = text[:5000]\n",
        "    lines = header.split('\\n')\n",
        "\n",
        "    patterns = [\n",
        "        r'nature\\s+(?:medicine|communications?|reviews?)',\n",
        "        r'(?:jama|lancet|bmj|plos|nejm)',\n",
        "        r'journal\\s+of\\s+[\\w\\s&-]{5,50}',\n",
        "        r'(?:annals?|archives?)\\s+of\\s+[\\w\\s&-]{5,50}',\n",
        "    ]\n",
        "\n",
        "    for line in lines[:80]:\n",
        "        line_clean = line.strip()\n",
        "        if 15 < len(line_clean) < 200:\n",
        "            for pattern in patterns:\n",
        "                if re.search(pattern, line_clean, re.IGNORECASE):\n",
        "                    return re.sub(r'\\s+', ' ', line_clean)\n",
        "    return None\n",
        "\n",
        "def extract_citation_from_text(text):\n",
        "    if not text or len(text) < 50:\n",
        "        return None\n",
        "\n",
        "    patterns = [\n",
        "        r'cited\\s+by[:\\s]+(\\d+)',\n",
        "        r'(\\d+)\\s+citations?',\n",
        "        r'times\\s+cited[:\\s]+(\\d+)',\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text[:15000], re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            try:\n",
        "                count = int(match)\n",
        "                if 1 <= count < 100000:\n",
        "                    return count\n",
        "            except:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "updated = {'year': 0, 'citation': 0, 'publication': 0}\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting metadata\"):\n",
        "    doi = row['doi']\n",
        "    txt_file = Path(TEXT_OUTPUT) / f\"{doi.replace('/', '_')}.txt\"\n",
        "\n",
        "    if not txt_file.exists():\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            text = f.read()\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    if len(text) < 50:\n",
        "        continue\n",
        "\n",
        "    if pd.isna(row['year']):\n",
        "        year = extract_year_from_text(text, doi)\n",
        "        if year:\n",
        "            df.at[idx, 'year'] = float(year)\n",
        "            updated['year'] += 1\n",
        "\n",
        "    if pd.isna(row['publication']):\n",
        "        pub = extract_publication_from_text(text)\n",
        "        if pub:\n",
        "            df.at[idx, 'publication'] = pub\n",
        "            updated['publication'] += 1\n",
        "\n",
        "    if pd.isna(row['citation_count']):\n",
        "        cites = extract_citation_from_text(text)\n",
        "        if cites:\n",
        "            df.at[idx, 'citation_count'] = float(cites)\n",
        "            updated['citation'] += 1\n",
        "\n",
        "df.to_csv(METADATA_CSV, index=False)\n",
        "\n",
        "print(\"METADATA EXTRACTION COMPLETE\")\n",
        "print(f\"Updated years: {updated['year']}\")\n",
        "print(f\"Updated publications: {updated['publication']}\")\n",
        "print(f\"Updated citations: {updated['citation']}\")"
      ],
      "metadata": {
        "id": "WeuXArHijrAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FETCH METADATA FROM APIS\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def get_crossref_metadata(doi):\n",
        "    try:\n",
        "        url = f\"https://api.crossref.org/works/{doi}\"\n",
        "        headers = {'User-Agent': f'mailto:{YOUR_EMAIL}'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            message = data.get('message', {})\n",
        "\n",
        "            year = None\n",
        "            published = message.get('published-print') or message.get('published-online')\n",
        "            if published:\n",
        "                date_parts = published.get('date-parts', [[]])[0]\n",
        "                if date_parts:\n",
        "                    year = date_parts[0]\n",
        "\n",
        "            citations = message.get('is-referenced-by-count', 0)\n",
        "\n",
        "            journal = None\n",
        "            container = message.get('container-title', [])\n",
        "            if container:\n",
        "                journal = container[0]\n",
        "\n",
        "            return year, citations, journal\n",
        "    except:\n",
        "        pass\n",
        "    return None, None, None\n",
        "\n",
        "def get_openalex_metadata(doi):\n",
        "    try:\n",
        "        url = f\"https://api.openalex.org/works/doi:{doi}\"\n",
        "        headers = {'User-Agent': f'mailto:{YOUR_EMAIL}'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "\n",
        "            year = data.get('publication_year')\n",
        "            citations = data.get('cited_by_count', 0)\n",
        "\n",
        "            journal = None\n",
        "            primary_location = data.get('primary_location', {})\n",
        "            if primary_location:\n",
        "                source = primary_location.get('source')\n",
        "                if source:\n",
        "                    journal = source.get('display_name')\n",
        "\n",
        "            return year, citations, journal\n",
        "    except:\n",
        "        pass\n",
        "    return None, None, None\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FETCHING METADATA FROM APIs\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "df = pd.read_csv(METADATA_CSV)\n",
        "api_updated = {'year': 0, 'citation': 0, 'publication': 0}\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"API lookup\"):\n",
        "    doi = row['doi']\n",
        "\n",
        "    if pd.notna(row['year']) and pd.notna(row['citation_count']) and pd.notna(row['publication']):\n",
        "        continue\n",
        "\n",
        "    year_cr, cite_cr, journal_cr = get_crossref_metadata(doi)\n",
        "\n",
        "    if pd.isna(row['year']) and year_cr:\n",
        "        df.at[idx, 'year'] = float(year_cr)\n",
        "        api_updated['year'] += 1\n",
        "\n",
        "    if pd.isna(row['citation_count']) and cite_cr:\n",
        "        df.at[idx, 'citation_count'] = float(cite_cr)\n",
        "        api_updated['citation'] += 1\n",
        "\n",
        "    if pd.isna(row['publication']) and journal_cr:\n",
        "        df.at[idx, 'publication'] = journal_cr\n",
        "        api_updated['publication'] += 1\n",
        "\n",
        "    if pd.isna(df.at[idx, 'year']) or pd.isna(df.at[idx, 'citation_count']) or pd.isna(df.at[idx, 'publication']):\n",
        "        year_oa, cite_oa, journal_oa = get_openalex_metadata(doi)\n",
        "\n",
        "        if pd.isna(df.at[idx, 'year']) and year_oa:\n",
        "            df.at[idx, 'year'] = float(year_oa)\n",
        "            api_updated['year'] += 1\n",
        "\n",
        "        if pd.isna(df.at[idx, 'citation_count']) and cite_oa:\n",
        "            df.at[idx, 'citation_count'] = float(cite_oa)\n",
        "            api_updated['citation'] += 1\n",
        "\n",
        "        if pd.isna(df.at[idx, 'publication']) and journal_oa:\n",
        "            df.at[idx, 'publication'] = journal_oa\n",
        "            api_updated['publication'] += 1\n",
        "\n",
        "    if (idx + 1) % 100 == 0:\n",
        "        df.to_csv(METADATA_CSV, index=False)\n",
        "        time.sleep(1)\n",
        "\n",
        "df.to_csv(METADATA_CSV, index=False)\n",
        "\n",
        "print(\"API FETCH COMPLETE\")\n",
        "print(f\"Updated years: {api_updated['year']}\")\n",
        "print(f\"Updated citations: {api_updated['citation']}\")\n",
        "print(f\"Updated publications: {api_updated['publication']}\")"
      ],
      "metadata": {
        "id": "fkrqRNpEjvCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ASSIGN CATR=EGORIES FROM SOURCE FILES\n",
        "\n",
        "print(\"ASSIGNING CATEGORIES FROM SOURCE FILES\")\n",
        "\n",
        "def normalize_doi(doi):\n",
        "    if pd.isna(doi):\n",
        "        return None\n",
        "    doi_str = str(doi).strip().lower()\n",
        "    doi_str = doi_str.replace('.', '/')\n",
        "    doi_str = doi_str.rstrip(',').strip()\n",
        "    return doi_str\n",
        "\n",
        "def parse_source_file(content):\n",
        "    pattern = r'(10\\.\\S+?)(success|failed)'\n",
        "    matches = re.findall(pattern, content, re.IGNORECASE)\n",
        "    return [(doi, status) for doi, status in matches]\n",
        "\n",
        "df = pd.read_csv(METADATA_CSV)\n",
        "print(f\"Loaded {len(df)} records\")\n",
        "\n",
        "df['category'] = None\n",
        "\n",
        "source_files = {\n",
        "    'liver': LIVER_SOURCE,\n",
        "    'lung': LUNG_SOURCE,\n",
        "    'heart': HEART_SOURCE,\n",
        "    'kidney': KIDNEY_SOURCE\n",
        "}\n",
        "\n",
        "doi_to_organ = {}\n",
        "organ_stats = {}\n",
        "\n",
        "for organ, filepath in source_files.items():\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        doi_status_pairs = parse_source_file(content)\n",
        "\n",
        "        success_count = 0\n",
        "        for doi, status in doi_status_pairs:\n",
        "            if status.lower() == 'success':\n",
        "                doi_normalized = normalize_doi(doi)\n",
        "                if doi_normalized:\n",
        "                    doi_to_organ[doi_normalized] = organ\n",
        "                    success_count += 1\n",
        "\n",
        "        organ_stats[organ] = success_count\n",
        "        print(f\"  {organ:10} : {success_count:4d} DOIs loaded\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  {organ:10} : File not found\")\n",
        "        organ_stats[organ] = 0\n",
        "\n",
        "print(f\"\\nTotal DOIs mapped: {len(doi_to_organ)}\")\n",
        "\n",
        "matched = 0\n",
        "for idx, row in df.iterrows():\n",
        "    doi_normalized = str(row['doi']).strip().lower()\n",
        "\n",
        "    if doi_normalized in doi_to_organ:\n",
        "        df.at[idx, 'category'] = doi_to_organ[doi_normalized]\n",
        "        matched += 1\n",
        "\n",
        "df['category'] = df['category'].fillna('unassigned')\n",
        "df.to_csv(METADATA_CSV, index=False)\n",
        "\n",
        "print(f\"\\nMatched: {matched}/{len(df)}\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "print(df['category'].value_counts())"
      ],
      "metadata": {
        "id": "YXaWNKbkjvIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFY UNASSIGNED PAPERS BY KEYWORDS\n",
        "\n",
        "print(\"CLASSIFYING UNASSIGNED PAPERS BY KEYWORDS\")\n",
        "\n",
        "df = pd.read_csv(METADATA_CSV)\n",
        "unassigned = df[df['category'] == 'unassigned']\n",
        "print(f\"Unassigned papers: {len(unassigned)}\")\n",
        "\n",
        "def keyword_classify(text):\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    scores = {'liver': 0, 'lung': 0, 'heart': 0, 'kidney': 0}\n",
        "\n",
        "    liver_terms = ['liver', 'hepatic', 'hepato', 'cirrhosis', 'bile', 'hepatitis']\n",
        "    lung_terms = ['lung', 'pulmonary', 'respiratory', 'bronch', 'pneumo', 'airway']\n",
        "    heart_terms = ['heart', 'cardiac', 'cardio', 'coronary', 'myocard', 'aortic']\n",
        "    kidney_terms = ['kidney', 'renal', 'nephro', 'dialysis', 'glomerular', 'urinary']\n",
        "\n",
        "    for term in liver_terms:\n",
        "        scores['liver'] += len(re.findall(r'\\b' + term, text_lower))\n",
        "    for term in lung_terms:\n",
        "        scores['lung'] += len(re.findall(r'\\b' + term, text_lower))\n",
        "    for term in heart_terms:\n",
        "        scores['heart'] += len(re.findall(r'\\b' + term, text_lower))\n",
        "    for term in kidney_terms:\n",
        "        scores['kidney'] += len(re.findall(r'\\b' + term, text_lower))\n",
        "\n",
        "    if max(scores.values()) > 0:\n",
        "        return max(scores, key=scores.get)\n",
        "    return 'other'\n",
        "\n",
        "text_dir = Path(TEXT_OUTPUT)\n",
        "\n",
        "for idx, row in unassigned.iterrows():\n",
        "    doi = row['doi']\n",
        "    txt_file = text_dir / f\"{doi.replace('/', '_')}.txt\"\n",
        "\n",
        "    if txt_file.exists():\n",
        "        try:\n",
        "            with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                text = f.read()[:5000]\n",
        "            organ = keyword_classify(text)\n",
        "            df.at[idx, 'category'] = organ\n",
        "        except:\n",
        "            df.at[idx, 'category'] = 'other'\n",
        "    else:\n",
        "        df.at[idx, 'category'] = 'other'\n",
        "\n",
        "df.to_csv(METADATA_CSV, index=False)\n",
        "\n",
        "print(f\"\\nFinal category distribution:\")\n",
        "print(df['category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "IfIEEmvHjvK_",
        "outputId": "b07760bf-5cc8-45ff-ee0c-3b6ce9b5a24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLASSIFYING UNASSIGNED PAPERS BY KEYWORDS\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2496566745.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETADATA_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0munassigned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'unassigned'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unassigned papers: {len(unassigned)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAN CITATION COUNT COLUMN\n",
        "\n",
        "print(\"CLEANING CITATION COUNT COLUMN\")\n",
        "\n",
        "df = pd.read_csv(METADATA_CSV)\n",
        "\n",
        "cleaned_count = 0\n",
        "for idx, row in df.iterrows():\n",
        "    year = row['year']\n",
        "    cite = row['citation_count']\n",
        "\n",
        "    if pd.notna(year) and pd.notna(cite):\n",
        "        if int(float(year)) == int(float(cite)):\n",
        "            df.at[idx, 'citation_count'] = None\n",
        "            cleaned_count += 1\n",
        "\n",
        "print(f\"Cleaned {cleaned_count} incorrect citation values\")\n",
        "df.to_csv(METADATA_CSV, index=False)"
      ],
      "metadata": {
        "id": "5R_H3PxEjvNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL STATISTICS\n",
        "\n",
        "print(\"FINAL DATASET STATISTICS\")\n",
        "\n",
        "df = pd.read_csv(METADATA_CSV)\n",
        "\n",
        "print(f\"\\nTotal papers: {len(df)}\")\n",
        "\n",
        "print(f\"\\nMetadata coverage:\")\n",
        "print(f\"  Year: {df['year'].notna().sum()}/{len(df)} ({df['year'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "print(f\"  Citation: {df['citation_count'].notna().sum()}/{len(df)} ({df['citation_count'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "print(f\"  Publication: {df['publication'].notna().sum()}/{len(df)} ({df['publication'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nCategory distribution:\")\n",
        "for cat, count in df['category'].value_counts().items():\n",
        "    print(f\"  {cat:12}: {count:5d} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "if df['year'].notna().sum() > 0:\n",
        "    print(f\"\\nYear range: {int(df['year'].min())} - {int(df['year'].max())}\")\n",
        "\n",
        "print(f\"\\nExtraction method distribution:\")\n",
        "print(df['extraction_method'].value_counts())\n",
        "\n",
        "print(\"DATASET READY\")"
      ],
      "metadata": {
        "id": "N-jIcuqdplXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEPENDENCIES FOR DEEPSEEK\n",
        "\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q transformers accelerate safetensors\n",
        "!pip install -q pdf2image pillow\n",
        "!apt-get install -q poppler-utils"
      ],
      "metadata": {
        "id": "u3EG17GVpuch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUTHENTICATE WITH HUGGING FACE\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = \"HF_TOKEN\"\n",
        "login(HF_TOKEN)"
      ],
      "metadata": {
        "id": "b8Lcvdn1puwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DEEPSEEK MODEL\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "print(\"Loading DeepSeek-VL2 model.\")\n",
        "print(\"This may take 5-10 minutes on first run.\")\n",
        "\n",
        "model_id = \"deepseek-ai/deepseek-vl2-tiny\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "vmZFPHThqHSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE DEEPSEEK OCR FUNCTIONS\n",
        "\n",
        "from PIL import Image\n",
        "import pdf2image\n",
        "\n",
        "def ocr_image_with_deepseek(image, model, processor):\n",
        "    \"\"\"Perform OCR on a single image using DeepSeek-VL2\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": \"Extract and transcribe all text from this image. Output only the extracted text, nothing else.\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    inputs = processor(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=False,\n",
        "            pad_token_id=processor.tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if \"assistant\" in response.lower():\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "def ocr_pdf_with_deepseek(pdf_path, model, processor, max_pages=20):\n",
        "    \"\"\"Perform OCR on all pages of a PDF using DeepSeek-VL2\"\"\"\n",
        "\n",
        "    try:\n",
        "        images = pdf2image.convert_from_path(str(pdf_path), dpi=150)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "    if max_pages:\n",
        "        images = images[:max_pages]\n",
        "\n",
        "    all_text = []\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        print(f\"  Processing page {i+1}/{len(images)}...\", end=\"\\r\")\n",
        "        try:\n",
        "            page_text = ocr_image_with_deepseek(image, model, processor)\n",
        "            if page_text:\n",
        "                all_text.append(f\"--- Page {i+1} ---\\n{page_text}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error on page {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"  Processed {len(images)} pages.          \")\n",
        "\n",
        "    return \"\\n\\n\".join(all_text)"
      ],
      "metadata": {
        "id": "754jcDEpqM_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESS SCANNED PDFS WITH DEEPSEEK OCR\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Make sure these match your configuration\n",
        "PDF_FOLDER = \"/content/drive/MyDrive/CapstoneProject/Capstone/papers\"\n",
        "TEXT_OUTPUT = \"/content/drive/MyDrive/CapstoneProject/Capstone/text_extraction\"\n",
        "METADATA_CSV = \"/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new.csv\"\n",
        "\n",
        "df = pd.read_csv(METADATA_CSV)\n",
        "\n",
        "# Find PDFs that need OCR (text_length < 500)\n",
        "needs_ocr = df[df['text_length'] < 500].copy()\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"PROCESSING {len(needs_ocr)} SCANNED PDFs WITH DEEPSEEK OCR\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "ocr_success = 0\n",
        "ocr_failed = 0\n",
        "\n",
        "for idx, row in needs_ocr.iterrows():\n",
        "    doi = row['doi']\n",
        "    pdf_filename = doi.replace('/', '_') + '.pdf'\n",
        "    pdf_path = Path(PDF_FOLDER) / pdf_filename\n",
        "    txt_file = Path(TEXT_OUTPUT) / f\"{doi.replace('/', '_')}.txt\"\n",
        "\n",
        "    if not pdf_path.exists():\n",
        "        print(f\"PDF not found: {doi}\")\n",
        "        ocr_failed += 1\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing: {doi}\")\n",
        "\n",
        "    try:\n",
        "        ocr_text = ocr_pdf_with_deepseek(pdf_path, model, processor, max_pages=20)\n",
        "\n",
        "        if ocr_text and len(ocr_text.strip()) > 100:\n",
        "            with open(txt_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(ocr_text)\n",
        "\n",
        "            df.at[idx, 'text_length'] = len(ocr_text.strip())\n",
        "            df.at[idx, 'extraction_method'] = 'deepseek_ocr'\n",
        "            df.at[idx, 'needs_ocr'] = False\n",
        "\n",
        "            ocr_success += 1\n",
        "            print(f\"  ✓ Success: {len(ocr_text)} characters extracted\")\n",
        "        else:\n",
        "            ocr_failed += 1\n",
        "            print(f\"  ✗ Failed: No text extracted\")\n",
        "\n",
        "    except Exception as e:\n",
        "        ocr_failed += 1\n",
        "        print(f\"  ✗ Error: {str(e)[:50]}\")\n",
        "\n",
        "    # Save progress every 5 files\n",
        "    if (ocr_success + ocr_failed) % 5 == 0:\n",
        "        df.to_csv(METADATA_CSV, index=False)\n",
        "        print(f\"\\n  Progress saved. Success: {ocr_success}, Failed: {ocr_failed}\")\n",
        "\n",
        "# Final save\n",
        "df.to_csv(METADATA_CSV, index=False)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DEEPSEEK OCR COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Successfully OCR'd: {ocr_success}\")\n",
        "print(f\"Failed: {ocr_failed}\")\n",
        "print(f\"\\nExtraction method distribution:\")\n",
        "print(df['extraction_method'].value_counts())"
      ],
      "metadata": {
        "id": "l6YsTDs5qHVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}