{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmxOc7p2mIdr",
        "outputId": "65e9b317-1f97-4acf-dfb2-4118df423860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries for PDF text extraction\n",
        "!pip install -q pypdfium2 PyMuPDF PyPDF2 tqdm"
      ],
      "metadata": {
        "id": "cJDE7H0MmWtx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pypdfium2 as pdfium\n",
        "import fitz  # PyMuPDF\n",
        "import PyPDF2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "import time\n",
        "\n",
        "# ------------- CONFIG -------------\n",
        "# Folder with your PDFs\n",
        "PDF_FOLDER = Path(\"/content/drive/MyDrive/CapstoneProject/Capstone/papers\")\n",
        "\n",
        "# Where to save extracted text files\n",
        "TEXT_OUTPUT = Path(\"/content/drive/MyDrive/CapstoneProject/Capstone/text_extraction_new\")\n",
        "\n",
        "# Ground-truth organ DOI logs\n",
        "LIVER_SOURCE  = \"/content/drive/MyDrive/CapstoneProject/Capstone/download_log_liver_transplant.csv\"\n",
        "LUNG_SOURCE   = \"/content/drive/MyDrive/CapstoneProject/Capstone/download_log_lung_transplant.csv\"\n",
        "HEART_SOURCE  = \"/content/drive/MyDrive/CapstoneProject/Capstone/download_log_heart_transplant.csv\"\n",
        "KIDNEY_SOURCE = \"/content/drive/MyDrive/CapstoneProject/Capstone/download_log_kidney_transplant.csv\"\n",
        "\n",
        "# Final metadata file (single file, same columns as old one)\n",
        "METADATA_CSV = \"/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_clean.csv\"\n",
        "\n",
        "TEXT_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_pdfs = sorted(list(PDF_FOLDER.glob(\"**/*.pdf\")))\n",
        "print(f\"Total PDFs found: {len(all_pdfs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-lHcOvmmg_I",
        "outputId": "fac417f0-e21e-47c2-ee8a-e625351db33b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total PDFs found: 3831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filename_to_doi_key(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Use the PDF filename (without .pdf) as the DOI key.\n",
        "    Example: '10.1186_1471-2482-8-2.pdf' -> '10.1186_1471-2482-8-2'\n",
        "    \"\"\"\n",
        "    return filename.replace(\".pdf\", \"\").strip()\n",
        "\n",
        "\n",
        "def normalize_doi_from_log_to_key(doi: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Normalize DOIs from log files to match the underscore DOI key format.\n",
        "\n",
        "    Example:\n",
        "      '10.1186/1471-2482-8-2'   -> '10.1186_1471-2482-8-2'\n",
        "      '10.1172/JCI100208,'      -> '10.1172_jci100208'\n",
        "    \"\"\"\n",
        "    if pd.isna(doi):\n",
        "        return None\n",
        "    s = str(doi).strip().lower()\n",
        "    s = s.rstrip(\",\")\n",
        "    s = s.replace(\"/\", \"_\")\n",
        "    return s"
      ],
      "metadata": {
        "id": "E4WQ2qO5mhCr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_source_file(content: str):\n",
        "    \"\"\"\n",
        "    Parse concatenated 'doi success' / 'doi failed' style content.\n",
        "    Pattern: (10.\\S+?)(success|failed)\n",
        "    \"\"\"\n",
        "    pattern = r\"(10\\.\\S+?)(success|failed)\"\n",
        "    matches = re.findall(pattern, content, re.IGNORECASE)\n",
        "    return [(doi, status) for doi, status in matches]\n",
        "\n",
        "print(\"\\n BUILDING DOI_KEY → ORGAN CATEGORY MAP FROM SOURCE LOGS \\n\")\n",
        "\n",
        "source_files = {\n",
        "    \"liver\":  LIVER_SOURCE,\n",
        "    \"lung\":   LUNG_SOURCE,\n",
        "    \"heart\":  HEART_SOURCE,\n",
        "    \"kidney\": KIDNEY_SOURCE,\n",
        "}\n",
        "\n",
        "doi_key_to_organ = {}\n",
        "organ_stats = {k: 0 for k in source_files.keys()}\n",
        "\n",
        "for organ, filepath in source_files.items():\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "        doi_status_pairs = parse_source_file(content)\n",
        "\n",
        "        success_count = 0\n",
        "        for doi, status in doi_status_pairs:\n",
        "            if status.lower() == \"success\":\n",
        "                key = normalize_doi_from_log_to_key(doi)\n",
        "                if key:\n",
        "                    doi_key_to_organ[key] = organ\n",
        "                    success_count += 1\n",
        "\n",
        "        organ_stats[organ] = success_count\n",
        "        print(f\"{organ:10} : {success_count:4d} successful downloads (from {len(doi_status_pairs)} total)\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"{organ:10} : File not found at {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{organ:10} : Error reading {filepath} - {e}\")\n",
        "\n",
        "print(f\"\\nTotal unique DOI keys in mapping: {len(doi_key_to_organ)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zg6dxU3nKPh",
        "outputId": "aded4342-f55d-4703-bb5e-ed6a7eb2e666"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " BUILDING DOI_KEY → ORGAN CATEGORY MAP FROM SOURCE LOGS \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\S'\n",
            "/tmp/ipython-input-263575910.py:4: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  Pattern: (10.\\S+?)(success|failed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liver      : 5649 successful downloads (from 71316 total)\n",
            "lung       : 1185 successful downloads (from 18588 total)\n",
            "heart      :  965 successful downloads (from 14303 total)\n",
            "kidney     : 2264 successful downloads (from 38502 total)\n",
            "\n",
            "Total unique DOI keys in mapping: 3829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROBUST TEXT EXTRACTION\n",
        "\n",
        "def extract_text_pdfium(pdf_path: Path) -> str:\n",
        "    chunks = []\n",
        "    pdf = pdfium.PdfDocument(str(pdf_path))\n",
        "    for i in range(len(pdf)):\n",
        "        page = pdf[i]\n",
        "        textpage = page.get_textpage()\n",
        "        page_text = textpage.get_text_range()\n",
        "        if page_text:\n",
        "            chunks.append(page_text)\n",
        "    pdf.close()\n",
        "    return \"\\n\\n\".join(chunks)\n",
        "\n",
        "\n",
        "def extract_text_pymupdf(pdf_path: Path) -> str:\n",
        "    chunks = []\n",
        "    doc = fitz.open(str(pdf_path))\n",
        "    for page in doc:\n",
        "        page_text = page.get_text()\n",
        "        if page_text:\n",
        "            chunks.append(page_text)\n",
        "    doc.close()\n",
        "    return \"\\n\\n\".join(chunks)\n",
        "\n",
        "\n",
        "def extract_text_pypdf2(pdf_path: Path) -> str:\n",
        "    chunks = []\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                chunks.append(page_text)\n",
        "    return \"\\n\\n\".join(chunks)\n",
        "\n",
        "\n",
        "def extract_text_robust(pdf_path: Path):\n",
        "    \"\"\"\n",
        "    Try multiple extractors in order.\n",
        "    Returns (text, method_used).\n",
        "    \"\"\"\n",
        "    # 1) pdfium\n",
        "    try:\n",
        "        text = extract_text_pdfium(pdf_path)\n",
        "        if text.strip():\n",
        "            return text, \"pdfium\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) PyMuPDF\n",
        "    try:\n",
        "        text = extract_text_pymupdf(pdf_path)\n",
        "        if text.strip():\n",
        "            return text, \"pymupdf\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 3) PyPDF2\n",
        "    try:\n",
        "        text = extract_text_pypdf2(pdf_path)\n",
        "        if text.strip():\n",
        "            return text, \"pypdf2\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return \"\", \"failed\""
      ],
      "metadata": {
        "id": "AOw0iAXQxoeH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TITLE EXTRACTION\n",
        "\n",
        "TITLE_BAD_PATTERNS = re.compile(\n",
        "    r\"(journal|volume|issue|no\\.|number|suppl|doi|dx\\.doi|http|www\\.|copyright|©|\"\n",
        "    r\"received|accepted|submitted|revised|open access|license|issn|printed|\"\n",
        "    r\"correspondence to|author information|keywords?:|references?|contents?)\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "def looks_like_author_line(line: str) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic for author/affiliation lines.\n",
        "    \"\"\"\n",
        "    # many commas + initials like \"J. Smith, A. Kumar, ...\"\n",
        "    if line.count(\",\") >= 2 and re.search(r\"\\b[A-Z]\\.\\b\", line):\n",
        "        return True\n",
        "\n",
        "    # affiliation keywords\n",
        "    if re.search(r\"(department|university|hospital|institute|school of|centre|center|clinic)\",\n",
        "                 line, re.IGNORECASE):\n",
        "        return True\n",
        "\n",
        "    # emails\n",
        "    if \"@\" in line or \"email\" in line.lower():\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_candidate_indices(lines):\n",
        "    \"\"\"\n",
        "    Choose candidate indices to look for titles:\n",
        "    - lines within a window above ABSTRACT / SUMMARY\n",
        "    - else, first ~350 lines\n",
        "    \"\"\"\n",
        "    indices = set()\n",
        "    n = len(lines)\n",
        "\n",
        "    abstract_idxs = [\n",
        "        i for i, l in enumerate(lines)\n",
        "        if re.match(r\"^\\s*abstract\\b\", l, re.IGNORECASE)\n",
        "    ]\n",
        "    summary_idxs = [\n",
        "        i for i, l in enumerate(lines)\n",
        "        if re.match(r\"^\\s*summary\\b\", l, re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for idx in abstract_idxs + summary_idxs:\n",
        "        start = max(0, idx - 12)\n",
        "        end = max(0, idx)\n",
        "        for j in range(start, end):\n",
        "            indices.add(j)\n",
        "\n",
        "    # Fallback: first 350 lines\n",
        "    if not indices:\n",
        "        for j in range(min(350, n)):\n",
        "            indices.add(j)\n",
        "\n",
        "    return sorted(indices)\n",
        "\n",
        "\n",
        "def extract_title_from_text_strict(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Stricter, deeper title search:\n",
        "    - scans up to 2000 non-empty lines\n",
        "    - prefers lines above ABSTRACT/SUMMARY\n",
        "    - heavily filters non-title junk\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    raw_lines = text.splitlines()\n",
        "    lines = [l.strip() for l in raw_lines if l.strip()]\n",
        "    if not lines:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    if len(lines) > 2000:\n",
        "        lines = lines[:2000]\n",
        "\n",
        "    candidate_indices = get_candidate_indices(lines)\n",
        "    candidates = []\n",
        "\n",
        "    for idx in candidate_indices:\n",
        "        line = lines[idx]\n",
        "\n",
        "        # length filter\n",
        "        if not (30 <= len(line) <= 220):\n",
        "            continue\n",
        "\n",
        "        # avoid obvious non-title stuff\n",
        "        if TITLE_BAD_PATTERNS.search(line):\n",
        "            continue\n",
        "\n",
        "        # avoid section headers\n",
        "        if re.match(r\"^(abstract|introduction|materials and methods|methods|results|discussion)\\b\",\n",
        "                    line, re.IGNORECASE):\n",
        "            continue\n",
        "\n",
        "        # digit-heavy lines (issue info, tables)\n",
        "        digit_ratio = sum(c.isdigit() for c in line) / len(line)\n",
        "        if digit_ratio > 0.25:\n",
        "            continue\n",
        "\n",
        "        # author/affiliation\n",
        "        if looks_like_author_line(line):\n",
        "            continue\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        # earlier index better\n",
        "        score += max(0, 400 - idx)\n",
        "\n",
        "        # bonus if near an abstract below\n",
        "        if any(\n",
        "            re.match(r\"^\\s*abstract\\b\", lines[j], re.IGNORECASE)\n",
        "            for j in range(idx + 1, min(idx + 5, len(lines)))\n",
        "        ):\n",
        "            score += 20\n",
        "\n",
        "        # titles often don't end with '.'\n",
        "        if not line.endswith(\".\"):\n",
        "            score += 5\n",
        "\n",
        "        # penalize ALL CAPS\n",
        "        letters_only = re.sub(r\"[^A-Za-z]+\", \"\", line)\n",
        "        if letters_only and letters_only.isupper():\n",
        "            score -= 10\n",
        "\n",
        "        # penalize journal-like \"Journal, 1985, 69, 320-325\" format\n",
        "        if re.search(r\"\\b(19|20)\\d{2}\\b\", line) and \",\" in line and any(\n",
        "            w in line.lower()\n",
        "            for w in [\"journal\", \"bmj\", \"lancet\", \"gut\", \"ophthalmology\"]\n",
        "        ):\n",
        "            score -= 15\n",
        "\n",
        "        candidates.append((score, line))\n",
        "\n",
        "    if not candidates:\n",
        "        # fallback: any reasonable line near top\n",
        "        for line in lines[:100]:\n",
        "            if 20 < len(line) < 200:\n",
        "                return line\n",
        "        return \"Unknown\"\n",
        "\n",
        "    best_score, best_line = max(candidates, key=lambda x: x[0])\n",
        "    return best_line"
      ],
      "metadata": {
        "id": "35fjMp_ayQGi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACT YEAR AND PUBLICATION\n",
        "\n",
        "def extract_year_simple(text: str):\n",
        "    \"\"\"\n",
        "    Simple year extractor:\n",
        "    - scan first 5000 chars\n",
        "    - pick the most frequent 4-digit year between 1950 and 2025\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    header = text[:5000]\n",
        "    years = []\n",
        "    for match in re.findall(r\"\\b(19[5-9]\\d|20[0-2]\\d)\\b\", header):\n",
        "        y = int(match)\n",
        "        if 1950 <= y <= 2025:\n",
        "            years.append(y)\n",
        "    if not years:\n",
        "        return None\n",
        "    return Counter(years).most_common(1)[0][0]\n",
        "\n",
        "\n",
        "def extract_publication_simple(text: str):\n",
        "    \"\"\"\n",
        "    Tiny heuristic to guess a journal-name-like line.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    header = text[:6000]\n",
        "    lines = [l.strip() for l in header.splitlines() if l.strip()]\n",
        "\n",
        "    for line in lines[:80]:\n",
        "        if len(line) < 20 or len(line) > 200:\n",
        "            continue\n",
        "\n",
        "        # contain journal-related keyword\n",
        "        if re.search(r\"\\bjournal\\b\", line, re.IGNORECASE) or any(\n",
        "            kw in line.lower()\n",
        "            for kw in [\"bmj\", \"lancet\", \"gut\", \"ophthalmology\", \"hepatology\", \"kidney\", \"circulation\"]\n",
        "        ):\n",
        "            # avoid obvious normal sentences\n",
        "            if line.count(\".\") > 1:\n",
        "                continue\n",
        "            return line\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "YL4Xh9zjyaua"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_records = []\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"\\n BUILDING METADATA (BETTER TITLES + GROUND-TRUTH CATEGORIES) \\n\")\n",
        "\n",
        "for pdf_path in tqdm(all_pdfs, desc=\"Processing PDFs\", unit=\"pdf\"):\n",
        "    doi_key = filename_to_doi_key(pdf_path.name)  # stored as 'doi'\n",
        "    txt_file = TEXT_OUTPUT / f\"{doi_key}.txt\"\n",
        "\n",
        "    record = {\n",
        "        \"pdf_title\": \"Unknown\",\n",
        "        \"doi\": doi_key,   # NOTE: underscore DOI key here\n",
        "        \"file_size_mb\": round(pdf_path.stat().st_size / (1024 * 1024), 3),\n",
        "        \"text_length\": 0,\n",
        "        \"is_scanned\": False,\n",
        "        \"needs_ocr\": False,\n",
        "        \"extraction_method\": \"none\",\n",
        "        \"year\": None,\n",
        "        \"citation_count\": None,\n",
        "        \"publication\": None,\n",
        "        \"category\": None,\n",
        "    }\n",
        "\n",
        "    # --- Get text (reuse if exists) ---\n",
        "    if txt_file.exists():\n",
        "        try:\n",
        "            with open(txt_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "            method = \"existing_txt\"\n",
        "        except Exception:\n",
        "            text = \"\"\n",
        "            method = \"failed_existing\"\n",
        "    else:\n",
        "        text, method = extract_text_robust(pdf_path)\n",
        "        try:\n",
        "            with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(text)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    record[\"extraction_method\"] = method\n",
        "    record[\"text_length\"] = len(text.strip())\n",
        "\n",
        "    # Mark potential scanned docs for DeepSeek OCR later\n",
        "    if record[\"text_length\"] < 200 and method != \"failed\":\n",
        "        record[\"is_scanned\"] = True\n",
        "        record[\"needs_ocr\"] = True\n",
        "\n",
        "    # Title extraction\n",
        "    if record[\"text_length\"] > 0:\n",
        "        record[\"pdf_title\"] = extract_title_from_text_strict(text)\n",
        "\n",
        "        # Year & publication – best effort\n",
        "        yr = extract_year_simple(text)\n",
        "        if yr is not None:\n",
        "            record[\"year\"] = float(yr)\n",
        "        pub = extract_publication_simple(text)\n",
        "        if pub:\n",
        "            record[\"publication\"] = pub\n",
        "\n",
        "    # Category from DOI logs\n",
        "    record[\"category\"] = doi_key_to_organ.get(doi_key, \"unassigned\")\n",
        "\n",
        "    metadata_records.append(record)\n",
        "\n",
        "elapsed = (time.time() - start_time) / 60\n",
        "print(f\"\\nProcessed {len(metadata_records)} PDFs in {elapsed:.1f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO2uoZN0ygUD",
        "outputId": "9c9ffc55-8268-4b17-e393-c2df9a804757"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " BUILDING METADATA (BETTER TITLES + GROUND-TRUTH CATEGORIES) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs: 100%|██████████| 3831/3831 [00:59<00:00, 64.32pdf/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed 3831 PDFs in 1.0 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE METADATA\n",
        "metadata_df = pd.DataFrame(metadata_records)\n",
        "metadata_df.to_csv(METADATA_CSV, index=False)\n",
        "\n",
        "print(\"\\n METADATA SAVED \\n\")\n",
        "print(f\"Path : {METADATA_CSV}\")\n",
        "print(f\"Rows : {len(metadata_df)}\")\n",
        "print(\"\\nPreview:\")\n",
        "print(metadata_df.head(10).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDei4Ae_g2Y4",
        "outputId": "8f591678-6914-49ca-ecb1-8a8f9b718941"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " METADATA SAVED \n",
            "\n",
            "Path : /content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_clean.csv\n",
            "Rows : 3831\n",
            "\n",
            "Preview:\n",
            "                                                                                 pdf_title                            doi  file_size_mb  text_length  is_scanned  needs_ocr extraction_method   year citation_count                                                                           publication   category\n",
            "                                                                 Preparing to download ... 10_1002_14651858_CD005055_pub3         0.001           54        True       True      existing_txt    NaN           None                                                                                  None unassigned\n",
            "                                                                 Preparing to download ... 10_1002_14651858_CD006660_pub3         0.001           54        True       True      existing_txt    NaN           None                                                                                  None unassigned\n",
            "                                    Better health. Cochrane Database of Systematic Reviews 10_1002_14651858_CD010985_pub4         0.289        86383       False      False      existing_txt 2020.0           None                                                                                  None unassigned\n",
            "                                                                 Preparing to download ...      10_1002_14651858_ED000049         0.001           54        True       True      existing_txt    NaN           None                                                                                  None unassigned\n",
            "                                                            Evaluation der Vermittlung von              10_1002_ccr3_2825         1.533       142303       False      False      existing_txt 2012.0           None                                                                                  None unassigned\n",
            "                                                                                   Unknown             10_1002_ehf2_12005         0.001            0        True       True      existing_txt    NaN           None                                                                                  None unassigned\n",
            "aminotransferase; AST, aspartate aminotransferase; HR, hazard ratio; sHR, subhazard ratio;              10_1002_hep_30368         0.643        59142       False      False      existing_txt 2004.0           None 1 Division of Gastroenterology and Hepatology, Indiana University School of Medicine, unassigned\n",
            "                                          Publisher's PDF, also known as Version of record              10_1002_hep_30915         0.257        42810       False      False      existing_txt 2019.0           None                                                                                  None unassigned\n",
            "           Normothermic Machine Perfusion of Donor Livers Without the Need for Human Blood               10_1002_lt_25005         0.963        44322       False      False      existing_txt 2018.0           None                                                                                  None unassigned\n",
            "                                                         Dr. Richard S. Mangus, MD MS FACS               10_1002_lt_25631         0.682        46290       False      False      existing_txt 2019.0           None       4. Division of Gastroenterology and Hepatology, Department of Medicine, Indiana unassigned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "NEW_METADATA_CSV = \"/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_clean.csv\"\n",
        "OLD_METADATA_CSV = \"/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_final.csv\"\n",
        "\n",
        "print(\"Loading new metadata from:\", NEW_METADATA_CSV)\n",
        "md_new = pd.read_csv(NEW_METADATA_CSV)\n",
        "print(\"New metadata shape:\", md_new.shape)\n",
        "\n",
        "print(\"\\nLoading old metadata from:\", OLD_METADATA_CSV)\n",
        "md_old = pd.read_csv(OLD_METADATA_CSV)\n",
        "print(\"Old metadata shape:\", md_old.shape)\n",
        "\n",
        "print(\"\\nNew columns:\", md_new.columns.tolist())\n",
        "print(\"Old columns:\", md_old.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUzJUEVUg_Es",
        "outputId": "ec168547-6cf9-4df2-c661-5ceee25df44b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading new metadata from: /content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_clean.csv\n",
            "New metadata shape: (3831, 11)\n",
            "\n",
            "Loading old metadata from: /content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_final.csv\n",
            "Old metadata shape: (3746, 11)\n",
            "\n",
            "New columns: ['pdf_title', 'doi', 'file_size_mb', 'text_length', 'is_scanned', 'needs_ocr', 'extraction_method', 'year', 'citation_count', 'publication', 'category']\n",
            "Old columns: ['pdf_title', 'doi', 'file_size_mb', 'text_length', 'is_scanned', 'needs_ocr', 'extraction_method', 'year', 'citation_count', 'publication', 'category']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we don't lose anything\n",
        "print(\"\\nExample DOIs:\")\n",
        "print(\"New:\", md_new[\"doi\"].head(3).tolist())\n",
        "print(\"Old:\", md_old[\"doi\"].head(3).tolist())\n",
        "\n",
        "# Create a unified key in both DataFrames\n",
        "md_new[\"doi_key\"] = md_new[\"doi\"].astype(str).str.strip()\n",
        "\n",
        "md_old[\"doi_key\"] = (\n",
        "    md_old[\"doi\"]\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace(\"/\", \"_\", regex=False)  # 10/1002/... -> 10_1002_...\n",
        ")\n",
        "\n",
        "print(\"\\nUnique doi_key counts:\")\n",
        "print(\"New:\", md_new[\"doi_key\"].nunique())\n",
        "print(\"Old:\", md_old[\"doi_key\"].nunique())"
      ],
      "metadata": {
        "id": "bwHwLJwCixZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb03cf09-cc79-4a44-aec0-778c6fad9018"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example DOIs:\n",
            "New: ['10_1002_14651858_CD005055_pub3', '10_1002_14651858_CD006660_pub3', '10_1002_14651858_CD010985_pub4']\n",
            "Old: ['10/1136/bjo/69/5/320', '10/1136/bmj/2/6203/1461', '10/1172/JCI109506']\n",
            "\n",
            "Unique doi_key counts:\n",
            "New: 3831\n",
            "Old: 3746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build mapping dicts from old metadata\n",
        "old_by_key = md_old.set_index(\"doi_key\")\n",
        "\n",
        "year_map        = old_by_key[\"year\"].to_dict()\n",
        "cites_map       = old_by_key[\"citation_count\"].to_dict()\n",
        "pub_map         = old_by_key[\"publication\"].to_dict()\n",
        "category_map    = old_by_key[\"category\"].to_dict()\n",
        "\n",
        "print(\"Mapping sizes:\")\n",
        "print(\"year       :\", len(year_map))\n",
        "print(\"citations  :\", len(cites_map))\n",
        "print(\"publication:\", len(pub_map))\n",
        "print(\"category   :\", len(category_map))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX0-1XSMmYc5",
        "outputId": "13693f14-211c-4a9d-87c6-c0389b14ddec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping sizes:\n",
            "year       : 3746\n",
            "citations  : 3746\n",
            "publication: 3746\n",
            "category   : 3746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "before_missing = {\n",
        "    \"year\": md_new[\"year\"].isna().sum(),\n",
        "    \"citation_count\": md_new[\"citation_count\"].isna().sum(),\n",
        "    \"publication\": md_new[\"publication\"].isna().sum(),\n",
        "    \"category_unassigned\": (md_new[\"category\"].isna() | (md_new[\"category\"] == \"unassigned\")).sum(),\n",
        "}\n",
        "\n",
        "print(\"Missing BEFORE merge:\")\n",
        "for k, v in before_missing.items():\n",
        "    print(f\"  {k:18s}: {v}\")\n",
        "\n",
        "# Temporary columns pulling from maps\n",
        "md_new[\"year_old\"]        = md_new[\"doi_key\"].map(year_map)\n",
        "md_new[\"cites_old\"]       = md_new[\"doi_key\"].map(cites_map)\n",
        "md_new[\"pub_old\"]         = md_new[\"doi_key\"].map(pub_map)\n",
        "md_new[\"category_old\"]    = md_new[\"doi_key\"].map(category_map)\n",
        "\n",
        "# ---- YEAR ----\n",
        "mask_year = md_new[\"year\"].isna() & md_new[\"year_old\"].notna()\n",
        "md_new.loc[mask_year, \"year\"] = md_new.loc[mask_year, \"year_old\"]\n",
        "\n",
        "# ---- CITATION COUNT ----\n",
        "mask_cites = md_new[\"citation_count\"].isna() & md_new[\"cites_old\"].notna()\n",
        "md_new.loc[mask_cites, \"citation_count\"] = md_new.loc[mask_cites, \"cites_old\"]\n",
        "\n",
        "# ---- PUBLICATION ----\n",
        "def is_empty_pub(x):\n",
        "    if pd.isna(x):\n",
        "        return True\n",
        "    s = str(x).strip()\n",
        "    return s == \"\" or s.lower() == \"none\"\n",
        "\n",
        "mask_pub = md_new[\"publication\"].apply(is_empty_pub) & md_new[\"pub_old\"].notna()\n",
        "md_new.loc[mask_pub, \"publication\"] = md_new.loc[mask_pub, \"pub_old\"]\n",
        "\n",
        "# ---- CATEGORY ----\n",
        "mask_cat = (md_new[\"category\"].isna() | (md_new[\"category\"] == \"unassigned\")) & md_new[\"category_old\"].notna()\n",
        "md_new.loc[mask_cat, \"category\"] = md_new.loc[mask_cat, \"category_old\"]\n",
        "\n",
        "# Enforce numeric types for year and citation_count\n",
        "md_new[\"year\"] = pd.to_numeric(md_new[\"year\"], errors=\"coerce\").astype(\"float64\")\n",
        "md_new[\"citation_count\"] = pd.to_numeric(md_new[\"citation_count\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "after_missing = {\n",
        "    \"year\": md_new[\"year\"].isna().sum(),\n",
        "    \"citation_count\": md_new[\"citation_count\"].isna().sum(),\n",
        "    \"publication\": md_new[\"publication\"].isna().sum(),\n",
        "    \"category_unassigned\": (md_new[\"category\"].isna() | (md_new[\"category\"] == \"unassigned\")).sum(),\n",
        "}\n",
        "\n",
        "print(\"\\nMissing AFTER merge:\")\n",
        "for k, v in after_missing.items():\n",
        "    print(f\"  {k:18s}: {v}\")\n",
        "\n",
        "print(\"\\nImprovements:\")\n",
        "for k in before_missing:\n",
        "    print(f\"  {k:18s}: {before_missing[k]} → {after_missing[k]}  (Δ = {before_missing[k] - after_missing[k]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEPuliabmbKh",
        "outputId": "e4f7062a-2862-4245-9fd6-8acd6f9ecb5e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing BEFORE merge:\n",
            "  year              : 209\n",
            "  citation_count    : 3831\n",
            "  publication       : 2131\n",
            "  category_unassigned: 3831\n",
            "\n",
            "Missing AFTER merge:\n",
            "  year              : 172\n",
            "  citation_count    : 3761\n",
            "  publication       : 713\n",
            "  category_unassigned: 85\n",
            "\n",
            "Improvements:\n",
            "  year              : 209 → 172  (Δ = 37)\n",
            "  citation_count    : 3831 → 3761  (Δ = 70)\n",
            "  publication       : 2131 → 713  (Δ = 1418)\n",
            "  category_unassigned: 3831 → 85  (Δ = 3746)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop temp columns we used for merge\n",
        "md_new = md_new.drop(columns=[\"doi_key\", \"year_old\", \"cites_old\", \"pub_old\", \"category_old\"], errors=\"ignore\")\n",
        "\n",
        "# Sanity check: dtypes\n",
        "print(\"\\nFinal dtypes:\")\n",
        "print(md_new.dtypes)\n",
        "\n",
        "# Save back to same CSV (or a new one if you want to be safe)\n",
        "FINAL_METADATA_CSV = \"/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_clean.csv\"\n",
        "md_new.to_csv(FINAL_METADATA_CSV, index=False)\n",
        "\n",
        "print(f\"\\n Merged metadata saved to: {FINAL_METADATA_CSV}\")\n",
        "print(f\"Rows: {len(md_new)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjE7tgQamfeB",
        "outputId": "cab42be6-7130-45ef-ecdc-c9edc575b677"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final dtypes:\n",
            "pdf_title             object\n",
            "doi                   object\n",
            "file_size_mb         float64\n",
            "text_length            int64\n",
            "is_scanned              bool\n",
            "needs_ocr               bool\n",
            "extraction_method     object\n",
            "year                 float64\n",
            "citation_count       float64\n",
            "publication           object\n",
            "category              object\n",
            "dtype: object\n",
            "\n",
            " Merged metadata saved to: /content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_clean.csv\n",
            "Rows: 3831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SPOT-CHECK: rows where we filled year/publication/category from old metadata\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Reload old with normalized key so we can join if needed\n",
        "md_old[\"doi_key\"] = (\n",
        "    md_old[\"doi\"]\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace(\"/\", \"_\", regex=False)\n",
        ")\n",
        "\n",
        "old_indexed = md_old.set_index(\"doi_key\")\n",
        "\n",
        "sample_mask = (\n",
        "    (md_new[\"doi\"].isin(old_indexed.index)) &\n",
        "    (\n",
        "        md_new[\"year\"].notna() |\n",
        "        md_new[\"publication\"].notna() |\n",
        "        md_new[\"citation_count\"].notna() |\n",
        "        (md_new[\"category\"] != \"unassigned\")\n",
        "    )\n",
        ")\n",
        "\n",
        "sample = md_new[sample_mask].head(10).copy()\n",
        "\n",
        "for _, row in sample.iterrows():\n",
        "    key = row[\"doi\"]\n",
        "    print(\"\\n------------------------------\")\n",
        "    print(\"DOI key        :\", key)\n",
        "    print(\"NEW title      :\", row[\"pdf_title\"][:80])\n",
        "    print(\"NEW year       :\", row[\"year\"])\n",
        "    print(\"NEW cites      :\", row[\"citation_count\"])\n",
        "    print(\"NEW publication:\", row[\"publication\"])\n",
        "    print(\"NEW category   :\", row[\"category\"])\n",
        "\n",
        "    if key in old_indexed.index:\n",
        "        o = old_indexed.loc[key]\n",
        "        print(\"OLD year       :\", o.get(\"year\", None))\n",
        "        print(\"OLD cites      :\", o.get(\"citation_count\", None))\n",
        "        print(\"OLD publication:\", (str(o.get(\"publication\", \"\"))[:80] if pd.notna(o.get(\"publication\", None)) else None))\n",
        "        print(\"OLD category   :\", o.get(\"category\", None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOvVyceSm2A9",
        "outputId": "4f7118c5-c6c4-43bc-e625-36f92bafb322"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SPOT-CHECK: rows where we filled year/publication/category from old metadata\n",
            "================================================================================\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_14651858_CD005055_pub3\n",
            "NEW title      : Preparing to download ...\n",
            "NEW year       : nan\n",
            "NEW cites      : nan\n",
            "NEW publication: nan\n",
            "NEW category   : heart\n",
            "OLD year       : nan\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : heart\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_14651858_CD006660_pub3\n",
            "NEW title      : Preparing to download ...\n",
            "NEW year       : nan\n",
            "NEW cites      : nan\n",
            "NEW publication: nan\n",
            "NEW category   : lung\n",
            "OLD year       : nan\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : lung\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_14651858_CD010985_pub4\n",
            "NEW title      : Better health. Cochrane Database of Systematic Reviews\n",
            "NEW year       : 2020.0\n",
            "NEW cites      : nan\n",
            "NEW publication: sickle cell disease (Review)\n",
            "NEW category   : liver\n",
            "OLD year       : 2020.0\n",
            "OLD cites      : nan\n",
            "OLD publication: sickle cell disease (Review)\n",
            "OLD category   : liver\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_14651858_ED000049\n",
            "NEW title      : Preparing to download ...\n",
            "NEW year       : nan\n",
            "NEW cites      : nan\n",
            "NEW publication: nan\n",
            "NEW category   : liver\n",
            "OLD year       : nan\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : liver\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_ccr3_2825\n",
            "NEW title      : Evaluation der Vermittlung von\n",
            "NEW year       : 2012.0\n",
            "NEW cites      : nan\n",
            "NEW publication: nan\n",
            "NEW category   : liver\n",
            "OLD year       : 2012.0\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : liver\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_hep_30368\n",
            "NEW title      : aminotransferase; AST, aspartate aminotransferase; HR, hazard ratio; sHR, subhaz\n",
            "NEW year       : 2004.0\n",
            "NEW cites      : nan\n",
            "NEW publication: 1 Division of Gastroenterology and Hepatology, Indiana University School of Medicine,\n",
            "NEW category   : liver\n",
            "OLD year       : nan\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : liver\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_hep_30915\n",
            "NEW title      : Publisher's PDF, also known as Version of record\n",
            "NEW year       : 2019.0\n",
            "NEW cites      : nan\n",
            "NEW publication: nan\n",
            "NEW category   : liver\n",
            "OLD year       : 2019.0\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : liver\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_lt_25005\n",
            "NEW title      : Normothermic Machine Perfusion of Donor Livers Without the Need for Human Blood\n",
            "NEW year       : 2018.0\n",
            "NEW cites      : nan\n",
            "NEW publication: nan\n",
            "NEW category   : liver\n",
            "OLD year       : 2018.0\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : liver\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_lt_25631\n",
            "NEW title      : Dr. Richard S. Mangus, MD MS FACS\n",
            "NEW year       : 2019.0\n",
            "NEW cites      : nan\n",
            "NEW publication: 4. Division of Gastroenterology and Hepatology, Department of Medicine, Indiana\n",
            "NEW category   : heart\n",
            "OLD year       : 2019.0\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : heart\n",
            "\n",
            "------------------------------\n",
            "DOI key        : 10_1002_mrm_26274\n",
            "NEW title      : Publisher's PDF, also known as Version of record\n",
            "NEW year       : 2017.0\n",
            "NEW cites      : nan\n",
            "NEW publication: nan\n",
            "NEW category   : heart\n",
            "OLD year       : 2017.0\n",
            "OLD cites      : nan\n",
            "OLD publication: None\n",
            "OLD category   : heart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install libraries for PDF text extraction\n",
        "!pip install -q pypdfium2 PyMuPDF PyPDF2 tqdm\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# CONFIG\n",
        "\n",
        "PDF_FOLDER = Path(\"/content/drive/MyDrive/CapstoneProject/Capstone/papers\")\n",
        "\n",
        "INPUT_METADATA_CSV  = \"/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_clean.csv\"\n",
        "OUTPUT_METADATA_CSV = \"/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_layout_fixed.csv\"\n",
        "\n",
        "# LOAD METADATA\n",
        "\n",
        "md = pd.read_csv(INPUT_METADATA_CSV)\n",
        "print(\"Loaded metadata:\", md.shape)\n",
        "\n",
        "if \"doi\" not in md.columns:\n",
        "    raise ValueError(\"Metadata must have a 'doi' column (underscore-style keys).\")\n",
        "\n",
        "md[\"doi\"] = md[\"doi\"].astype(str).str.strip()\n",
        "\n",
        "# PATTERNS\n",
        "\n",
        "TITLE_BAD_PATTERNS = re.compile(\n",
        "    r\"(journal|volume|issue|no\\.|number|suppl|supplement|pages|pp\\.|doi|dx\\.doi|http|www\\.|\"\n",
        "    r\"copyright|©|received|accepted|submitted|revised|open access|license|issn|printed|references?|\"\n",
        "    r\"table\\s+\\d+|figure\\s+\\d+|contents?|index)\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "SECTION_HEADER_PAT = re.compile(\n",
        "    r\"^(abstract|introduction|background|methods?|materials and methods|results?|discussion|conclusions?)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "AFFILIATION_PAT = re.compile(\n",
        "    r\"(department|university|hospital|institute|school of|faculty|centre|center|clinic|address)\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "PUBLISHER_KEYWORDS = re.compile(\n",
        "    r\"(published by|publisher[: ]|publishing|press\\b|wiley|elsevier|springer|sage publications|\"\n",
        "    r\"oxford university press|cambridge university press)\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "COPYRIGHT_LINE = re.compile(\n",
        "    r\"(?:©|copyright)\\s*(?:\\d{4})?.*\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "YEAR_PATTERN = re.compile(r\"\\b(19[5-9]\\d|20[0-2]\\d)\\b\")\n",
        "\n",
        "\n",
        "\n",
        "# CORE HELPERS\n",
        "\n",
        "def extract_title_from_spans(spans):\n",
        "    if not spans:\n",
        "        return None\n",
        "\n",
        "    spans_sorted = sorted(spans, key=lambda s: (s[\"page\"], s[\"y\"], s[\"x\"]))\n",
        "    sizes = sorted({round(s[\"size\"], 1) for s in spans}, reverse=True)\n",
        "    if not sizes:\n",
        "        return None\n",
        "\n",
        "    top_sizes = sizes[:3]\n",
        "\n",
        "    groups = {}\n",
        "    for s in spans_sorted:\n",
        "        if s[\"size\"] not in top_sizes:\n",
        "            continue\n",
        "        row_key = int(round(s[\"y\"] / 3.0))\n",
        "        key = (s[\"page\"], row_key, s[\"size\"])\n",
        "        groups.setdefault(key, []).append(s)\n",
        "\n",
        "    candidates = []\n",
        "    for (page, row, size), gspans in groups.items():\n",
        "        gspans_sorted = sorted(gspans, key=lambda s: s[\"x\"])\n",
        "        text = \" \".join(gs[\"text\"] for gs in gspans_sorted).strip()\n",
        "\n",
        "        if len(text) < 15 or len(text) > 250:\n",
        "            continue\n",
        "        if TITLE_BAD_PATTERNS.search(text):\n",
        "            continue\n",
        "        if SECTION_HEADER_PAT.search(text):\n",
        "            continue\n",
        "        if AFFILIATION_PAT.search(text):\n",
        "            continue\n",
        "\n",
        "        score = 0\n",
        "        score += size * 2\n",
        "        score += max(0, 100 - page * 30 - row * 0.5)\n",
        "\n",
        "        if 40 <= len(text) <= 150:\n",
        "            score += 15\n",
        "        elif 25 <= len(text) < 40 or 150 < len(text) <= 220:\n",
        "            score += 5\n",
        "        else:\n",
        "            score -= 5\n",
        "\n",
        "        letters_only = re.sub(r\"[^A-Za-z]+\", \"\", text)\n",
        "        if letters_only and letters_only.isupper():\n",
        "            score -= 10\n",
        "\n",
        "        digit_ratio = sum(c.isdigit() for c in text) / len(text)\n",
        "        if digit_ratio > 0.25:\n",
        "            score -= 8\n",
        "\n",
        "        candidates.append((score, page, row, size, text))\n",
        "\n",
        "    if not candidates:\n",
        "        for s in spans_sorted[:60]:\n",
        "            t = s[\"text\"].strip()\n",
        "            if 20 < len(t) < 200 and not TITLE_BAD_PATTERNS.search(t):\n",
        "                return t\n",
        "        return None\n",
        "\n",
        "    best = max(candidates, key=lambda x: x[0])\n",
        "    return best[-1]\n",
        "\n",
        "\n",
        "def extract_publisher_from_lines(lines):\n",
        "    if not lines:\n",
        "        return None\n",
        "\n",
        "    publisher_candidates = []\n",
        "    for line in lines[:80]:\n",
        "        if PUBLISHER_KEYWORDS.search(line) or COPYRIGHT_LINE.search(line):\n",
        "            publisher_candidates.append(line.strip())\n",
        "\n",
        "    if not publisher_candidates:\n",
        "        return None\n",
        "\n",
        "    pb = [l for l in publisher_candidates if \"published by\" in l.lower()]\n",
        "    if pb:\n",
        "        return min(pb, key=len)\n",
        "\n",
        "    return min(publisher_candidates, key=len)\n",
        "\n",
        "\n",
        "def extract_year_from_lines(lines, publisher_line=None):\n",
        "    candidates = []\n",
        "\n",
        "    def add_years_from_text(txt, weight=1):\n",
        "        if not txt:\n",
        "            return\n",
        "        for m in YEAR_PATTERN.findall(txt):\n",
        "            y = int(m)\n",
        "            if 1950 <= y <= 2025:\n",
        "                candidates.extend([y] * weight)\n",
        "\n",
        "    if publisher_line:\n",
        "        add_years_from_text(publisher_line, weight=3)\n",
        "\n",
        "    for line in lines[:100]:\n",
        "        low = line.lower()\n",
        "        if \"published\" in low or \"copyright\" in low or \"©\" in line:\n",
        "            add_years_from_text(line, weight=2)\n",
        "\n",
        "    for line in lines[:80]:\n",
        "        add_years_from_text(line, weight=1)\n",
        "\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    cnt = Counter(candidates)\n",
        "    most_common = cnt.most_common()\n",
        "    top_freq = most_common[0][1]\n",
        "    top_years = [y for y, f in most_common if f == top_freq]\n",
        "    return min(top_years)\n",
        "\n",
        "\n",
        "def extract_from_pdf_layout_safe(pdf_path: Path):\n",
        "    \"\"\"\n",
        "    Safe wrapper:\n",
        "      - skips very large PDFs (>50 MB)\n",
        "      - only first page\n",
        "      - handles all exceptions\n",
        "    \"\"\"\n",
        "    # skip insane files\n",
        "    size_mb = pdf_path.stat().st_size / (1024 * 1024)\n",
        "    if size_mb > 50:\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(str(pdf_path))\n",
        "    except Exception:\n",
        "        return None, None, None\n",
        "\n",
        "    spans = []\n",
        "    text_lines = []\n",
        "\n",
        "    try:\n",
        "        max_pages = min(1, len(doc))  # only FIRST PAGE to avoid heavy stuff\n",
        "        for page_idx in range(max_pages):\n",
        "            page = doc[page_idx]\n",
        "\n",
        "            try:\n",
        "                page_text = page.get_text(\"text\")\n",
        "                text_lines.extend(\n",
        "                    [l.strip() for l in page_text.splitlines() if l.strip()]\n",
        "                )\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            try:\n",
        "                dict_text = page.get_text(\"dict\")\n",
        "                for block in dict_text.get(\"blocks\", []):\n",
        "                    for line in block.get(\"lines\", []):\n",
        "                        for span in line.get(\"spans\", []):\n",
        "                            txt = span.get(\"text\", \"\").strip()\n",
        "                            if not txt:\n",
        "                                continue\n",
        "                            size = span.get(\"size\", 0.0)\n",
        "                            bbox = span.get(\"bbox\", [0, 0, 0, 0])\n",
        "                            x0, y0, _, _ = bbox\n",
        "                            spans.append(\n",
        "                                {\n",
        "                                    \"page\": page_idx,\n",
        "                                    \"x\": x0,\n",
        "                                    \"y\": y0,\n",
        "                                    \"size\": size,\n",
        "                                    \"text\": txt,\n",
        "                                }\n",
        "                            )\n",
        "            except Exception:\n",
        "                # If dict extraction fails, we still might have text_lines\n",
        "                pass\n",
        "\n",
        "    finally:\n",
        "        doc.close()\n",
        "\n",
        "    if not spans and not text_lines:\n",
        "        return None, None, None\n",
        "\n",
        "    title = extract_title_from_spans(spans) if spans else None\n",
        "    publisher = extract_publisher_from_lines(text_lines)\n",
        "    year = extract_year_from_lines(text_lines, publisher_line=publisher)\n",
        "\n",
        "    return title, publisher, year\n",
        "\n",
        "\n",
        "\n",
        "# APPLY TO ALL PDFs (SAFE)\n",
        "\n",
        "new_titles = 0\n",
        "new_pubs = 0\n",
        "new_years = 0\n",
        "missing_pdfs = 0\n",
        "errors = []\n",
        "\n",
        "for idx, row in tqdm(md.iterrows(), total=len(md), desc=\"Fixing from layout\", unit=\"pdf\"):\n",
        "    doi_key = str(row[\"doi\"]).strip()\n",
        "    pdf_path = PDF_FOLDER / f\"{doi_key}.pdf\"\n",
        "\n",
        "    if idx % 200 == 0:\n",
        "        print(f\"\\n[INFO] At row {idx}, doi_key = {doi_key}\")\n",
        "\n",
        "    if not pdf_path.exists():\n",
        "        missing_pdfs += 1\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        title, publisher, year = extract_from_pdf_layout_safe(pdf_path)\n",
        "    except Exception as e:\n",
        "        errors.append((doi_key, str(e)[:200]))\n",
        "        continue\n",
        "\n",
        "    if title and isinstance(title, str):\n",
        "        md.at[idx, \"pdf_title\"] = title\n",
        "        new_titles += 1\n",
        "\n",
        "    if publisher and isinstance(publisher, str):\n",
        "        md.at[idx, \"publication\"] = publisher\n",
        "        new_pubs += 1\n",
        "\n",
        "    if year is not None:\n",
        "        md.at[idx, \"year\"] = float(year)\n",
        "        new_years += 1\n",
        "\n",
        "print(\"\\n SUMMARY\")\n",
        "print(\"Missing PDFs (not found on disk):\", missing_pdfs)\n",
        "print(\"Titles updated                   :\", new_titles)\n",
        "print(\"Publication (publisher) updated  :\", new_pubs)\n",
        "print(\"Years updated                    :\", new_years)\n",
        "print(\"Errors during extraction         :\", len(errors))\n",
        "\n",
        "if errors:\n",
        "    print(\"\\nSample errors:\")\n",
        "    for doi_key, msg in errors[:5]:\n",
        "        print(f\"  {doi_key}: {msg}\")\n",
        "\n",
        "md[\"year\"] = pd.to_numeric(md[\"year\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "md.to_csv(OUTPUT_METADATA_CSV, index=False)\n",
        "print(\"\\nSaved updated metadata with layout-based fields to:\")\n",
        "print(OUTPUT_METADATA_CSV)\n",
        "\n",
        "print(\"\\nPreview (first 10 rows):\")\n",
        "print(md[[\"doi\", \"pdf_title\", \"year\", \"publication\"]].head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "ghhwZNyum62N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1daa9dc3-382d-47f2-df75-cbbedb31acbd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded metadata: (3831, 11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:   0%|          | 2/3831 [00:00<05:59, 10.64pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 0, doi_key = 10_1002_14651858_CD005055_pub3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:   3%|▎         | 108/3831 [00:57<16:20,  3.80pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:   5%|▌         | 205/3831 [01:07<02:11, 27.58pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 200, doi_key = 10_1038_modpathol_2014_79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  11%|█         | 405/3831 [01:26<02:42, 21.03pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 400, doi_key = 10_1038_s41598-020-57728-x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  16%|█▌        | 605/3831 [01:35<02:28, 21.69pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 600, doi_key = 10_1097_MPG_0000000000001792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  21%|██        | 803/3831 [03:16<07:38,  6.60pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 800, doi_key = 10_1136_bmj_e3054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  26%|██▌       | 1004/3831 [04:00<02:40, 17.66pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 1000, doi_key = 10_1136_gutjnl-2019-319630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  31%|███▏      | 1201/3831 [05:41<10:54,  4.02pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 1200, doi_key = 10_1136_thx_2006_068494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  33%|███▎      | 1265/3831 [05:55<08:19,  5.14pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  37%|███▋      | 1400/3831 [06:37<01:47, 22.60pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 1400, doi_key = 10_1172_JCI119114\n",
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n",
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFixing from layout:  37%|███▋      | 1404/3831 [06:37<01:46, 22.85pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFixing from layout:  37%|███▋      | 1407/3831 [06:37<01:40, 24.21pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFixing from layout:  37%|███▋      | 1411/3831 [06:37<01:33, 25.99pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n",
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n",
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n",
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  42%|████▏     | 1605/3831 [06:46<01:20, 27.56pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 1600, doi_key = 10_1186_1471-2253-14-125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  47%|████▋     | 1804/3831 [06:54<02:23, 14.10pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 1800, doi_key = 10_1186_1741-7015-11-161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  52%|█████▏    | 2006/3831 [07:03<01:02, 29.06pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 2000, doi_key = 10_1186_cc2978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  58%|█████▊    | 2205/3831 [07:12<01:08, 23.88pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 2200, doi_key = 10_1186_s13000-020-01049-0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  63%|██████▎   | 2403/3831 [08:04<09:16,  2.57pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 2400, doi_key = 10_12688_f1000research_12239_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  68%|██████▊   | 2604/3831 [08:15<00:55, 22.08pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 2600, doi_key = 10_1371_journal_pone_0053960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  73%|███████▎  | 2802/3831 [08:22<00:49, 20.91pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 2800, doi_key = 10_1371_journal_pone_0188494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  77%|███████▋  | 2937/3831 [08:29<00:58, 15.28pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  78%|███████▊  | 3001/3831 [08:32<00:30, 26.90pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 3000, doi_key = 10_1590_S0102-76382010000400005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  82%|████████▏ | 3127/3831 [08:44<01:12,  9.71pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  84%|████████▎ | 3203/3831 [08:48<00:26, 23.33pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 3200, doi_key = 10_3174_ajnr_A1768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  88%|████████▊ | 3380/3831 [09:02<00:17, 25.58pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: out of range code encountered in lzw decode\n",
            "\n",
            "MuPDF error: library error: FT_New_Memory_Face(OTATET+HelveticaNeueLTStd-Lt): unknown file format\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  89%|████████▉ | 3403/3831 [09:02<00:14, 29.28pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 3400, doi_key = 10_3389_fmed_2020_599434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  94%|█████████▍| 3600/3831 [09:18<01:47,  2.16pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 3600, doi_key = 10_4049_jimmunol_164_2_656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout:  99%|█████████▉| 3806/3831 [10:24<00:00, 27.77pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] At row 3800, doi_key = 10_7164_antibiotics_34_1619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout: 100%|█████████▉| 3828/3831 [10:25<00:00, 22.75pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuPDF error: format error: cmsOpenProfileFromMem failed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fixing from layout: 100%|██████████| 3831/3831 [10:26<00:00,  6.12pdf/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " SUMMARY\n",
            "Missing PDFs (not found on disk): 0\n",
            "Titles updated                   : 3644\n",
            "Publication (publisher) updated  : 1929\n",
            "Years updated                    : 3436\n",
            "Errors during extraction         : 0\n",
            "\n",
            "Saved updated metadata with layout-based fields to:\n",
            "/content/drive/MyDrive/CapstoneProject/Capstone/metadata_new_layout_fixed.csv\n",
            "\n",
            "Preview (first 10 rows):\n",
            "                           doi                                                                       pdf_title   year                                                                                                                               publication\n",
            "10_1002_14651858_CD005055_pub3                                                    HHS Vulnerability Disclosure    NaN                                                                                                                                       NaN\n",
            "10_1002_14651858_CD006660_pub3                                                    HHS Vulnerability Disclosure    NaN                                                                                                                                       NaN\n",
            "10_1002_14651858_CD010985_pub4              Interventions for treating intrahepatic cholestasis in people with 2020.0                                                         Copyright © 2020 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd.\n",
            "     10_1002_14651858_ED000049                                                    HHS Vulnerability Disclosure    NaN                                                                                                                                       NaN\n",
            "             10_1002_ccr3_2825                                          Pädagogische Hochschule Zentralschweiz 2012.0                                                                                                                                       NaN\n",
            "            10_1002_ehf2_12005                                                                         Unknown    NaN                                                                                                                                       NaN\n",
            "             10_1002_hep_30368  Eduardo Vilar-Gomez , Raj Vuppalanchi , Samer Gawrieh , Marwan Ghabril , Romil 2004.0                                                     1 Division of Gastroenterology and Hepatology, Indiana University School of Medicine,\n",
            "             10_1002_hep_30915                                Publisher's PDF, also known as Version of record 2019.0 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright\n",
            "              10_1002_lt_25005 Normothermic Machine Perfusion of Donor Livers Without the Need for Human Blood 2018.0                                                                                                                                 Copyright\n",
            "              10_1002_lt_25631                     DR. CHANDRASHEKHAR A KUBAL (Orcid ID : 0000-0003-4043-2943) 2019.0                                                           4. Division of Gastroenterology and Hepatology, Department of Medicine, Indiana\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enrich titles/year/journal/citations from DOI APIs\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# CONFIG\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/CapstoneProject/Capstone\")\n",
        "\n",
        "# Your current metadata file (with layout-based titles)\n",
        "INPUT_METADATA_CSV  = BASE_DIR / \"metadata_new_layout_fixed.csv\"\n",
        "OUTPUT_METADATA_CSV = BASE_DIR / \"metadata_enriched_from_apis.csv\"\n",
        "\n",
        "# Original download logs (ground truth DOIs)\n",
        "LIVER_SOURCE  = BASE_DIR / \"download_log_liver_transplant.csv\"\n",
        "LUNG_SOURCE   = BASE_DIR / \"download_log_lung_transplant.csv\"\n",
        "HEART_SOURCE  = BASE_DIR / \"download_log_heart_transplant.csv\"\n",
        "KIDNEY_SOURCE = BASE_DIR / \"download_log_kidney_transplant.csv\"\n",
        "\n",
        "# polite user agent for APIs\n",
        "YOUR_EMAIL = \"tn2463@nyu.edu\"\n",
        "\n",
        "# 1) LOAD METADATA\n",
        "\n",
        "md = pd.read_csv(INPUT_METADATA_CSV)\n",
        "print(\"Loaded metadata:\", md.shape)\n",
        "\n",
        "if \"doi\" not in md.columns:\n",
        "    raise ValueError(\"Metadata must contain a 'doi' column (underscore-style keys).\")\n",
        "\n",
        "md[\"doi\"] = md[\"doi\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "\n",
        "# 2) REBUILD KEY → CANONICAL DOI FROM LOGS\n",
        "\n",
        "def parse_source_file(content: str):\n",
        "    \"\"\"\n",
        "    Your logs look like: '10.1002/hep.30368success10.1002/hep.30370failed...'\n",
        "    We pull (doi, status) pairs.\n",
        "    \"\"\"\n",
        "    pattern = r\"(10\\.\\S+?)(success|failed)\"\n",
        "    return re.findall(pattern, content, re.IGNORECASE)\n",
        "\n",
        "def doi_to_key(doi: str) -> str:\n",
        "    \"\"\"\n",
        "    Recreate the filename-style key used for PDFs/metadata.\n",
        "    Example:\n",
        "      DOI  '10.1002/hep.30368'   -> '10_1002_hep_30368'\n",
        "      DOI  '10.1186/1471-2482-8-2' -> '10_1186_1471-2482-8-2'\n",
        "    i.e. replace '/' and '.' with '_' then lowercase.\n",
        "    \"\"\"\n",
        "    s = str(doi).strip().rstrip(\",\").lower()\n",
        "    s = s.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "    return s\n",
        "\n",
        "source_files = {\n",
        "    \"liver\":  LIVER_SOURCE,\n",
        "    \"lung\":   LUNG_SOURCE,\n",
        "    \"heart\":  HEART_SOURCE,\n",
        "    \"kidney\": KIDNEY_SOURCE,\n",
        "}\n",
        "\n",
        "key_to_doi = {}\n",
        "organ_stats = {k: 0 for k in source_files.keys()}\n",
        "\n",
        "print(\"\\nRebuilding DOI key → canonical DOI mapping from logs...\")\n",
        "\n",
        "for organ, path in source_files.items():\n",
        "    try:\n",
        "        content = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        doi_status_pairs = parse_source_file(content)\n",
        "\n",
        "        success_count = 0\n",
        "        for doi, status in doi_status_pairs:\n",
        "            if status.lower() == \"success\":\n",
        "                key = doi_to_key(doi)\n",
        "                # last one wins if duplicates, that's fine\n",
        "                key_to_doi[key] = doi.strip().rstrip(\",\")\n",
        "                success_count += 1\n",
        "\n",
        "        organ_stats[organ] = success_count\n",
        "        print(f\"  {organ:10}: {success_count:5d} successful DOIs parsed\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  {organ:10}: LOG FILE MISSING at {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {organ:10}: Error reading {path}: {e}\")\n",
        "\n",
        "print(f\"\\nTotal unique keys in mapping: {len(key_to_doi)}\")\n",
        "\n",
        "# Attach canonical_doi to metadata\n",
        "md[\"canonical_doi\"] = md[\"doi\"].map(key_to_doi)\n",
        "known_canonical = md[\"canonical_doi\"].notna().sum()\n",
        "print(f\"Rows with known canonical DOI from logs: {known_canonical}/{len(md)}\")\n",
        "\n",
        "\n",
        "# 3) CROSSREF & OPENALEX HELPERS\n",
        "\n",
        "CR_BASE = \"https://api.crossref.org/works/\"\n",
        "OA_BASE = \"https://api.openalex.org/works/doi:\"\n",
        "\n",
        "CR_HEADERS = {\"User-Agent\": f\"taruni-metadata-bot/1.0 (mailto:{YOUR_EMAIL})\"}\n",
        "OA_HEADERS = {\"User-Agent\": f\"taruni-metadata-bot/1.0 (mailto:{YOUR_EMAIL})\"}\n",
        "\n",
        "def fetch_crossref(doi: str):\n",
        "    url = CR_BASE + doi\n",
        "    try:\n",
        "        r = requests.get(url, headers=CR_HEADERS, timeout=10)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.json().get(\"message\", {})\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def fetch_openalex(doi: str):\n",
        "    url = OA_BASE + doi\n",
        "    try:\n",
        "        r = requests.get(url, headers=OA_HEADERS, timeout=10)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.json()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_cr_metadata(msg: dict):\n",
        "    \"\"\"\n",
        "    Extract title, journal, year, citations from Crossref 'message'.\n",
        "    Returns (title, journal, year, citations)\n",
        "    \"\"\"\n",
        "    if not msg:\n",
        "        return None, None, None, None\n",
        "\n",
        "    # title\n",
        "    title = None\n",
        "    tlist = msg.get(\"title\")\n",
        "    if isinstance(tlist, list) and tlist:\n",
        "        title = tlist[0].strip() or None\n",
        "\n",
        "    # journal/container\n",
        "    journal = None\n",
        "    clist = msg.get(\"container-title\")\n",
        "    if isinstance(clist, list) and clist:\n",
        "        journal = clist[0].strip() or None\n",
        "\n",
        "    # year\n",
        "    year = None\n",
        "    def pull_year(field):\n",
        "        d = msg.get(field)\n",
        "        if isinstance(d, dict):\n",
        "            parts = d.get(\"date-parts\")\n",
        "            if isinstance(parts, list) and parts and parts[0]:\n",
        "                return parts[0][0]\n",
        "        return None\n",
        "\n",
        "    year = pull_year(\"published-print\") or pull_year(\"published-online\") or pull_year(\"issued\")\n",
        "    if isinstance(year, list):\n",
        "        year = year[0]\n",
        "    if isinstance(year, str):\n",
        "        try:\n",
        "            year = int(year)\n",
        "        except:\n",
        "            year = None\n",
        "\n",
        "    # citations\n",
        "    cites = msg.get(\"is-referenced-by-count\")\n",
        "    if isinstance(cites, str):\n",
        "        try:\n",
        "            cites = int(cites)\n",
        "        except:\n",
        "            cites = None\n",
        "\n",
        "    return title, journal, year, cites\n",
        "\n",
        "def parse_oa_metadata(obj: dict):\n",
        "    \"\"\"\n",
        "    Extract title, journal, year, citations from OpenAlex object.\n",
        "    Returns (title, journal, year, citations)\n",
        "    \"\"\"\n",
        "    if not obj:\n",
        "        return None, None, None, None\n",
        "\n",
        "    title = obj.get(\"title\") or None\n",
        "    journal = None\n",
        "    loc = obj.get(\"primary_location\") or {}\n",
        "    src = loc.get(\"source\") or {}\n",
        "    if src:\n",
        "        journal = src.get(\"display_name\") or None\n",
        "\n",
        "    year = obj.get(\"publication_year\") or None\n",
        "    cites = obj.get(\"cited_by_count\") or None\n",
        "\n",
        "    return title, journal, year, cites\n",
        "\n",
        "\n",
        "\n",
        "# 4) ENRICH METADATA FROM APIS\n",
        "\n",
        "md[\"metadata_source\"] = md.get(\"metadata_source\", pd.Series([\"original\"] * len(md)))\n",
        "\n",
        "updated_title = 0\n",
        "updated_pub   = 0\n",
        "updated_year  = 0\n",
        "updated_cites = 0\n",
        "api_calls_cr  = 0\n",
        "api_calls_oa  = 0\n",
        "errors        = []\n",
        "\n",
        "print(\"\\nEnriching metadata from Crossref + OpenAlex ...\")\n",
        "print(\"This may take a while; we only hit rows with canonical_doi.\")\n",
        "\n",
        "for idx, row in tqdm(md.iterrows(), total=len(md), desc=\"DOI enrichment\", unit=\"paper\"):\n",
        "    canon = row[\"canonical_doi\"]\n",
        "    if not isinstance(canon, str) or not canon.strip():\n",
        "        continue  # no DOI -> skip\n",
        "\n",
        "    canon = canon.strip()\n",
        "\n",
        "    # Crossref\n",
        "    cr = fetch_crossref(canon)\n",
        "    time.sleep(0.15)   # be nice to API\n",
        "    api_calls_cr += 1\n",
        "\n",
        "    cr_title, cr_journal, cr_year, cr_cites = parse_cr_metadata(cr)\n",
        "\n",
        "    # OpenAlex (only if we still miss stuff or want citations)\n",
        "    oa_title = oa_journal = oa_year = oa_cites = None\n",
        "    # Only call OpenAlex if we still need citations OR year OR title/journal\n",
        "    need_oa = (cr_cites is None) or (cr_year is None) or (cr_title is None) or (cr_journal is None)\n",
        "    if need_oa:\n",
        "        oa = fetch_openalex(canon)\n",
        "        time.sleep(0.15)\n",
        "        api_calls_oa += 1\n",
        "        oa_title, oa_journal, oa_year, oa_cites = parse_oa_metadata(oa)\n",
        "\n",
        "    # ---------------- MERGE LOGIC ----------------\n",
        "    # TITLE: prefer Crossref > OpenAlex > existing\n",
        "    new_title = cr_title or oa_title\n",
        "    if new_title and isinstance(new_title, str):\n",
        "        if new_title != row.get(\"pdf_title\", \"\"):\n",
        "            md.at[idx, \"pdf_title\"] = new_title\n",
        "            updated_title += 1\n",
        "        src = md.at[idx, \"metadata_source\"]\n",
        "        if \"crossref\" not in src and cr_title:\n",
        "            md.at[idx, \"metadata_source\"] = (src + \"+crossref\").strip(\"+\")\n",
        "        elif \"openalex\" not in src and oa_title and not cr_title:\n",
        "            md.at[idx, \"metadata_source\"] = (src + \"+openalex\").strip(\"+\")\n",
        "\n",
        "    # PUBLICATION/JOURNAL\n",
        "    new_pub = cr_journal or oa_journal\n",
        "    if new_pub and isinstance(new_pub, str):\n",
        "        if pd.isna(row.get(\"publication\")) or not str(row.get(\"publication\")).strip():\n",
        "            md.at[idx, \"publication\"] = new_pub\n",
        "            updated_pub += 1\n",
        "\n",
        "    # YEAR\n",
        "    new_year = cr_year or oa_year\n",
        "    if new_year:\n",
        "        try:\n",
        "            new_year = int(new_year)\n",
        "            if new_year >= 1950 and new_year <= 2030:\n",
        "                old_year = row.get(\"year\")\n",
        "                if pd.isna(old_year) or int(old_year) != new_year:\n",
        "                    md.at[idx, \"year\"] = float(new_year)\n",
        "                    updated_year += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # CITATIONS\n",
        "    new_cites = cr_cites if cr_cites is not None else oa_cites\n",
        "    if new_cites is not None:\n",
        "        try:\n",
        "            new_cites = int(new_cites)\n",
        "            old_cites = row.get(\"citation_count\")\n",
        "            if pd.isna(old_cites) or int(old_cites) != new_cites:\n",
        "                md.at[idx, \"citation_count\"] = float(new_cites)\n",
        "                updated_cites += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "print(\"\\nSUMMARY\")\n",
        "print(f\"Rows with canonical DOI            : {known_canonical}\")\n",
        "print(f\"Crossref API calls                 : {api_calls_cr}\")\n",
        "print(f\"OpenAlex API calls                 : {api_calls_oa}\")\n",
        "print(f\"Titles updated from APIs           : {updated_title}\")\n",
        "print(f\"Publication/journal updated        : {updated_pub}\")\n",
        "print(f\"Years updated                      : {updated_year}\")\n",
        "print(f\"Citation counts updated            : {updated_cites}\")\n",
        "\n",
        "# Ensure citation_count stays numeric too\n",
        "md[\"citation_count\"] = pd.to_numeric(md[\"citation_count\"], errors=\"coerce\")\n",
        "\n",
        "# Convert year to proper integer type\n",
        "md[\"year\"] = pd.to_numeric(md[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "md.to_csv(OUTPUT_METADATA_CSV, index=False)\n",
        "print(\"Saved with integer year:\", OUTPUT_METADATA_CSV)\n",
        "\n",
        "\n",
        "md.to_csv(OUTPUT_METADATA_CSV, index=False)\n",
        "print(\"\\nSaved enriched metadata to:\")\n",
        "print(OUTPUT_METADATA_CSV)\n",
        "\n",
        "print(\"\\nPreview of enriched rows:\")\n",
        "print(md[[\"doi\", \"canonical_doi\", \"pdf_title\", \"year\", \"publication\", \"citation_count\", \"metadata_source\"]].head(12).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "80RnNxcy-TyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed63522-59a6-49be-a45c-531d31a977ef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded metadata: (3831, 11)\n",
            "\n",
            "Rebuilding DOI key → canonical DOI mapping from logs...\n",
            "  liver     :  5649 successful DOIs parsed\n",
            "  lung      :  1185 successful DOIs parsed\n",
            "  heart     :   965 successful DOIs parsed\n",
            "  kidney    :  2264 successful DOIs parsed\n",
            "\n",
            "Total unique keys in mapping: 3829\n",
            "Rows with known canonical DOI from logs: 3819/3831\n",
            "\n",
            "Enriching metadata from Crossref + OpenAlex ...\n",
            "This may take a while; we only hit rows with canonical_doi.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DOI enrichment: 100%|██████████| 3831/3831 [12:11<00:00,  5.24paper/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SUMMARY\n",
            "Rows with canonical DOI            : 3819\n",
            "Crossref API calls                 : 3819\n",
            "OpenAlex API calls                 : 29\n",
            "Titles updated from APIs           : 3678\n",
            "Publication/journal updated        : 488\n",
            "Years updated                      : 970\n",
            "Citation counts updated            : 3815\n",
            "Saved with integer year: /content/drive/MyDrive/CapstoneProject/Capstone/metadata_enriched_from_apis.csv\n",
            "\n",
            "Saved enriched metadata to:\n",
            "/content/drive/MyDrive/CapstoneProject/Capstone/metadata_enriched_from_apis.csv\n",
            "\n",
            "Preview of enriched rows:\n",
            "                           doi                  canonical_doi                                                                                                                                                                                                                                                               pdf_title  year                                                                                                                               publication  citation_count   metadata_source\n",
            "10_1002_14651858_cd005055_pub3 10.1002/14651858.CD005055.pub3                                                                                                                                   Inhaled nitric oxide for the postoperative management of pulmonary hypertension in infants and children with congenital heart disease  2014                                                                                                   Cochrane Database of Systematic Reviews             9.0 original+crossref\n",
            "10_1002_14651858_cd006660_pub3 10.1002/14651858.CD006660.pub3                                                                                                                                                                              Methods of preventing bacterial sepsis and wound complications after liver transplantation  2014                                                                                                   Cochrane Database of Systematic Reviews            17.0 original+crossref\n",
            "10_1002_14651858_cd010985_pub4 10.1002/14651858.CD010985.pub4                                                                                                                                                                                  Interventions for treating intrahepatic cholestasis in people with sickle cell disease  2020                                                         Copyright © 2020 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd.             2.0 original+crossref\n",
            "     10_1002_14651858_ed000049      10.1002/14651858.ED000049                                                                                                                                                                   Convincing Evidence from Controlled and Uncontrolled Studies on the Lipid-Lowering Effect of a Statin  2012                                                                                                   Cochrane Database of Systematic Reviews             4.0 original+crossref\n",
            "             10_1002_ccr3_2825              10.1002/ccr3.2825 Retraction: Omid Daneshjoo, Pirooz Ebrahimi, Leila B. Salehi, Antonio Pizzuti, Masoud Garshasbi (2020). Identification of a novel RUNX2 gene mutation and early diagnosis of CCD in a cleidocranial dysplasia suspected Iranian family. <i>Clin Case Rep</i>. 2020; 1‐8  2020                                                                                                                     Clinical Case Reports             2.0 original+crossref\n",
            "            10_1002_ehf2_12005                            NaN                                                                                                                                                                                                                                                                 Unknown  <NA>                                                                                                                                       NaN             NaN          original\n",
            "             10_1002_hep_30368              10.1002/hep.30368                                                                                                                           Vitamin E Improves Transplant‐Free Survival and Hepatic Decompensation Among Patients With Nonalcoholic Steatohepatitis and Advanced Fibrosis  2020                                                     1 Division of Gastroenterology and Hepatology, Indiana University School of Medicine,           156.0 original+crossref\n",
            "             10_1002_hep_30915              10.1002/hep.30915                                                                                                         Mixed Fibrinolytic Phenotypes in Decompensated Cirrhosis and Acute‐on‐Chronic Liver Failure with Hypofibrinolysis in Those With Complications and Poor Survival  2020 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright            93.0 original+crossref\n",
            "              10_1002_lt_25005               10.1002/lt.25005                                                                                                                                                                                Normothermic machine perfusion of donor livers without the need for human blood products  2018                                                                                                                                 Copyright            86.0 original+crossref\n",
            "              10_1002_lt_25631               10.1002/lt.25631                                                                                                                                Postoperative Atrial Fibrillation and Flutter in Liver Transplantation: An Important Predictor of Early and Late Morbidity and Mortality  2020                                                           4. Division of Gastroenterology and Hepatology, Department of Medicine, Indiana            25.0 original+crossref\n",
            "             10_1002_mrm_26274              10.1002/mrm.26274                                                                                                                                                                                  Highly efficient nonrigid motion‐corrected 3D whole‐heart coronary vessel wall imaging  2017 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright           101.0 original+crossref\n",
            "              10_1002_sim_7606               10.1002/sim.7606                                                                                                                                                                                   A threshold‐free summary index of prediction accuracy for censored time to event data  2018                                                                                            Department of Statistics and Actuarial Science            18.0 original+crossref\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"SCHOLAR\" (SEMANTIC SCHOLAR) ENRICHMENT + CLEANUP\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# CONFIG\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/CapstoneProject/Capstone\")\n",
        "\n",
        "# Use your latest enriched file here\n",
        "INPUT_CSV  = BASE_DIR / \"metadata_enriched_from_apis.csv\"\n",
        "OUTPUT_CSV = BASE_DIR / \"metadata_enriched_scholar.csv\"\n",
        "\n",
        "print(\"Loading metadata...\")\n",
        "md = pd.read_csv(INPUT_CSV)\n",
        "print(\"Shape:\", md.shape)\n",
        "\n",
        "required_cols = [\"doi\", \"canonical_doi\", \"pdf_title\", \"publication\", \"year\", \"citation_count\"]\n",
        "for col in required_cols:\n",
        "    if col not in md.columns:\n",
        "        raise ValueError(f\"Expected column '{col}' not found in metadata!\")\n",
        "\n",
        "# HELPER FUNCTIONS: detect bad metadata\n",
        "\n",
        "def is_bad_publication(pub):\n",
        "    if not isinstance(pub, str):\n",
        "        return True\n",
        "    s = pub.strip().lower()\n",
        "    if not s:\n",
        "        return True\n",
        "\n",
        "    # obvious junk / affiliation / portal text\n",
        "    bad_substrings = [\n",
        "        \"division of\",\n",
        "        \"department of\",\n",
        "        \"school of\",\n",
        "        \"university\",\n",
        "        \"hospital\",\n",
        "        \"research portal\",\n",
        "        \"research explorer\",\n",
        "        \"king's research\",\n",
        "        \"edinburgh research\",\n",
        "        \"portal are retained by the authors\",\n",
        "        \"copyright and moral rights\",\n",
        "        \"copyright ©\",\n",
        "        \"viewing from\",\n",
        "        \"preparing to download\",\n",
        "        \"request unsuccessful\",\n",
        "        \"incapsula incident\",\n",
        "        \"zora (zurich open repository\",\n",
        "        \"this article is protected by copyright\",\n",
        "        \"all rights reserved\"\n",
        "    ]\n",
        "\n",
        "    if any(bad in s for bad in bad_substrings):\n",
        "        return True\n",
        "\n",
        "    # if it has no letters at all\n",
        "    if not re.search(r\"[a-zA-Z]\", s):\n",
        "        return True\n",
        "\n",
        "    # extremely long = probably a sentence, not journal name\n",
        "    if len(s) > 200:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_bad_title(title):\n",
        "    if not isinstance(title, str):\n",
        "        return True\n",
        "    s = title.strip()\n",
        "    if not s:\n",
        "        return True\n",
        "\n",
        "    lower = s.lower()\n",
        "\n",
        "    # obvious garbage / placeholders\n",
        "    bad_exact = {\n",
        "        \"unknown\",\n",
        "        \"preparing to download ...\",\n",
        "        \"publisher's pdf, also known as version of record\"\n",
        "    }\n",
        "    if lower in bad_exact:\n",
        "        return True\n",
        "\n",
        "    # looks like affiliation / generic header\n",
        "    if \"division of\" in lower or \"department of\" in lower or \"university\" in lower:\n",
        "        return True\n",
        "\n",
        "    # too short / too long\n",
        "    if len(s) < 15 or len(s) > 350:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def strip_html(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    # remove HTML tags\n",
        "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
        "    # collapse whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "\n",
        "# DECIDE WHICH ROWS TO TOUCH\n",
        "\n",
        "md[\"canonical_doi\"] = md[\"canonical_doi\"].astype(str)\n",
        "has_canon = md[\"canonical_doi\"].notna() & md[\"canonical_doi\"].str.strip().ne(\"nan\")\n",
        "\n",
        "bad_pub_mask   = md[\"publication\"].apply(is_bad_publication)\n",
        "bad_title_mask = md[\"pdf_title\"].apply(is_bad_title)\n",
        "missing_cites  = md[\"citation_count\"].isna()\n",
        "\n",
        "to_fix_mask = has_canon & (bad_pub_mask | bad_title_mask | missing_cites)\n",
        "rows_to_fix = md[to_fix_mask].copy()\n",
        "\n",
        "print(\"\\nROW STATS\")\n",
        "print(\"---------\")\n",
        "print(f\"Total rows:               {len(md)}\")\n",
        "print(f\"Rows with canonical DOI:  {has_canon.sum()}\")\n",
        "print(f\"Rows with bad publication:{bad_pub_mask.sum()}\")\n",
        "print(f\"Rows with bad title:      {bad_title_mask.sum()}\")\n",
        "print(f\"Rows missing citations:   {missing_cites.sum()}\")\n",
        "print(f\"Rows to hit via SemScholar: {rows_to_fix.shape[0]}\")\n",
        "\n",
        "# SEMANTIC SCHOLAR API\n",
        "\n",
        "# Docs: https://api.semanticscholar.org\n",
        "# We'll use the public Graph API: /graph/v1/paper/DOI:<doi>?fields=...\n",
        "\n",
        "SS_BASE = \"https://api.semanticscholar.org/graph/v1/paper/DOI:\"\n",
        "SS_FIELDS = \"title,year,venue,citationCount\"\n",
        "\n",
        "SS_HEADERS = {\n",
        "    \"User-Agent\": \"taruni-metadata-bot/1.0 (semantic-scholar enrichment)\",\n",
        "}\n",
        "\n",
        "def fetch_semantic_scholar(doi):\n",
        "    url = f\"{SS_BASE}{doi}\"\n",
        "    params = {\"fields\": SS_FIELDS}\n",
        "    try:\n",
        "        r = requests.get(url, headers=SS_HEADERS, params=params, timeout=10)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.json()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_ss_meta(obj):\n",
        "    if not obj:\n",
        "        return None, None, None, None\n",
        "    title = obj.get(\"title\") or None\n",
        "    year = obj.get(\"year\") or None\n",
        "    venue = obj.get(\"venue\") or None\n",
        "    cites = obj.get(\"citationCount\") or None\n",
        "    return title, venue, year, cites\n",
        "\n",
        "# APPLY ENRICHMENT\n",
        "\n",
        "if \"metadata_source\" not in md.columns:\n",
        "    md[\"metadata_source\"] = \"original\"\n",
        "\n",
        "updated_title = 0\n",
        "updated_pub   = 0\n",
        "updated_year  = 0\n",
        "updated_cites = 0\n",
        "\n",
        "api_calls = 0\n",
        "\n",
        "print(\"\\nEnriching from Semantic Scholar (targeted)...\")\n",
        "\n",
        "for idx in tqdm(rows_to_fix.index, desc=\"SemScholar\", unit=\"row\"):\n",
        "    canon = md.at[idx, \"canonical_doi\"]\n",
        "    if not isinstance(canon, str) or not canon.strip() or canon.strip().lower() == \"nan\":\n",
        "        continue\n",
        "    canon = canon.strip()\n",
        "\n",
        "    ss_obj = fetch_semantic_scholar(canon)\n",
        "    api_calls += 1\n",
        "    time.sleep(0.15)  # chill a bit to respect rate limits\n",
        "\n",
        "    ss_title, ss_venue, ss_year, ss_cites = parse_ss_meta(ss_obj)\n",
        "\n",
        "    # TITLE\n",
        "    old_title = md.at[idx, \"pdf_title\"]\n",
        "    new_title = old_title\n",
        "\n",
        "    if is_bad_title(old_title) and ss_title:\n",
        "        new_title = ss_title\n",
        "    elif ss_title and len(ss_title) > 15 and len(ss_title) < 350:\n",
        "        # if existing title looks kind of meh, prefer SemScholar one\n",
        "        if is_bad_title(old_title):\n",
        "            new_title = ss_title\n",
        "\n",
        "    if isinstance(new_title, str):\n",
        "        new_title_clean = strip_html(new_title)\n",
        "    else:\n",
        "        new_title_clean = new_title\n",
        "\n",
        "    if isinstance(new_title_clean, str) and new_title_clean and new_title_clean != old_title:\n",
        "        md.at[idx, \"pdf_title\"] = new_title_clean\n",
        "        updated_title += 1\n",
        "        if \"semanticscholar\" not in str(md.at[idx, \"metadata_source\"]).lower():\n",
        "            md.at[idx, \"metadata_source\"] = str(md.at[idx, \"metadata_source\"]) + \"+semanticscholar\"\n",
        "\n",
        "    # PUBLICATION / VENUE\n",
        "    old_pub = md.at[idx, \"publication\"]\n",
        "    if (is_bad_publication(old_pub) or pd.isna(old_pub)) and isinstance(ss_venue, str) and ss_venue.strip():\n",
        "        md.at[idx, \"publication\"] = ss_venue.strip()\n",
        "        updated_pub += 1\n",
        "\n",
        "    # YEAR\n",
        "    old_year = md.at[idx, \"year\"]\n",
        "    if ss_year:\n",
        "        try:\n",
        "            ss_year_int = int(ss_year)\n",
        "            if 1950 <= ss_year_int <= 2030:\n",
        "                if pd.isna(old_year) or int(old_year) != ss_year_int:\n",
        "                    md.at[idx, \"year\"] = ss_year_int\n",
        "                    updated_year += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # CITATIONS\n",
        "    old_cites = md.at[idx, \"citation_count\"]\n",
        "    if ss_cites is not None:\n",
        "        try:\n",
        "            ss_cites_int = int(ss_cites)\n",
        "            if ss_cites_int >= 0 and (pd.isna(old_cites) or int(old_cites) != ss_cites_int):\n",
        "                md.at[idx, \"citation_count\"] = ss_cites_int\n",
        "                updated_cites += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "print(\"\\nSEMANTIC SCHOLAR SUMMARY\")\n",
        "print(f\"API calls                : {api_calls}\")\n",
        "print(f\"Titles updated           : {updated_title}\")\n",
        "print(f\"Publication updated      : {updated_pub}\")\n",
        "print(f\"Year updated             : {updated_year}\")\n",
        "print(f\"Citation count updated   : {updated_cites}\")\n",
        "\n",
        "# Enforce numeric dtypes\n",
        "md[\"year\"] = pd.to_numeric(md[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "md[\"citation_count\"] = pd.to_numeric(md[\"citation_count\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "\n",
        "md.to_csv(OUTPUT_CSV, index=False)\n",
        "print(\"\\nSaved enriched metadata to:\", OUTPUT_CSV)\n",
        "\n",
        "# BEFORE/AFTER DEMO ON A FEW TOUCHED ROWS\n",
        "\n",
        "cols_show = [\"doi\", \"canonical_doi\", \"pdf_title\", \"year\", \"publication\", \"citation_count\"]\n",
        "sample_idx = list(rows_to_fix.index[:10])\n",
        "\n",
        "print(\"\\nBEFORE (from input file)\")\n",
        "md_before = pd.read_csv(INPUT_CSV).loc[sample_idx, cols_show]\n",
        "print(md_before.to_string(index=False))\n",
        "\n",
        "print(\"\\nAFTER (Semantic Scholar)\")\n",
        "print(md.loc[sample_idx, cols_show].to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LdYiK1CraKR",
        "outputId": "5f900c37-bb6d-4db7-b72a-9df4ceb2ecae"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading metadata...\n",
            "Shape: (3831, 13)\n",
            "\n",
            "ROW STATS\n",
            "---------\n",
            "Total rows:               3831\n",
            "Rows with canonical DOI:  3819\n",
            "Rows with bad publication:348\n",
            "Rows with bad title:      17\n",
            "Rows missing citations:   2\n",
            "Rows to hit via SemScholar: 357\n",
            "\n",
            "Enriching from Semantic Scholar (targeted)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SemScholar: 100%|██████████| 357/357 [01:30<00:00,  3.93row/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SEMANTIC SCHOLAR SUMMARY\n",
            "API calls                : 357\n",
            "Titles updated           : 8\n",
            "Publication updated      : 0\n",
            "Year updated             : 0\n",
            "Citation count updated   : 0\n",
            "\n",
            "Saved enriched metadata to: /content/drive/MyDrive/CapstoneProject/Capstone/metadata_enriched_scholar.csv\n",
            "\n",
            "BEFORE (from input file)\n",
            "                           doi                  canonical_doi                                                                                                                                                       pdf_title   year                                                                                                                               publication  citation_count\n",
            "10_1002_14651858_cd010985_pub4 10.1002/14651858.CD010985.pub4                                                                          Interventions for treating intrahepatic cholestasis in people with sickle cell disease 2020.0                                                         Copyright © 2020 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd.             2.0\n",
            "             10_1002_hep_30368              10.1002/hep.30368                   Vitamin E Improves Transplant‐Free Survival and Hepatic Decompensation Among Patients With Nonalcoholic Steatohepatitis and Advanced Fibrosis 2020.0                                                     1 Division of Gastroenterology and Hepatology, Indiana University School of Medicine,           156.0\n",
            "             10_1002_hep_30915              10.1002/hep.30915 Mixed Fibrinolytic Phenotypes in Decompensated Cirrhosis and Acute‐on‐Chronic Liver Failure with Hypofibrinolysis in Those With Complications and Poor Survival 2020.0 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright            93.0\n",
            "              10_1002_lt_25631               10.1002/lt.25631                        Postoperative Atrial Fibrillation and Flutter in Liver Transplantation: An Important Predictor of Early and Late Morbidity and Mortality 2020.0                                                           4. Division of Gastroenterology and Hepatology, Department of Medicine, Indiana            25.0\n",
            "             10_1002_mrm_26274              10.1002/mrm.26274                                                                          Highly efficient nonrigid motion‐corrected 3D whole‐heart coronary vessel wall imaging 2017.0 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright           101.0\n",
            "              10_1002_sim_7606               10.1002/sim.7606                                                                           A threshold‐free summary index of prediction accuracy for censored time to event data 2018.0                                                                                            Department of Statistics and Actuarial Science            18.0\n",
            "    10_1007_s11222-020-09958-2     10.1007/s11222-020-09958-2                                                          Comparing clusterings and numbers of clusters by aggregation of calibrated clustering validity indexes 2020.0                                                                                                    (1) Department of Statistical Science,            56.0\n",
            "     10_1007_s12265-013-9518-4      10.1007/s12265-013-9518-4             Sustained Delivery of Insulin-Like Growth Factor-1/Hepatocyte Growth Factor Stimulates Endogenous Cardiac Repair in the Chronic Infarcted Pig Heart 2014.0 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright            97.0\n",
            "10_1016_j_cardfail_2017_06_007 10.1016/j.cardfail.2017.06.007                                                                                            Increased Secondary/Primary Bile Acid Ratio in Chronic Heart Failure 2017.0 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright           138.0\n",
            "   10_1016_j_chest_2017_03_055    10.1016/j.chest.2017.03.055                                              Expert Statements on the Standard of Care in Critically Ill Adult Patients With Atypical Hemolytic Uremic Syndrome 2017.0 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright            56.0\n",
            "\n",
            "AFTER (Semantic Scholar)\n",
            "                           doi                  canonical_doi                                                                                                                                                       pdf_title  year                                                                                                                               publication  citation_count\n",
            "10_1002_14651858_cd010985_pub4 10.1002/14651858.CD010985.pub4                                                                          Interventions for treating intrahepatic cholestasis in people with sickle cell disease  2020                                                         Copyright © 2020 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd.               2\n",
            "             10_1002_hep_30368              10.1002/hep.30368                   Vitamin E Improves Transplant‐Free Survival and Hepatic Decompensation Among Patients With Nonalcoholic Steatohepatitis and Advanced Fibrosis  2020                                                     1 Division of Gastroenterology and Hepatology, Indiana University School of Medicine,             156\n",
            "             10_1002_hep_30915              10.1002/hep.30915 Mixed Fibrinolytic Phenotypes in Decompensated Cirrhosis and Acute‐on‐Chronic Liver Failure with Hypofibrinolysis in Those With Complications and Poor Survival  2020 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright              93\n",
            "              10_1002_lt_25631               10.1002/lt.25631                        Postoperative Atrial Fibrillation and Flutter in Liver Transplantation: An Important Predictor of Early and Late Morbidity and Mortality  2020                                                           4. Division of Gastroenterology and Hepatology, Department of Medicine, Indiana              25\n",
            "             10_1002_mrm_26274              10.1002/mrm.26274                                                                          Highly efficient nonrigid motion‐corrected 3D whole‐heart coronary vessel wall imaging  2017 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright             101\n",
            "              10_1002_sim_7606               10.1002/sim.7606                                                                           A threshold‐free summary index of prediction accuracy for censored time to event data  2018                                                                                            Department of Statistics and Actuarial Science              18\n",
            "    10_1007_s11222-020-09958-2     10.1007/s11222-020-09958-2                                                          Comparing clusterings and numbers of clusters by aggregation of calibrated clustering validity indexes  2020                                                                                                    (1) Department of Statistical Science,              56\n",
            "     10_1007_s12265-013-9518-4      10.1007/s12265-013-9518-4             Sustained Delivery of Insulin-Like Growth Factor-1/Hepatocyte Growth Factor Stimulates Endogenous Cardiac Repair in the Chronic Infarcted Pig Heart  2014 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright              97\n",
            "10_1016_j_cardfail_2017_06_007 10.1016/j.cardfail.2017.06.007                                                                                            Increased Secondary/Primary Bile Acid Ratio in Chronic Heart Failure  2017 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright             138\n",
            "   10_1016_j_chest_2017_03_055    10.1016/j.chest.2017.03.055                                              Expert Statements on the Standard of Care in Critically Ill Adult Patients With Atypical Hemolytic Uremic Syndrome  2017 Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright              56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBAL SEMANTIC SCHOLAR ENRICHMENT + PUBLICATION CLEANUP + CATEGORY FIX\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# CONFIG\n",
        "BASE_DIR   = Path(\"/content/drive/MyDrive/CapstoneProject/Capstone\")\n",
        "INPUT_CSV  = BASE_DIR / \"metadata_enriched_scholar.csv\"   # latest file you created\n",
        "OUTPUT_CSV = BASE_DIR / \"metadata_enriched_scholar_full.csv\"\n",
        "\n",
        "print(\"Loading metadata\")\n",
        "md = pd.read_csv(INPUT_CSV)\n",
        "print(\"Shape:\", md.shape)\n",
        "\n",
        "# Ensure expected columns exist\n",
        "expected_cols = [\"doi\", \"canonical_doi\", \"pdf_title\", \"publication\", \"year\", \"citation_count\", \"category\"]\n",
        "for col in expected_cols:\n",
        "    if col not in md.columns:\n",
        "        raise ValueError(f\"Expected column '{col}' not found in metadata!\")\n",
        "\n",
        "\n",
        "\n",
        "# HELPERS: VALIDATION + CLEANUP\n",
        "\n",
        "def is_bad_publication(pub: str) -> bool:\n",
        "    \"\"\"\n",
        "    Decide whether 'publication' looks like junk (affiliations, portal clutter, etc.)\n",
        "    \"\"\"\n",
        "    if not isinstance(pub, str):\n",
        "        return True\n",
        "\n",
        "    s = pub.strip()\n",
        "    if not s:\n",
        "        return True\n",
        "\n",
        "    lower = s.lower()\n",
        "\n",
        "    bad_substrings = [\n",
        "        \"division of\",\n",
        "        \"department of\",\n",
        "        \"school of\",\n",
        "        \"faculty of\",\n",
        "        \"university\",\n",
        "        \"hospital\",\n",
        "        \"research portal\",\n",
        "        \"research explorer\",\n",
        "        \"king's research\",\n",
        "        \"edinburgh research\",\n",
        "        \"portal are retained by the authors\",\n",
        "        \"copyright and moral rights\",\n",
        "        \"copyright ©\",\n",
        "        \"all rights reserved\",\n",
        "        \"preparing to download\",\n",
        "        \"viewing from:\",\n",
        "        \"request unsuccessful\",\n",
        "        \"incapsula incident\",\n",
        "        \"zora (zurich open repository\",\n",
        "        \"this article is protected by copyright\",\n",
        "        \"this is the published version\"\n",
        "    ]\n",
        "\n",
        "    if any(bad in lower for bad in bad_substrings):\n",
        "        return True\n",
        "\n",
        "    # no letters at all → junk\n",
        "    if not re.search(r\"[a-zA-Z]\", s):\n",
        "        return True\n",
        "\n",
        "    # extremely long is probably a sentence, not a journal name\n",
        "    if len(s) > 200:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_bad_title(title: str) -> bool:\n",
        "    if not isinstance(title, str):\n",
        "        return True\n",
        "\n",
        "    s = title.strip()\n",
        "    if not s:\n",
        "        return True\n",
        "\n",
        "    lower = s.lower()\n",
        "\n",
        "    bad_exact = {\n",
        "        \"unknown\",\n",
        "        \"preparing to download ...\",\n",
        "        \"publisher's pdf, also known as version of record\",\n",
        "    }\n",
        "    if lower in bad_exact:\n",
        "        return True\n",
        "\n",
        "    if \"division of\" in lower or \"department of\" in lower or \"university\" in lower:\n",
        "        return True\n",
        "\n",
        "    if len(s) < 15 or len(s) > 400:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def strip_html(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    text = re.sub(r\"<[^>]+>\", \"\", text)           # remove HTML tags\n",
        "    text = re.sub(r\"\\s+\", \" \", text)             # collapse whitespace\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def clean_publication_name(pub: str):\n",
        "    \"\"\"\n",
        "    Clean up the publication string; if it's garbage, return None.\n",
        "    \"\"\"\n",
        "    if not isinstance(pub, str):\n",
        "        return None\n",
        "\n",
        "    s = strip_html(pub)\n",
        "\n",
        "    # Remove leading/trailing punctuation\n",
        "    s = re.sub(r\"^[\\s\\.,;:\\-\\|]+\", \"\", s)\n",
        "    s = re.sub(r\"[\\s\\.,;:\\-\\|]+$\", \"\", s)\n",
        "\n",
        "    # If now it looks bad, mark as None\n",
        "    if is_bad_publication(s):\n",
        "        return None\n",
        "\n",
        "    return s\n",
        "\n",
        "# SEMANTIC SCHOLAR API (FOR ALL CANONICAL DOIs)\n",
        "\n",
        "SS_BASE   = \"https://api.semanticscholar.org/graph/v1/paper/DOI:\"\n",
        "SS_FIELDS = \"title,year,venue,citationCount\"\n",
        "SS_HEADERS = {\n",
        "    \"User-Agent\": \"taruni-metadata-bot/1.0 (semantic-scholar enrichment)\",\n",
        "}\n",
        "\n",
        "def fetch_semantic_scholar(doi: str):\n",
        "    url = f\"{SS_BASE}{doi}\"\n",
        "    params = {\"fields\": SS_FIELDS}\n",
        "    try:\n",
        "        r = requests.get(url, headers=SS_HEADERS, params=params, timeout=10)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.json()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_ss_meta(obj):\n",
        "    if not obj:\n",
        "        return None, None, None, None\n",
        "    title = obj.get(\"title\") or None\n",
        "    year  = obj.get(\"year\") or None\n",
        "    venue = obj.get(\"venue\") or None\n",
        "    cites = obj.get(\"citationCount\") or None\n",
        "    return title, venue, year, cites\n",
        "\n",
        "\n",
        "# BUILD MASK OF ROWS WE CAN HIT (ALL WITH CANONICAL DOI)\n",
        "\n",
        "md[\"canonical_doi\"] = md[\"canonical_doi\"].astype(str)\n",
        "has_canon = md[\"canonical_doi\"].notna() & md[\"canonical_doi\"].str.strip().ne(\"nan\")\n",
        "\n",
        "rows_to_hit = md[has_canon].copy()\n",
        "print(\"\\nSEMANTIC SCHOLAR GLOBAL ENRICHMENT\")\n",
        "print(f\"Total rows:              {len(md)}\")\n",
        "print(f\"Rows with canonical DOI: {has_canon.sum()} (will call S2 on all of these)\")\n",
        "\n",
        "if \"metadata_source\" not in md.columns:\n",
        "    md[\"metadata_source\"] = \"original\"\n",
        "\n",
        "updated_title = 0\n",
        "updated_pub   = 0\n",
        "updated_year  = 0\n",
        "updated_cites = 0\n",
        "api_calls     = 0\n",
        "\n",
        "print(\"\\nCalling Semantic Scholar for all canonical DOIs...\")\n",
        "for idx in tqdm(rows_to_hit.index, desc=\"SemanticScholar (full)\", unit=\"paper\"):\n",
        "    canon = md.at[idx, \"canonical_doi\"]\n",
        "    if not isinstance(canon, str) or not canon.strip() or canon.strip().lower() == \"nan\":\n",
        "        continue\n",
        "\n",
        "    canon = canon.strip()\n",
        "    ss_obj = fetch_semantic_scholar(canon)\n",
        "    api_calls += 1\n",
        "    time.sleep(0.15)  # throttle\n",
        "\n",
        "    ss_title, ss_venue, ss_year, ss_cites = parse_ss_meta(ss_obj)\n",
        "\n",
        "    # TITLE\n",
        "    old_title = md.at[idx, \"pdf_title\"]\n",
        "    new_title = old_title\n",
        "\n",
        "    if ss_title:\n",
        "        ss_title_clean = strip_html(ss_title)\n",
        "        if is_bad_title(old_title) and not is_bad_title(ss_title_clean):\n",
        "            new_title = ss_title_clean\n",
        "        else:\n",
        "            # if old title looks kinda meh, we can still prefer S2\n",
        "            if not is_bad_title(ss_title_clean) and len(ss_title_clean) >= len(str(old_title) or \"\") / 2:\n",
        "                new_title = ss_title_clean\n",
        "\n",
        "    if isinstance(new_title, str) and new_title.strip() and new_title != old_title:\n",
        "        md.at[idx, \"pdf_title\"] = new_title.strip()\n",
        "        updated_title += 1\n",
        "        src = str(md.at[idx, \"metadata_source\"])\n",
        "        if \"semanticscholar\" not in src.lower():\n",
        "            md.at[idx, \"metadata_source\"] = src + \"+semanticscholar_full\"\n",
        "\n",
        "    # PUBLICATION\n",
        "    old_pub = md.at[idx, \"publication\"]\n",
        "    cleaned_old = clean_publication_name(old_pub) if isinstance(old_pub, str) else None\n",
        "\n",
        "    final_pub = cleaned_old\n",
        "    if isinstance(ss_venue, str) and ss_venue.strip():\n",
        "        ss_venue_clean = clean_publication_name(ss_venue)\n",
        "        # prefer Semantic Scholar venue if our current pub is junk or missing\n",
        "        if ss_venue_clean and (final_pub is None or is_bad_publication(old_pub)):\n",
        "            final_pub = ss_venue_clean\n",
        "\n",
        "    if final_pub is None:\n",
        "        # keep as NaN/None\n",
        "        if not pd.isna(old_pub):\n",
        "            md.at[idx, \"publication\"] = None\n",
        "            updated_pub += 1\n",
        "    else:\n",
        "        if old_pub != final_pub:\n",
        "            md.at[idx, \"publication\"] = final_pub\n",
        "            updated_pub += 1\n",
        "\n",
        "    # YEAR\n",
        "    old_year = md.at[idx, \"year\"]\n",
        "    if ss_year:\n",
        "        try:\n",
        "            ss_year_int = int(ss_year)\n",
        "            if 1900 <= ss_year_int <= 2035:\n",
        "                if pd.isna(old_year) or int(old_year) != ss_year_int:\n",
        "                    md.at[idx, \"year\"] = ss_year_int\n",
        "                    updated_year += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # CITATIONS\n",
        "    old_cites = md.at[idx, \"citation_count\"]\n",
        "    if ss_cites is not None:\n",
        "        try:\n",
        "            ss_cites_int = int(ss_cites)\n",
        "            if ss_cites_int >= 0 and (pd.isna(old_cites) or int(old_cites) != ss_cites_int):\n",
        "                md.at[idx, \"citation_count\"] = ss_cites_int\n",
        "                updated_cites += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "print(\"\\nSEMANTIC SCHOLAR (FULL) SUMMARY\")\n",
        "\n",
        "print(f\"API calls              : {api_calls}\")\n",
        "print(f\"Titles updated         : {updated_title}\")\n",
        "print(f\"Publication updated    : {updated_pub}\")\n",
        "print(f\"Year updated           : {updated_year}\")\n",
        "print(f\"Citation count updated : {updated_cites}\")\n",
        "\n",
        "\n",
        "# CATEGORY FIX FOR UNASSIGNED (USING TITLE + PUBLICATION KEYWORDS)\n",
        "\n",
        "print(\"\\nCATEGORY FIX FOR UNASSIGNED\")\n",
        "\n",
        "if \"category\" not in md.columns:\n",
        "    md[\"category\"] = \"unassigned\"\n",
        "\n",
        "unassigned_mask = (md[\"category\"].isna()) | (md[\"category\"].str.lower() == \"unassigned\")\n",
        "print(f\"Unassigned rows before classification: {unassigned_mask.sum()}\")\n",
        "\n",
        "# keyword sets\n",
        "HEART_KWS = [\n",
        "    \"heart\", \"cardiac\", \"cardio\", \"myocard\", \"coronary\", \"ventricular\", \"atrial\",\n",
        "    \"cardiomyopathy\", \"cardiovascular\", \"heart failure\", \"ischemic\", \"myocarditis\"\n",
        "]\n",
        "LUNG_KWS = [\n",
        "    \"lung\", \"pulmon\", \"respiratory\", \"bronch\", \"copd\", \"asthma\", \"pneumon\", \"airway\",\n",
        "    \"ventilator\", \"ventilation\", \"emphysema\", \"cystic fibrosis\", \"interstitial lung\"\n",
        "]\n",
        "LIVER_KWS = [\n",
        "    \"liver\", \"hepatic\", \"hepat\", \"cirrhosis\", \"cholest\", \"biliary\", \"portal vein\",\n",
        "    \"steatohepatitis\", \"nash\", \"transaminase\", \"bilirubin\", \"ascites\"\n",
        "]\n",
        "KIDNEY_KWS = [\n",
        "    \"kidney\", \"renal\", \"nephro\", \"glomer\", \"dialysis\", \"uremia\", \"hemodialysis\",\n",
        "    \"proteinuria\", \"hematuria\", \"ckd\", \"chronic kidney\"\n",
        "]\n",
        "\n",
        "def classify_category_from_text(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "    s = text.lower()\n",
        "\n",
        "    scores = {\"heart\": 0, \"lung\": 0, \"liver\": 0, \"kidney\": 0}\n",
        "\n",
        "    for kw in HEART_KWS:\n",
        "        if kw in s:\n",
        "            scores[\"heart\"] += 1\n",
        "    for kw in LUNG_KWS:\n",
        "        if kw in s:\n",
        "            scores[\"lung\"] += 1\n",
        "    for kw in LIVER_KWS:\n",
        "        if kw in s:\n",
        "            scores[\"liver\"] += 1\n",
        "    for kw in KIDNEY_KWS:\n",
        "        if kw in s:\n",
        "            scores[\"kidney\"] += 1\n",
        "\n",
        "    best_cat = max(scores, key=scores.get)\n",
        "    best_score = scores[best_cat]\n",
        "\n",
        "    # require at least 1 keyword and a unique winner\n",
        "    if best_score == 0:\n",
        "        return None\n",
        "    # check ties\n",
        "    if list(scores.values()).count(best_score) > 1:\n",
        "        return None\n",
        "\n",
        "    return best_cat\n",
        "\n",
        "new_cats = 0\n",
        "\n",
        "for idx in md[unassigned_mask].index:\n",
        "    title = md.at[idx, \"pdf_title\"]\n",
        "    pub   = md.at[idx, \"publication\"]\n",
        "    text_for_cat = \" \".join([str(title or \"\"), str(pub or \"\")])\n",
        "    cat = classify_category_from_text(text_for_cat)\n",
        "    if cat is not None:\n",
        "        md.at[idx, \"category\"] = cat\n",
        "        new_cats += 1\n",
        "\n",
        "unassigned_after = (md[\"category\"].isna()) | (md[\"category\"].str.lower() == \"unassigned\")\n",
        "print(f\"Newly classified unassigned rows: {new_cats}\")\n",
        "print(f\"Unassigned rows after classification: {unassigned_after.sum()}\")\n",
        "\n",
        "# FINAL TYPE CLEANUP + SAVE\n",
        "\n",
        "md[\"year\"] = pd.to_numeric(md[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "md[\"citation_count\"] = pd.to_numeric(md[\"citation_count\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "md.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"\\nSaved fully enriched + cleaned metadata to:\\n{OUTPUT_CSV}\")\n",
        "\n",
        "print(\"\\nQuick check:\")\n",
        "print(md[[\"doi\", \"canonical_doi\", \"pdf_title\", \"year\", \"publication\", \"citation_count\", \"category\"]].head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqKfVJDVPOK0",
        "outputId": "5acfe9a4-c7d2-4f5b-b28b-892dbb66ee7e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading metadata\n",
            "Shape: (3831, 13)\n",
            "\n",
            "SEMANTIC SCHOLAR GLOBAL ENRICHMENT\n",
            "Total rows:              3831\n",
            "Rows with canonical DOI: 3819 (will call S2 on all of these)\n",
            "\n",
            "Calling Semantic Scholar for all canonical DOIs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SemanticScholar (full): 100%|██████████| 3819/3819 [16:03<00:00,  3.96paper/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SEMANTIC SCHOLAR (FULL) SUMMARY\n",
            "API calls              : 3819\n",
            "Titles updated         : 58\n",
            "Publication updated    : 1024\n",
            "Year updated           : 31\n",
            "Citation count updated : 225\n",
            "\n",
            "CATEGORY FIX FOR UNASSIGNED\n",
            "Unassigned rows before classification: 85\n",
            "Newly classified unassigned rows: 29\n",
            "Unassigned rows after classification: 56\n",
            "\n",
            "Saved fully enriched + cleaned metadata to:\n",
            "/content/drive/MyDrive/CapstoneProject/Capstone/metadata_enriched_scholar_full.csv\n",
            "\n",
            "Quick check:\n",
            "                           doi                  canonical_doi                                                                                                                                                                                                                                                               pdf_title  year                             publication  citation_count   category\n",
            "10_1002_14651858_cd005055_pub3 10.1002/14651858.CD005055.pub3                                                                                                                                  Inhaled nitric oxide for the postoperative management of pulmonary hypertension in infants and children with congenital heart disease.  2014 Cochrane Database of Systematic Reviews              11      heart\n",
            "10_1002_14651858_cd006660_pub3 10.1002/14651858.CD006660.pub3                                                                                                                                                                             Methods of preventing bacterial sepsis and wound complications after liver transplantation.  2014 Cochrane Database of Systematic Reviews              50       lung\n",
            "10_1002_14651858_cd010985_pub4 10.1002/14651858.CD010985.pub4                                                                                                                                                                                 Interventions for treating intrahepatic cholestasis in people with sickle cell disease.  2020 Cochrane Database of Systematic Reviews               1      liver\n",
            "     10_1002_14651858_ed000049      10.1002/14651858.ED000049                                                                                                                                                                  Convincing evidence from controlled and uncontrolled studies on the lipid-lowering effect of a statin.  2012 Cochrane Database of Systematic Reviews              14      liver\n",
            "             10_1002_ccr3_2825              10.1002/ccr3.2825 Retraction: Omid Daneshjoo, Pirooz Ebrahimi, Leila B. Salehi, Antonio Pizzuti, Masoud Garshasbi (2020). Identification of a novel RUNX2 gene mutation and early diagnosis of CCD in a cleidocranial dysplasia suspected Iranian family. <i>Clin Case Rep</i>. 2020; 1‐8  2020                   Clinical Case Reports               2      liver\n",
            "            10_1002_ehf2_12005                            nan                                                                                                                                                                                                                                                                 Unknown  <NA>                                     NaN            <NA> unassigned\n",
            "             10_1002_hep_30368              10.1002/hep.30368                                                                                                                           Vitamin E Improves Transplant‐Free Survival and Hepatic Decompensation Among Patients With Nonalcoholic Steatohepatitis and Advanced Fibrosis  2020                              Hepatology             156      liver\n",
            "             10_1002_hep_30915              10.1002/hep.30915                                                                                                         Mixed Fibrinolytic Phenotypes in Decompensated Cirrhosis and Acute‐on‐Chronic Liver Failure with Hypofibrinolysis in Those With Complications and Poor Survival  2019                              Hepatology              91      liver\n",
            "              10_1002_lt_25005               10.1002/lt.25005                                                                                                                                                                                Normothermic machine perfusion of donor livers without the need for human blood products  2018                               Copyright              96      liver\n",
            "              10_1002_lt_25631               10.1002/lt.25631                                                                                                                                Postoperative Atrial Fibrillation and Flutter in Liver Transplantation: An Important Predictor of Early and Late Morbidity and Mortality  2019                   Liver transplantation              24      heart\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bjamw46XTTAV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}