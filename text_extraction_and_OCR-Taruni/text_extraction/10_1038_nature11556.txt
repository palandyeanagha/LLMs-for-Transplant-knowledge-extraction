PERSPECTIVE
doi:10.1038/nature11556
A call for transparent reporting to
optimize the predictive value of
preclinical research
Story C. Landis1
, Susan G. Amara2
, Khusru Asadullah3
, Chris P. Austin4
, Robi Blumenstein5
, EileenW. Bradley6
, Ronald G. Crystal7
,
Robert B. Darnell8
, Robert J. Ferrante9
, Howard Fillit10, Robert Finkelstein1
, Marc Fisher11, Howard E. Gendelman12,
Robert M. Golub13, John L. Goudreau14, Robert A. Gross15, Amelie K. Gubitz1
, Sharon E. Hesterlee16, David W. Howells17,
John Huguenard18, Katrina Kelner19, Walter Koroshetz1
, Dimitri Krainc20, Stanley E. Lazic21, Michael S. Levine22,
Malcolm R. Macleod23, John M. McCall24, Richard T. Moxley III25, Kalyani Narasimhan26, Linda J. Noble27, Steve Perrin28,
John D. Porter1
, Oswald Steward29, Ellis Unger30, Ursula Utz1 & Shai D. Silberberg1
The US National Institute of Neurological Disorders and Stroke convened major stakeholders in June 2012 to discuss
how to improve the methodological reporting of animal studies in grant applications and publications. The main
workshop recommendation is that at a minimum studies should report on sample-size estimation, whether and how
animals were randomized, whether investigators were blind to the treatment, and the handling of data. We recognize
that achieving a meaningful improvement in the quality of reporting will require a concerted effort by investigators,
reviewers, funding agencies and journal editors. Requiring better reporting of animal studies will raise awareness of the
importance of rigorous study design to accelerate scientific progress.
Dissemination of knowledge is the engine that drives scientific
progress. Because advances hinge primarily on previous obser￾vations, it is essential that studies are reported in sufficient detail
to allow the scientific community, research funding agencies and disease
advocacy organizations to evaluate the reliability of previous findings.
Numerous publications have called attention to the lack of transparency
in reporting, yet studies in the life sciences in general, and in animals in
particular, still often lack adequate reporting on the design, conduct and
analysis of the experiments. To develop a plan for addressing this critical
issue, the US National Institute of Neurological Disorders and Stroke
(NINDS) convened academic researchers and educators, reviewers,
journal editors and representatives from funding agencies, disease advo￾cacy communities and the pharmaceutical industry to discuss the causes
of deficient reporting and how they can be addressed. The specific goal
of the meeting was to develop recommendations for improving how the
results of animal research are reported in manuscripts and grant appli￾cations. There was broad agreement that: (1) poor reporting, often
associated with poor experimental design, is a significant issue across
the life sciences; (2) a core set of research parameters exist that should be
addressed when reporting the results of animal experiments; and (3) a
concerted effort by all stakeholders, including funding agencies and
journals, will be necessary to disseminate and implement best reporting
practices throughout the research community. Here we describe the
impetus for the meeting and the specific recommendations that were
generated.
Widespread deficiencies in methods reporting
In the life sciences, animals are used to elucidate normal biology, to
improve understanding of disease pathogenesis, and to develop therapeutic
interventions. Animal models are valuable, provided that experiments
employing them are carefully designed, interpreted and reported. Several
recent articles, commentaries and editorials highlight that inadequate
experimental reporting can result in such studies being un-interpretable
and difficult to reproduce1–8. For instance, replication of spinal cord injury
studies through an NINDS-funded program determined that many studies
could not be replicated because of incomplete or inaccurate description
of experimental design, especially how randomization of animals to the
various test groups, group formulation and delineation of animal attri￾tion and exclusion were addressed7
. A review of 100 articles published in
Cancer Research in 2010 revealed that only 28% of papers reported that
animals were randomly allocated to treatment groups, just 2% of papers
reported that observers were blinded to treatment, and none stated the
methods used to determine the number of animals per group, a deter￾mination required to avoid false outcomes2
. In addition, analysis of several
1
National Institute of Neurological Disorders and Stroke, NIH, Bethesda, Maryland 20892, USA. 2
Department of Neurobiology, University of Pittsburgh School of Medicine, Pittsburgh, Pennsylvania 15213,
USA. 3
Bayer HealthCare, 13342 Berlin, Germany. 4
National Center for Advancing Translational Sciences, NIH, Rockville, Maryland 20854, USA. 5
CHDI Management/CHDI Foundation, New York, New York
10001, USA. 6
Center for Review, NIH, Bethesda, Maryland 20892, USA. 7
Department of Genetic Medicine, Weill Cornell Medical College, New York, New York 10021, USA. 8
Howard Hughes Medical Institute,
The Rockefeller University, New York, New York 10065, USA. 9
Department of Neurological Surgery, University of Pittsburgh, Pittsburgh, Pennsylvania 15213, USA. 10Alzheimer’s Drug Discovery
Foundation, New York, New York 10019, USA. 11Department of Neurology, University of Massachusetts Medical School, Worcester, Massachusetts 01545, USA. 12Department of Pharmacology and
Experimental Neuroscience, University of Nebraska Medical Center, Omaha, Nebraska 68198, USA. 13JAMA, Chicago, Illinois 60654, USA. 14Department of Neurology, Michigan State University, East
Lansing, Michigan 48824, USA. 15Department of Neurology, University of Rochester Medical Center, Rochester, New York 14642, USA. 16Parent Project Muscular Dystrophy, Hackensack, New Jersey
07601, USA. 17The Florey Institute of Neuroscience and Mental Health, University of Melbourne, Heidelberg 3081, Australia. 18Neurology and Neurological Sciences and Cellular and Molecular Physiology,
Stanford University, Stanford, California 94305, USA. 19Science Translational Medicine, AAAS, Washington DC 22201, USA. 20Department of Neurology, Harvard Medical School, Massachusetts General
Hospital, Boston, Massachusetts 02114, USA. 21F. Hoffmann-La Roche, 4070 Basel, Switzerland. 22Department of Psychiatry and Biobehavioral Sciences, University of California Los Angeles, Los Angeles,
California 90095, USA. 23Department of Clinical Neurosciences, University of Edinburgh, Western General Hospital, Edinburgh EH4 2XU, UK. 24PharMac LLC, Boca Grande, Florida 33921, USA. 25University
of Rochester Medical Center, School of Medicine and Dentistry, Rochester, New York 14642, USA. 26Nature Neuroscience, New York, New York 10013, USA. 27Department of Neurological Surgery,
University of California San Francisco, San Francisco, California 94143, USA. 28ALS Therapy Development Institute, Cambridge, Massachusetts 02139, USA. 29Reeve-Irvine Research Center, University of
California Irvine, Irvine, California 92697, USA. 30Office of New Drugs, Center for Drug Evaluation and Research, US Food and Drug Administration, Silver Spring, Maryland 20993, USA.
11 OCTOBER 2012 | VOL 490 | NATURE | 187
©2012 Macmillan Publishers Limited. All rights reserved

hundred studies conducted in animal models of stroke, Parkinson’s disease
and multiple sclerosis also revealed deficiencies in reporting key methodo￾logical parameters that can introduce bias6
. Similarly, a review of 76 high￾impact (cited more than 500 times) animal studies showed that the pub￾lications lacked descriptions of crucial methodological information that
would allow informed judgment about the findings9
. These deficiencies in
the reporting of animal study design, which are clearly widespread, raise
the concern that the reviewers of these studies could not adequately identify
potential limitations in the experimental design and/or data analysis, lim￾iting the benefit of the findings.
Some poorly reported studies may in fact be well-designed and well￾conducted, but analysis suggests that inadequate reporting correlates
with overstated findings10–14. Problems related to inadequate study
design surfaced early in the stroke research community, as investigators
tried to understand why multiple clinical trials based on positive results
in animal studies ultimately failed. Part of the problem is, of course, that
no animal model can fully reproduce all the features of human stroke.
It also became clear, however, that many of the difficulties stemmed
from a lack of methodological rigor in the preclinical studies that were
not adequately reported15. For instance, a systematic review and meta￾analysis of studies testing the efficacy of the free-radical scavenger NXY￾059 in models of ischaemic stroke revealed that publications that
included information on randomization, concealment of group alloca￾tion, or blinded assessment of outcomes reported significantly smaller
effect sizes of NXY-059 in comparison to studies lacking this informa￾tion10. In certain cases, a series of poorly designed studies, obscured by
deficient reporting, may, in aggregate, serve erroneously as the scientific
rationale for large, expensive and ultimately unsuccessful clinical trials.
Such trials may unnecessarily expose patients to potentially harmful
agents, prevent these patients from participating in other trials of possibly
effective agents, and drain valuable resources and energy that might
otherwise be more productively spent.
A core set of reporting standards
The large fraction of poorly reported animal studies and the empirical
evidence of associated bias6,10–14,16–20, defined broadly as the introduction
of an unintentional difference between comparison groups, led various
disease communities to adopt general21–23 and animal-model-specific6,24–26
reporting guidelines. However, for guidelines to be effective and broadly
accepted by all stakeholders, they should be universal and focus on widely
accepted core issues that are important for study evaluation. Therefore,
based on available data, we recommend that, at minimum, authors of
grant applications and scientific publications should report on randomi￾zation, blinding, sample-size estimation and the handling of all data (see
below and Box 1).
Randomization and blinding
Choices made by investigators during the design, conduct and inter￾pretation of experiments can introduce bias, resulting in false-positive
results. Many have emphasized the importance of randomization and
blinding as a means to reduce bias6,21–23,27, yet inadequate reporting of
these aspects of study design remains widespread in preclinical research.
It is important to report whether the allocation, treatment and handling
of animals were the same across study groups. The selection and source
of control animals needs to be reported as well, including whether they
are true littermates of the test groups. Best practices should also include
reporting on the methods of animal randomization to the various
experimental groups, as well as on random (or appropriately blocked)
sample processing and collection of data. Attention to these details will
avoid mistaking batch effects for treatment effects (for example, dividing
samples from a large study into multiple lots, which are then processed
separately). Investigators should also report on whether the individuals
caring for the animals and conducting the experiments were blinded to
the allocation sequence, blinded to group allocation and, whenever
possible, whether the persons assessing, measuring or quantifying the
experimental outcomes were blinded to the intervention.
Sample-size estimation
Minimizing the use of animals in research is not only a requirement of
funding agencies around the world but also an ethical obligation. It is
unethical, however, to perform underpowered experiments with insuf￾ficient numbers of animals that have little prospect of detecting meaningful
differences between groups. In addition, with smaller studies, the positive
predictive value is lower, and false-positive results can ensue, leading to the
needless use of animals in subsequent studies that build upon the incorrect
results28. Studies with an inadequate sample size may also provide false￾negative results, where potentially important findings go undetected. For
these reasons it is crucial to report how many animals were used per group
and what statistical methods were used to determine this number.
Data handling
Common practices related to data handling that can also lead to false
positives include interim data analysis29, the ad hoc exclusion of data30,
retrospective primary end point selection31, pseudo replication32 and
small effect sizes33.
Interim data analysis
It is not uncommon for investigators to collect some data and perform
an interim data analysis. If the results are statistically significant in
favour of the working hypothesis, the study is terminated and a paper
BOX 1
A core set of reporting standards
for rigorous study design
Randomization
NAnimals should be assigned randomly to the various experimental
groups, and the method of randomization reported.
NData should be collected and processed randomly or appropriately
blocked.
Blinding
NAllocation concealment: the investigator should be unaware of the
group to which the next animal taken from a cage will be allocated.
NBlinded conduct of the experiment: animal caretakers and
investigators conducting the experiments should be blinded to the
allocation sequence.
NBlinded assessment of outcome: investigators assessing,
measuring or quantifying experimental outcomes should be
blinded to the intervention.
Sample-size estimation
NAn appropriate sample size should be computed when the study is
beingdesigned and thestatisticalmethod of computation reported.
NStatistical methods that take into account multiple evaluations of
the data should be used when an interim evaluation is carried out.
Data handling
NRules for stopping data collection should be defined in advance.
NCriteria for inclusion and exclusion of data should be established
prospectively.
NHow outliers will be defined and handled should be decided when
the experiment is being designed, and any data removed before
analysis should be reported.
NThe primary end point should be prospectively selected. If multiple
end points are to be assessed, then appropriate statistical
corrections should be applied.
NInvestigators should report on data missing because of attrition or
exclusion.
NPseudo replicate issues need to be considered during study design
and analysis.
NInvestigators should report how often a particular experiment was
performed and whether results were substantiated by repetition
under a range of conditions.
RESEARCH PERSPECTIVE
188 | NATURE | VOL 490 | 11 OCTOBER 2012
©2012 Macmillan Publishers Limited. All rights reserved

is written. If the results look ‘promising’ but are not statistically signifi￾cant, additional data are collected. This has been referred to as ‘sampling
to a foregone conclusion’ and can lead to a high rate of false-positive
findings29,30. Therefore, sample size and rules for stopping data collec￾tion should be defined in advance and properly reported. Unplanned
interim analyses, which can inflate false-positive outcomes and require
unblinding of the allocation code, should be avoided. If there are interim
analyses, however, these should be reported in the publication.
Ad hoc exclusion of data
Animal studies are often complex and outliers are not unusual.
Decisions to include or exclude specific animals on the basis of outcomes
(for example, state of health, dissimilarity to other data) have the poten￾tial to influence the study results. Thus, rules for inclusion and exclusion
of data should be defined prospectively and reported. It is also important
to report whether all animals that were entered into the experiment
actually completed it, or whether they were removed, and if so, for what
reason. Differential attrition between groups can introduce bias. For
example, a treatment may appear effective if it kills off the weakest or
most severely affected animals whose fates are then not reported. In
addition, it is important to report whether any data were removed before
analysis and the reasons for this data exclusion.
Retrospective primary end-point selection
It is well known that assessment of multiple end points, and/or assess￾ment of a single end point at multiple time points, inflates the type-I
error (false-positive results)31. Yet it is not uncommon for investigators
to select a primary end point only after data analyses. False-positive
conclusions arising from such practices can be avoided by specifying a
primary end point before the study is undertaken, the time(s) at which
the end point will be assessed, and the method(s) of analysis. Significant
findings for secondary end points can and should be reported, but
should be delineated as exploratory in nature. If multiple end points
are to be assessed, then appropriate statistical corrections should be
applied to control type-I error, such as Bonferroni corrections31,34.
Pseudo replicates
When considering sample-size determination and experimental design,
pseudo-replication issues need to be considered32. There is a clear, but
often misunderstood or misrepresented, distinction between technical
and biologic replicates. For example, in analysing effects of pollutants on
reproductive health, multiple sampling from a litter, regardless of
how many littermates are quantified, provides data from only a single
biologic replicate. When biologic variation in response to some interven￾tion is the variable of interest, as in many animal experiments, analysis of
samples from multiple litters is essential. The unit of assessment is the
smallest unit (animal, cage, litter) to which the intervention in question
can be independently administered35.
Small effect sizes
A statistically significant result does not provide information on the
magnitude of the effect and thus does not necessarily mean that the
effect is robust, which could accountfor the poor reproducibility of certain
studies36. Therefore, reporting whether results were substantiated by repe￾tition, preferably under a range of conditions that demonstrate the robust￾ness of the effect is encouraged. Also, reporting how often the particular
experiment was performed as a means to control for a general tendency to
publish only the best results would strengthen the validity of experimental
results. To this end, carefully designed and powered animal studies should
be budgeted for in the grant applications and funding agencies should
consider supporting repetition studies where appropriate.
An important note about exploratory experiments
For the most part, these best practices do not apply to early-stage obser￾vational experiments searching for possible differences among experi￾mental groups. Such exploratory testing is frequently conducted using a
small sample size, does not have a primary outcome and is often
unblinded. However, because such experiments are likely to be subject
to many of the limitations described above, they should be viewed as
hypothesis-generating experiments and interpreted as such. Potential
discoveries arising from the exploratory phase of the research should be
supported by follow-up, hypothesis-testing experiments that take into
consideration and adequately report on the core standards detailed
above (Box 1).
The path to implementation
Improving the transparency and quality of reporting cannot be achieved
by a single party, but will require cooperation among all stakeholders,
including investigators, reviewers, funding agencies and journals.
Calling upon investigators to provide key information about the design,
execution and analysis of animal experiments described in grant appli￾cations and manuscripts and encouraging reviewers to consider these
issues in their evaluations should, over time, increase both the quality
and predictive value of preclinical research. Potential strategies for
achieving this goal can be adopted from the clinical trials community,
which also contended with poor reporting and associated bias. Evidence
that clinical trials can yield biased results if they lack methodological
rigor37–42 led to the development and implementation of the CONSORT
guidelines for randomized clinical trials (among other guidelines), now
adopted by many clinical journals and funding organizations. These
guidelines require that authors report whether and how their studies were
carried out blind and randomized, how sample size was determined,
whether data are missing owing to attrition or exclusion, and supply
information about other important experimental parameters43–45.
Importantly, the guidelines have improved the transparency of clinical
study reporting in journals that have adopted them46–49. Additional evid￾ence for the power of such guidelines can be deduced from the obser￾vation that, although few animal studies report on randomization,
blinding or sample-size determination, most describe compliance with
animal regulations, which is required by journals6,9,10,50,51.
As a first step, we recommend that funding organizations and journals
provide reviewers with clear guidance about core features of animal study
design (listed in Box 1). The goal is not to be prescriptive or proscrip￾tive, but rather to delineate the minimum set of standards that should
routinely be considered in evaluating the appropriateness of a study. Such
guidance would make the task easier for reviewers of manuscripts and
grant applications who volunteer their time and are often overextended.
In addition, investigators and reviewers should be encouraged to consult
published generic and model-specific guidelines for designing in vivo
animal experiments6,21–27,52,53. To assist reviewers, editors and funding
organizations in making sure that applications and manuscripts contain
sufficient information on the core reporting recommendation (Box 1),
authors could be asked to append relevant information on a standardized
form that accompanies the submission. This form could be as simple as a
checkbox indicating the page on which the key reporting standard is
addressed. Such a form is currently used by clinical research journals.
In addition to the measures proposed above, better dissemination
of knowledge will be greatly facilitated by addressing publication bias,
the phenomenon that few studies showing negative outcomes are
published54–63. Such deficiency in reporting contributes to needless repe￾tition of similar studies by investigators unaware of earlier efforts60,61.
There is a widely accepted belief that the scientific community, promo￾tions committees, funding agencies and journals favour positive out￾comes, an impression that can lead to bias64. Possible solutions include
incentivizing investigators to publish negative outcomes, supporting
studies of independent replication, encouraging journals to publish a
greater number of studies reporting negative outcomes, creating a data￾base for negative outcomes (analogous to http://ClinicalTrials.gov/),
and linking the raw data to publications.
Change will not occur overnight. The importance of training scientists
to properly design and adequately report animal studies cannot be over￾stated. Training and education focused on key features of experimental
PERSPECTIVE RESEARCH
11 OCTOBER 2012 | VOL 490 | NATURE | 189
©2012 Macmillan Publishers Limited. All rights reserved

design should be an ongoing process for both the novice and veteran
involved in biomedical research. Attention to better study design reporting
should be communicated at major meetings, brought to the attention of
reviewers, editors and funders, required by the publishers of peer-review
journals, and included in the training program of graduate and postdoc￾toral students. Furthermore, good mentorship is crucial for developing
such skills and should be encouraged and rewarded. Rigorous experi￾mental design and adequate reporting needs to be emphasized across the
board and monitored in training grants awarded by the US National
Institute of Health (NIH) and other funding agencies. Professional soci￾eties can also have an important role by highlighting this issue in their
respective communities.
An important gatekeeper of quality remains the peer review of grant
applications and journal manuscripts. We therefore call upon funding
agencies and publishing groups to take actions to reinforce the import￾ance of methodological rigor and reporting. NINDS has begun taking
steps to promote best practices for preclinical therapy development
studies. In 2011, a Notice was published in the NIH Guide encouraging
the scientific community to address the issues described above in their
grant applications, in describing both the project being proposed and the
supporting data upon which it is based (http://grants.nih.gov/grants/
guide/notice-files/NOT-NS-11-023.html). Points that should be consid￾ered in a well-designed study are listed on the NINDS website (http://
www.ninds.nih.gov/funding/transparency_in_reporting_guidance.pdf).
Furthermore, the reviewers of applications reviewed by the NINDS
Scientific Review Branch are reminded of these issues and asked to pay
careful attention to the scientific premise of the proposed projects.
We believe that improving how animal studies are reported will raise
awareness of the importance of rigorous study design. Such increased
awareness will accelerate both scientific progress and the development
of new therapies.
Received 21 August; accepted 10 September 2012.
1. Begley, C. G. & Ellis, L. M. Raise standards for preclinical cancer research. Nature
483, 531–533 (2012).
2. Hess, K. R. Statistical design considerations in animal studies published recently in
Cancer Research. Cancer Res. 71, 625 (2011).
3. Kilkenny, C. et al. Survey of the quality of experimental design, statistical analysis
and reporting of research using animals. PLoS ONE 4, e7824 (2009).
4. Moher, D., Simera, I., Schulz, K. F., Hoey, J. & Altman, D. G. Helping editors, peer
reviewers and authors improve the clarity, completeness and transparency of
reporting health research. BMC Med. 6, 13 (2008).
5. Prinz, F., Schlange, T. & Asadullah, K. Believe it or not: how much can we rely on
published data on potential drug targets? Nature Rev. Drug Discov. 10, 712 (2011).
The first report that many published studies cannot be reproduced by the
pharmaceutical industry.
6. Sena, E., van der Worp, H. B., Howells, D. & Macleod, M. How can we improve the
pre-clinical development of drugs for stroke? Trends Neurosci. 30, 433–439
(2007).
7. Steward, O., Popovich, P. G., Dietrich, W. D. & Kleitman, N. Replication and
reproducibility in spinal cord injury research. Exp. Neurol. 233, 597–605 (2012).
8. van der Worp, H. B. & Macleod, M. R. Preclinical studies of human disease: time to
take methodological quality seriously. J. Mol. Cell. Cardiol. 51, 449–450 (2011).
9. Hackam, D. G. & Redelmeier, D. A. Translation of research evidence from animals
to humans. J. Am. Med. Assoc. 296, 1727–1732 (2006).
A study reporting that a large fraction of high-impact publications in highly
reputable journals lack important information related to experimental design.
10. Macleod, M. R. et al. Evidence for the efficacy of NXY-059 in experimental focal
cerebral ischaemia is confounded by study quality. Stroke39, 2824–2829 (2008).
A study demonstrating that lack of reporting of key methodological parameters
is associated with bias.
11. Bebarta, V., Luyten, D. & Heard, K. Emergency medicine animal research: does use
of randomization and blinding affect the results? Acad. Emerg. Med. 10, 684–687
(2003).
12. Crossley, N. A. et al. Empirical evidence of bias in the design of experimental stroke
studies – A metaepidemiologic approach. Stroke 39, 929–934 (2008).
13. Rooke, E. D., Vesterinen, H. M., Sena, E. S., Egan, K. J. & Macleod, M. R. Dopamine
agonists in animal models of Parkinson’s disease: a systematic review and meta￾analysis. Parkinsonism Relat. Disord. 17, 313–320 (2011).
14. Vesterinen, H.M. et al. Improving the translational hit of experimental treatments in
multiple sclerosis. Mult. Scler. J. 16, 1044–1055 (2010).
15. Stroke Therapy Academic Industry Roundtable (STAIR). Recommendations for
standards regarding preclinical neuroprotective and restorative drug
development. Stroke 30, 2752–2758 (1999).
16. Fanelli, D. ‘‘Positive’’ results increase down the hierarchy of the sciences. PLoS ONE
5, e10068 (2010).
17. Jerndal, M. et al. A systematic review and meta-analysis of erythropoietin in
experimental stroke. J. Cereb. Blood Flow Metab. 30, 961–968 (2010).
18. Macleod, M. R., O’Collins, T., Horky, L. L., Howells, D. W. & Donnan, G. A. Systematic
review and metaanalysis of the efficacy of FK506 in experimental stroke. J. Cereb.
Blood Flow Metab. 25, 713–721 (2005).
19. Sena, E. S. et al. Factors affecting the apparent efficacy and safety of tissue
plasminogen activator in thrombotic occlusion models of stroke: systematic
review and meta-analysis. J. Cereb. Blood Flow Metab. 30, 1905–1913 (2010).
20. Wheble, P. C. R., Sena, E. S. & Macleod, M. R. A systematic review andmeta-analysis
of the efficacy of piracetam and piracetam-like compounds in experimental
stroke. Cerebrovasc. Dis. 25, 5–11 (2008).
21. Festing, M. F. & Altman, D. G. Guidelines for the design and statistical analysis of
experiments using laboratory animals. ILAR J. 43, 244–258 (2002).
22. Kilkenny, C., Browne, W. J., Cuthill, I. C., Emerson, M. & Altman, D. G. Improving
bioscience research reporting: the ARRIVE guidelines for reporting animal
research. PLoS Biol. 8, e1000412 (2010).
23. van der Worp, H. B. et al. Can animal models of disease reliably inform human
studies? PLoS Med. 7, e1000245 (2010).
24. Fisher, M. et al. Update of the stroke therapy academic industry roundtable
preclinical recommendations. Stroke 40, 2244–2250 (2009).
25. Ludolph, A. C. et al. Guidelines for preclinical animal research in ALS/MND: a
consensus meeting. Amyotroph. Lateral Scler. 11, 38–45 (2010).
26. Shineman, D. W. et al. Accelerating drug discovery for Alzheimer’s disease: best
practices for preclinical animal studies. Alzheimers Res. Ther. 3, 28 (2011).
27. Unger, E. F. All is not well in the world of translational research. J. Am. Coll. Cardiol.
50, 738–740 (2007).
28. Ioannidis, J. P. A. Why most published research findings are false. PLoS Med. 2,
e124 (2005).
29. Dienes, Z. Bayesian versus orthodox statistics: which side are you on? Perspect.
Psychol. Sci. 6, 274–290 (2011).
30. Simmons, J. P., Nelson, L. D. & Simonsohn, U. False-positive psychology:
undisclosed flexibility in data collection and analysis allows presenting anything as
significant. Psychol. Sci. 22, 1359–1366 (2011).
31. Beal, K. G. & Khamis, H. J. A problem in statistical-analysis: simultaneous
inference. Condor 93, 1023–1025 (1991).
32. Lazic, S. E. The problem of pseudoreplication in neuroscientific studies: is it
affecting your analysis? BMC Neurosci. 11, 5 (2010).
33. Scott, S. et al. Design, power, and interpretation of studies in the standard murine
model of ALS. Amyotroph. Lateral Scler. 9, 4–15 (2008).
An enlightening analysis of how small sample sizes can lead to false-positive
outcomes.
34. Proschan, M. A. & Waclawiw, M. A. Practical guidelines for multiplicity adjustment
in clinical trials. Control. Clin. Trials 21, 527–539 (2000).
35. Festing, M. F. W. Design and statistical methods in studies using animal models of
development. ILAR J. 47, 5–14 (2006).
36. Nakagawa, S. & Cuthill, I. C. Effect size, confidence interval and statistical
significance: a practical guide for biologists. Biol. Rev. Camb. Philos. Soc. 82,
591–605 (2007).
37. Chalmers, T. C., Celano, P., Sacks, H. S. & Smith, H. Bias in treatment assignment in
controlled clinical-trials. N. Engl. J. Med. 309, 1358–1361 (1983).
38. Ju¨ni, P., Altman, D. G. & Egger, M. Systematic reviews in health care - assessing the
quality of controlled clinical trials. Br. Med. J. 323, 42 (2001).
39. Pildal, J. et al. Impact of allocation concealment on conclusions drawn from meta￾analyses of randomized trials. Int. J. Epidemiol. 36, 847–857 (2007).
40. Pocock, S. J., Hughes, M. D. & Lee, R. J. Statistical problems in the reporting of
clinical-trials. A survey of three medical journals. N. Engl. J. Med. 317, 426–432
(1987).
41. Schulz, K. F., Chalmers, I., Hayes, R. J. & Altman, D. G. Empirical evidence of bias.
Dimensions of methodological quality associated with estimates of treatment
effects in controlled trials. J. Am. Med. Assoc. 273, 408–412 (1995).
42. Wood, L. et al. Empirical evidence of bias in treatment effect estimates in controlled
trials with different interventions and outcomes: meta-epidemiological study. Br.
Med. J. 336, 601–605 (2008).
43. Moher, D. CONSORT 2010 Explanation and Elaboration: updated guidelines for
reporting parallel group randomised trials. Br. Med. J. 340, c869 (2011).
44. Moher, D., Schulz, K. F. & Altman, D. G. The CONSORT statement: revised
recommendations for improving the quality of reports of parallel-group
randomised trials. Lancet 357, 1191–1194 (2001).
Revision of guidelines by the CONSORT group to improve the reporting of
randomized clinical trials.
45. Schulz, K. F., Altman, D. G. & Moher, D. CONSORT 2010 statement: updated
guidelines for reporting parallel group randomised trials. PLoS Med. 7, e1000251
(2010).
46. Plint, A. C. et al. Does the CONSORT checklist improve the quality of reports of
randomised controlled trials? A systematic review. Med. J. Aust. 185, 263–267
(2006).
47. Kane, R. L., Wang, J. & Garrard, J. Reporting in randomized clinical trials improved
after adoption of the CONSORT statement. J. Clin. Epidemiol. 60, 241–249 (2007).
48. Prady, S. L., Richmond, S. J., Morton, V. M. & Macpherson, H. A systematic
evaluation of the impact of STRICTA and CONSORT recommendations on quality
of reporting for acupuncture trials. PLoS ONE 3, e1577 (2008).
49. Smith, B. A. et al. Quality of reporting randomized controlled trials (RCTs) in
nursing literature: application of the consolidated standards reporting trials
(CONSORT). Nurs. Outlook 56, 31–37 (2008).
50. Macleod, M. R., O’Collins, T., Howells, D. W. & Donnan, G. A. Pooling of animal
experimental data reveals influence of study design and publication bias. Stroke
35, 1203–1208 (2004).
RESEARCH PERSPECTIVE
190 | NATURE | VOL 490 | 11 OCTOBER 2012
©2012 Macmillan Publishers Limited. All rights reserved

51. Macleod, M. R., O’Collins, T., Horky, L. L., Howells, D. W. & Donnan, G. A. Systematic
review and meta-analysis of the efficacy of melatonin in experimental stroke.
J. Pineal Res. 38, 35–41 (2005).
52. Gallo, J. M. Pharmacokinetic/pharmacodynamic-driven drug development. Mt.
Sinai J. Med. 77, 381–388 (2010).
53. Moher, D. et al. Describing reporting guidelines for health research: a systematic
review. J. Clin. Epidemiol. 64, 718–742 (2011).
54. Callaham, M. L., Wears, R. L., Weber, E. J., Barton, C. & Young, G. Positive-outcome
bias and other limitations in the outcome of research abstracts submitted to a
scientific meeting. J. Am. Med. Assoc. 280, 254–257 (1998).
55. Dickersin, K. & Chalmers, I. Recognizing, investigation and dealing with incomplete
and biased reporting of clinical research: from Francis Bacon to the WHO. J. R. Soc.
Med. 104, 532–538 (2011).
56. Fanelli, D. Negative results are disappearing from most disciplines and countries.
Scientometrics 90, 891–904 (2012).
57. Kyzas, P. A., Denaxa-Kyza, D. & Ioannidis, J. P. A. Almost all articles on cancer
prognostic markers report statistically significant results. Eur. J. Cancer 43,
2559–2579 (2007).
58. Liu, S. Dealing with publication bias in translational stroke research. J. Exp. Stroke
Transl. Med. 2, 16–21 (2009).
59. Rockwell, S., Kimler, B. E. & Moulder, J. E. Publishing negative results: the problem
of publication bias. Radiat. Res. 165, 623–625 (2006).
60. Rosenthal, R. The file drawer problem and tolerance for null results. Psychol. Bull.
86, 638–641 (1979).
61. Sterling, T. D. Publication decisions and their possible effects on inferences drawn
from tests of significance—or vice versa. J. Am. Stat. Assoc. 54, 30–34 (1959).
62. Song, F. et al. Dissemination and publication of research findings: an updated
review of related biases. Health Technol. Assess. 14, 1–220 (2010).
63. Sena, E. S., van der Worp, H. B., Bath, P. M. W., Howells, D. W. & Macleod, M. R.
Publication bias in reports of animal stroke studies leads to major overstatement
of efficacy. PLoS Biol. 8, e1000344 (2010).
64. Fanelli, D. Do pressures to publish increase scientists’ bias? An empirical support
from US states data. PLoS ONE 5, e10271 (2010).
Acknowledgements Funded by NINDS.
Author Contributions R.F., A.K.G., S.C.L., J.D.P., S.D.S., U.U. and W.K. organized the
workshop. R.B.D., S.E.L., S.C.L., M.R.M. and S.D.S. wrote the manuscript. All authors
participated in the workshop and contributed to the editing of the manuscript.
Author Information Reprints and permissions information is available at
www.nature.com/reprints. The authors declare no competing financial interests.
Readers are welcome to comment on the online version of the paper. Correspondence
and requests for materials should be addressed to S.D.S. (silberbs@ninds.nih.gov).
PERSPECTIVE RESEARCH
11 OCTOBER 2012 | VOL 490 | NATURE | 191
©2012 Macmillan Publishers Limited. All rights reserved

