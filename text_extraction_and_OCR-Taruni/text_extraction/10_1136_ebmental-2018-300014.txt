King’s Research Portal
DOI:
10.1136/ebmental-2018-300014
Document Version
Peer reviewed version
Link to publication record in King's Research Portal
Citation for published version (APA):
Fusar-Poli, P., & Radua, J. (2018). Ten simple rules for conducting umbrella reviews. Evidence-Based Mental
Health. Advance online publication. https://doi.org/10.1136/ebmental-2018-300014
Citing this paper
Please note that where the full-text provided on King's Research Portal is the Author Accepted Manuscript or Post-Print version this may
differ from the final Published version. If citing, it is advised that you check and use the publisher's definitive version for pagination,
volume/issue, and date of publication details. And where the final published version is provided on the Research Portal, if citing you are
again advised to check the publisher's website for any subsequent corrections.
General rights
Copyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright
owners and it is a condition of accessing publications that users recognize and abide by the legal requirements associated with these rights.
•Users may download and print one copy of any publication from the Research Portal for the purpose of private study or research.
•You may not further distribute the material or use it for any profit-making activity or commercial gain
•You may freely distribute the URL identifying the publication in the Research Portal
Take down policy
If you believe that this document breaches copyright please contact librarypure@kcl.ac.uk providing details, and we will remove access to
the work immediately and investigate your claim.
Download date: 03. Nov. 2025

Page 1
TEN SIMPLE RULES FOR CONDUCTING UMBRELLA REVIEWS
Paolo Fusar-Poli, MD, PhD1,2,3*, and Joaquim Radua, MD, Bstat, PhD1,4,5
Affiliations
1. Early Psychosis: Interventions and Clinical Detection (EPIC) lab, Department of Psychosis Studies, 
Institute of Psychiatry, Psychology & Neuroscience, King's College London, London, United 
Kingdom;
2. OASIS service, South London and Maudsley NHS Foundation Trust, London, United Kingdom;
3. Department of Brain and Behavioral Sciences, University of Pavia, Pavia, Italy;
4. FIDMAG Germanes Hospitalaries, CIBERSAM, Saint Boi de Llobregat, Barcelona, Spain;
5. Centre for Psychiatry Research, Department of Clinical Neuroscience, Karolinska Institute, 
Stockholm, Sweden.
Abstract: 223
Text: 3865
Tables: 2
Figures: 3
Endnote reference file: Umbrella_EBMH.enl
Correspondence to: Dr Paolo Fusar-Poli MD PhD, Department of Psychosis Studies, 5th Floor, 
Institute of Psychiatry, Psychology & Neuroscience, PO63, 16 De Crespigny Park, SE5 8AF London, 
UK. E-mail: paolo.fusar-poli@kcl.ac.uk
Acknowledgements: This work is supported by a King’s College London Confidence in Concept 
award (MC_PC_16048) from the Medical Research Council (MRC) to PFP and a Miguel Servet 
Research Contract (MS14/00041) to JR from the Instituto de Salud Carlos III and the European 
Regional Development Fund (FEDER).

Page 2
ABSTRACT
Objective
Evidence syntheses such as systematic reviews and meta-analyses provide a rigorous and 
transparent knowledge base for translating clinical research into decisions, and thus they represent the 
basic unit of knowledge in medicine. Umbrella reviews are reviews of previously published systematic 
reviews or meta-analyses. Therefore, they represent one of the highest levels of evidence synthesis 
currently available, and are becoming increasingly influential in biomedical literature. However, 
practical guidance on how to conduct umbrella reviews is relatively limited. 
Methods
We present a critical educational review of published umbrella reviews, focusing on the essential 
practical steps required to produce robust umbrella reviews in the medical field.
Results
The current manuscript discusses ten key points to consider for conducting robust umbrella reviews.
The points are: ensure that the umbrella review is really needed, pre-specify the protocol, clearly define 
the variables of interest, estimate a common effect size, report the heterogeneity and potential biases, 
perform a stratification of the evidence, conduct sensitivity analyses, report transparent results, use 
appropriate software and acknowledge the limitations. We illustrate these points through recent 
examples from umbrella reviews and suggest specific practical recommendations.
Conclusions
The current manuscript provides a practical guidance for conducting umbrella reviews in medical areas. 
Researchers, clinicians and policy makers might use the key points illustrated here to inform the 
planning, conduction and reporting of umbrella reviews in medicine.

Page 3
INTRODUCTION
Medical knowledge traditionally differs from other domains of human culture by its progressive nature, 
with clear standards or criteria for identifying improvements and advances. Evidence based synthesis 
methods are traditionally thought to meet these standards. They can be thought of as the basic unit of 
knowledge in medicine, and allow making sense of several and often contrasting findings, which is 
crucial to advance clinical knowledge. In fact, clinicians accessing international databases such as 
PubMed to find the best evidence on a determinate topic may soon feel overwhelmed with too many 
findings, often contradictory and not replicating each other. Some authors have argued that biomedical
science1 suffers from a serious replication crisis2
, to the point that scientifically, replication becomes 
equally as -or even more- important than discovery3
. For example, extensive research has investigated 
the factors that may be associated with an increased (risk factors) or decreased (protective factors) 
likelihood of developing serious mental disorders such as psychosis. Despite several decades of 
research, results have been inconclusive because published findings have been conflicting and affected 
by several types of biases (for a detailed discussion on biases see4
). Systematic reviews and meta￾analyses aim to synthetize the findings and investigate the biases. However, as the number of reviews 
of meta-analyses also increased, clinicians may also feel overwhelmed with too many of them.
Umbrella reviews have been developed to overcome such a gap of knowledge. They are reviews of 
previously published systematic reviews or meta-analyses, and consist in the repetition of the meta￾analyses following a uniform approach for all factors to allow their comparison5
. Therefore, they 
represent one of the highest levels of evidence synthesis currently available (Figure 1). Not surprisingly, 
umbrella reviews are becoming increasingly influential in biomedical literature. This is empirically
confirmed by the proliferation of this type of studies over the recent years. In fact, by searching 
“umbrella review” in the titles of articles published on Web of Knowledge (up to April 1st
, 2018), we 
found a substantial increase in the number of umbrella reviews published over the past decade, as 
detailed in Figure 2. The umbrella reviews identified through our literature search were investigating a 
wide portion of medical branches (Figure 3). Furthermore, several protocols of upcoming umbrella 
reviews have been recently published, confirming the exponential trend6-12. 
However, guidance on how to conduct or report umbrella reviews is relatively limited5
. The current 
manuscript addresses this area by providing practical tips for conducting good umbrella reviews in 
medical areas. Rather than being an exhaustive primer on the methodological underpinning of umbrella 
reviews, we only highlight ten key points that to our opinion are essential for conducting robust 
umbrella reviews. As reference example, we will use an umbrella review on risk and protective factors 
for psychotic disorders recently completed by our group13. However, we generalize the considerations
presented in this manuscript and the relative recommendations to any other area of medical knowledge.
METHODS

Page 4
Educational and critical (non-systematic) review of the literature focusing on key practical issues that 
are necessary for conducting and reporting robust umbrella reviews. The authors selected illustrative 
umbrella reviews to highlight key methodological findings. In the results, we present ten simple key 
points that the authors of umbrella reviews should carefully address when planning and conducting 
umbrella reviews in the medical field.
RESULTS
1. Ensure that the umbrella review is really needed
The decision to develop a new umbrella review in medical areas of knowledge should be stimulated by 
several factors. For example, the topic of interest may be highly controversial or it may be affected by 
potential biases that have not been investigated systematically. The authors can explore these issues in 
the existing literature. e.g., they may want to survey and identify a few examples of meta-analyses on 
the same topic that present contrasting findings. Second, a clear link between the need to address 
uncertainty and advancing clinical knowledge should be identified a-priori, and acknowledged as the 
strong rationale for conducting an umbrella review. For example, in our previous work we speculated 
that by clarifying the evidence for an association between risk or protective factors and psychotic 
disorders we could improve our ability to identify those at risk of developing psychosis13. Clearly, 
improving the detection of individuals at risk is the first step towards the implementation of preventive 
approaches, which are becoming a cornerstone of medicine14-16. Third, provided that the two points are 
satisfactory, it is essential to check whether there are enough meta-analyses that address a determinate 
topic17. Larger databases can increase the statistical power and therefore improve accurateness of the 
estimates and interpretability of the results. Furthermore, they are also likely to reflect a topic of wider 
interest and impact for clinical practice. These considerations are of particular relevance when
considering the mass production of useless evidence synthesis studies that are redundant, not necessary 
and addressing clinically irrelevant outcomes18.
2. Pre-specify the protocol 
As for any other evidence synthesis approach, it is essential to prepare a study protocol ahead of 
initiating the work and upload it to international databases such as PROSPERO 
(https://www.crd.york.ac.uk/PROSPERO/). The authors may also publish the protocol in an open￾access journal, as it is common for randomized controlled trials. The protocol should clearly define the 
methods for reviewing the literature and extracting data and the statistical analysis plan. Importantly, 
specific inclusion and exclusion criteria should be pre-specified. For example, inclusion criteria from 
our umbrella review13 were: (a) systematic reviews or meta-analyses of individual observational studies 
(case-control, cohort, cross-sectional and ecological studies) that examined the association between risk 

Page 5
or protective factors and psychotic disorders; (b) studies considering any established diagnosis of non￾organic psychotic disorders defined by the International Classification of Disease (ICD) or the 
Diagnostic and Statistical Manual of Mental Disorders (DSM); (c) inclusion of a comparison group of 
non-psychotic healthy controls, as defined by each study; and (d) studies reporting enough data to 
perform the analyses. Similarly, the reporting of the literature search should adhere to the Preferred 
Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) recommendations19 and
additional specific guidelines depending on the nature of the studies included (e.g.in case of 
observational studies, the Meta-analysis of Observational Studies in Epidemiology (MOOSE) 
guidelines20). Quality assessment of the included studies is traditionally required in evidence synthesis 
studies. In the absence of specific guidelines for quality assessment in umbrella reviews, the AMSTAR 
(A Measurement Tool to Assess Systematic Reviews), a validated instrument21,22 can be used. 
3. Clearly define the variables of interest
Umbrella reviews are traditionally conducted to measure the association between certain factors and a 
determinate clinical outcome. The first relevant point to conducting a good umbrella review is therefore 
to define consistent and reliable factors and outcomes to be analyzed. 
The definition of the type of factor (e.g., risk factor or biomarker) of interest may be particularly 
challenging. For example, in our review we found that childhood trauma was considered as a common 
risk factor for psychosis13, but available literature lacked standard operationalisation. Our pragmatic 
approach was to define the factors as each meta-analysis or systematic review had defined them. 
Another issue relates to whether and how analysts should group similar factors. For example, in our 
umbrella review13 we wondered whether to merge the 1st generation immigrant and 2nd generation 
immigrant risk factors for psychosis in a unique category of “immigrants”. However, this would have 
introduced newly defined categories of risk factors that were not available in the underlying literature. 
Our solution was not to combine similar factors if the meta-analysis or systematic review had
considered and analyzed them separately. Similarly, it may be important not to split categories into 
subgroups (e.g., childhood sexual abuse, emotional neglect, physical abuse) if the meta-analysis or 
systematic review had considered them as a whole (e.g., childhood trauma). The additional problem 
may be that a meta-analysis or a systematic review could report both results, i.e., pooled across 
categories and divided according to specific subgroups. In this case, it is important to define a-priori 
what kind of results is to be used. Pooled results may be preferred since they usually include larger 
sample sizes. Restricting the analyses to only the factors that each individual meta-analysis or 
systematic review had originally introduced may mitigate the risk of introducing newly defined factors 
not originally present in the literature. Such an approach is also advantageous to minimize the risk of 
artificially inflating the sample size by creating large and unpublished factors, therefore biasing the 
hierarchical classification of the evidence. Finally, there may be two meta-analyses or systematic 
reviews that address the same factor or that include individual studies that are overlapping. In our 

Page 6
previous umbrella review we selected the meta-analysis or systematic review with the largest database 
and the most recent one13.
A collateral challenge in this domain may relate to the type of factors that analysts should exclude. For 
example, in our previous umbrella review13 we decided to focus on risk and protective factors for 
psychosis only, and not on biomarkers. However, in the lack of clear etiopathogenic mechanisms for
the onset of psychosis, the boundaries between biomarkers collected before the onset of the disorder 
and risk and protective factors were not always clear. To solve this problem, we have again adopted a 
pragmatic approach, adopting the definitions of risk and protective factors vs biomarker as provided by 
each article included in the umbrella review. A further point is that if systematic reviews are included, 
some of them may not have performed quantitative data on specific factors. 
The additional challenge would be that individual meta-analyses or systematic reviews might have 
similar but not identical definition of these outcomes. For example, we intended to investigate only 
psychotic disorders defined by standard international validated diagnostic manuals such as the 
International Classification of Disease (ICD) or the Diagnostic and Statistical Manual of Mental 
Disorders (DSM). We found that some meta-analyses that were apparently investigating psychotic 
disorders in realty did also include studies that were measuring psychotic symptoms not officially coded 
in these manuals13. To overcome this problem, we took the decision to verify the same inclusion and 
exclusion criteria that were used for reviewing the literature (e.g., inclusion of DSM or ICD psychotic 
disorders) for each individual study included in every eligible meta-analysis or systematic review13.
Such a process is extremely time-consuming and analysts should account for it during the early planning 
stages to ensure sufficient resources are in place. The authors of an umbrella review may also 
exclusively rely on the information provided in the systematic reviews and meta-analyses, though in 
that case the analysts should clearly acknowledge its potential limitations in the text. Alternatively, they 
may rely on the systematic reviews and meta-analyses to conduct a pre-selection of the factors with a 
greater level of evidence, and then verify the data for each individual study of these (much fewer) 
factors.
4. Estimate a common effect size
The systematic reviews and meta-analyses use different measures of effect size depending on the design 
and analytical approach of the studies that they review. For example, meta-analyses of case-control 
studies may use standardized mean differences such as the Hedge’s g to compare continuous variables,
and odds ratios (OR) to compare binary variables. Similarly, meta-analyses of cohort studies comparing 
incidences between exposed and non-exposed may use a ratio of incidences such as the incidence rate 
ratio (IRR). In addition, other measures of effect size are possible. The use of these different measures 
of effect size is enriching because each of them is appropriate for a type of studies, and thus we 
recommend also using them in the umbrella review. For example, a hazard ratio may be very appropriate 

Page 7
for summarizing a survival analysis, whilst it would be hard to interpret in a cross-sectional study, 
ultimately preventing the readers from easily getting a glimpse of the current evidence.
However, one of the main aims of an umbrella is also to allow a comparison of the size of the effects 
across all factors investigated, and the use of a common effect size for all factors clearly makes this 
comparison straightforward. For example, in our previous umbrella review of risk and protective factors
for psychosis, we found that the effect size of parental communication deviance (a vague, fragmented 
and contradictory intra-familial communication) was Hedge’s g=1.35, whereas the effect size of heavy 
cannabis use was OR=5.1713. Which of these factors had a larger effect? To allow a straightforward 
comparison, we converted all effect sizes to OR, and the equivalent OR of parental communication 
deviance was 11.55. Thus, reporting an equivalent OR for each factor, the readers can straightforwardly 
compare the factors and conclude that the effect size of parental communication deviance is 
substantially larger than the effect size of heavy cannabis use. To further facilitate the comparison of 
factors, the analysts may even force all equivalent OR to be greater than one (i.e., inverting any OR < 
1). For example, in our previous umbrella review, we found that the equivalent odds ratio of self￾directedness was 0.1713. The inversion of this OR would be 5.72, which the reader could 
straightforwardly compare with other equivalent ORs > 1.
An exact conversion of an effect size into an equivalent OR may not always be possible, because the 
measures of effect size may be inherently different and the calculations may need data that may be 
unavailable. For example, to convert an IRR into an OR, the analysts should first convert the IRR into 
a risk ratio (RR), and then the RR into an OR. However, an IRR and a RR have an important difference: 
the IRR accounts for the time that the researchers could follow each individual, whilst the RR only 
considers the initial sizes of the samples. In addition, even if the analysts could convert the IRR into a 
RR, they could not convert the RR into an OR without knowing the incidence in the non-exposed, which 
the papers may not report.
Fortunately, approximate conversions are relatively straightforward23 (see Table 1). On the one hand, 
the analysts may assume that hazard, incidence rate, risk and odds ratios are approximately equivalent 
as far as the incidence is not too large. Similarly, they may also assume that Cohen’s d, Glass’ Δ and
Hedge’s g are approximately equivalent as far as the variances in patients and controls are not too 
different and the sample sizes are not too small. On the other hand, the analysts can convert Hedge’s g
into equivalent OR using a standard formula23. For other measures such as the risk difference, the ratio 
of means or the mean difference, the analyst will need a few general estimations (see Table 1). In any 
case, such approximations are acceptable because the only aim of the equivalent OR is to provide a 
visual number to allow an easy comparison of the effect sizes of the different factors.
5. Report the heterogeneity and potential biases
As with single meta-analyses, an umbrella review should study and report the heterogeneity across the 
studies included in each meta-analysis and the potential biases in the studies to show a more complete 

Page 8
picture of the evidence. Independently of the effect size and the p-value, the level of evidence of an 
effect (e.g., a risk factor) is lower when there is large heterogeneity, as well as when there is potential 
reporting or excess significance bias. The presence of a large between-study heterogeneity may indicate, 
for example, that there are two groups of studies investigating two different groups of patients, and the 
results of a single meta-analysis for the two groups may not represent either of the groups. The presence 
of potential reporting bias, on the other hand, might mean that studies are only published timely in 
indexed journals if they find one type of results, e.g., if they find that a given psychotherapy works. Of 
course, if the meta-analysis only includes these studies, the results will be that the psychotherapy works, 
even if it does not. Analysts can explore the reporting bias that affects the smallest studies with a number 
of tools, such as the funnel plot, Egger and similar tests, and trim and fill methods4
. Finally, the presence 
of potential excess significance bias would mean that the number of studies with statistically significant
results is suspiciously high, and this may be related not only to reporting bias but also to other issues
such as data dredging4
.
6. Perform a stratification of evidence
A more detailed analysis of the umbrella reviews identified in our literature search revealed that some 
of them, pertaining to several clinical medical areas (neurology, oncology, nutrition medicine, internal 
medicine, psychiatry, pediatrics, dermatology, and neurosurgery) additionally stratified the evidence 
using a classification method. This classification was obtained through strict criteria, equal or similar 
to the one listed below24-26: 
• convincing (class I) when number of cases >1000, p<10−6
, I2
<50%, 95% prediction interval 
excluding the null, no small-study effects, and no excess significance bias; 
• highly suggestive (class II) when number of cases >1000, p<10−6
, largest study with a statistically 
significant effect, and class I criteria not met; 
• suggestive (class III) when number of cases >1000, p<10−3
, and class I-II criteria not met; 
• weak (class IV) when p<0.05 and class I-III criteria not met;
• non-significant when p>0.05.
We strongly recommend the use of these or similar criteria because they allow an objective, 
standardized classification of the level of evidence. However, the analysts should not forget that the 
variables used in these criteria are continuous and the set of cut-off points are only cut-off points. For 
example, the difference between a factor that includes 1000 patients and a factor that includes 1001 
patients is negligible, but according to the criteria, the former can only be class IV, whereas the latter 
could be class I.
7. Conduct (study-level) sensitivity analyses

Page 9
Depending on the type of umbrella review (e.g., risk or protective factors, biomarkers, and etcetera), a 
few sensitivity analyses may enrich the final picture. For instance, in an umbrella review of potential 
risk and protective factors, establishing the temporality of the association is critical in order to minimize 
reverse causation. This may be seen in scenarios similar to the following example: many smokers quit 
smoking after developing lung cancer, and thus a cross-sectional study could report that the prevalence 
of lung cancer is higher in ex-smokers than in smokers, and erroneously conclude that quitting smoking 
causes lung cancer. To avoid this reverse causation, studies must address the temporality, i.e., observing 
that patients first developed lung cancer and after quit smoking, rather than the other way round. In an 
umbrella review, the analysts may address temporality with a sensitivity analysis that includes only 
prospective studies. Our recent umbrella review provides an example of sensitivity analyses 
investigating temporality of association13.
8. Report transparent results
An umbrella review generates a wealth of interesting data, but the analysts should present them
adequately in order to achieve one of the main aims: to summarize clearly the evidence. This is not 
always straightforward. They may design tables or plots that report all information of interest in a 
simplified way. One approach could be, for example, including a table with the effect size (and its 
confidence interval), the equivalent OR, the features used for the classification of the level of evidence, 
and the resulting evidence. Parts of this table could be graphical, e.g., the analysts may choose to present 
the equivalent OR as a forest plot. In any case, the readers should be able to know easily the effect size 
and the degree of evidence of the factors from the tables and plots. Table 2 shows a summary of the key 
statistics that we suggest to report in any umbrella review.
9. Use appropriate software
The analysts can conduct a large part of the calculations of an umbrella review with usual meta￾analytical software, such as “meta”, “metafor" or “metansue” packages for R 27-29. Some software 
includes better estimation methods of the between-study heterogeneity than others 30-32, but this may
probably represent a minor difference. That said, we recommend that the meta-analytical software is 
complete enough to fit random-effects models, assess between-study heterogeneity, estimate prediction 
intervals and assess potential reporting bias.
However, even if using good meta-analytic software, the analysts will still have to write the code for 
some parts of the umbrella review. On the one hand, some specific computations may not be available 
in standard software, such as the estimation of the statistical power in some studies, required to evaluate 
excess significance bias. On the other hand, meta-analytic software aims to conduct and show the results 
of one meta-analysis, whereas an umbrella review may include hundreds of meta-analyses, for what the 
analysts will have to manage and show the results of all these meta-analyses as an integrated set. For 
example, to create the forest plots, the analysts may write a code that takes the results of the different 

Page 10
meta-analyses as if they were individual studies, and then calls the forest plot function of the meta￾analytic software (without displaying a pooled effect). We are developing new and free umbrella-review 
software to minimize these burdens.
10. Acknowledge its limitations
To report transparently the evidence, the analysts must adequately acknowledge the limitations of the 
umbrella review. Some limitations may be specific of a given umbrella, and others are relatively 
general. Among them, probably one of the most important issues is that umbrella reviews can only 
report what researchers have investigated, published and systematically reviewed or meta-analyzed. For 
example, a factor may have an amazingly strong effect, but if few studies have investigated the factor, 
it will probably be classified as only class IV evidence because of involving less than 1000 patients.
Indeed, if the factor were not part of any systematic review or meta-analysis, it would not be even 
included in the umbrella review. Fortunately, given the mass production of evidence synthesis studies 
it is also unlikely that a relevant area of medical knowledge is not addressed by any published systematic 
review or meta-analysis
18. On the other hand, an umbrella could include all studies published, beyond 
those included in published reviews, but this would require updating the literature search at the level of 
each sub-domain included in the umbrella review. This extra work would highly increase the already 
very high working time needed to conduct an umbrella review, to the level that most umbrella reviews 
could become unfeasible. Furthermore, it would probably involve the definition of new subgroups or 
factors that the systematic review or meta-analysis had not originally reported, making the interpretation 
of the final findings more difficult. Another issue is that the use of a systematic approach analysis would 
not allow conducting the rigorous assessment of several types of biases. Another similar limitation is 
that the umbrella review will have most of the limitations of the included studies. For instance, if the 
latter assess association but not causation, the umbrella review will assess association but not causation.
CONCLUSIONS
Umbrella reviews are becoming widely used as a means to provide one of the highest levels of evidence
in medical knowledge. Key points to be considered to conducting robust umbrella reviews are to ensure 
that they are really needed, pre-specify the protocol, clearly define the variables of interest, estimate a 
common effect size, report the heterogeneity and potential biases, perform a stratification of the 
evidence, conduct (study-level) sensitivity analyses, report transparent results, use appropriate software 
and acknowledge the limitations.

Page 11
Figure 1. Hierarchy of evidence synthesis methods
Individual)studies
Systematic)reviews
Meta4analyses
Umbrella)
reviews
1.)Select)
predictors
2.)Prepare)
data 3.)Model)
development
4.)Internal)
validation
5.)External)
validation
6.)
Implementation)
&)Impact
2 3
4 5
Holdout K4fold
Leave4
one4out K
Model)Prediction error
Model)complexity /)number of)predictors
Variance
Bias
Underfit Fit Overfit
Model)Prediction error
Model)Accuracy
Model)Accuracy
Model)complexity /)number of)
predictors
Apparent)validation
External)validation
Optimism
Internal)validation
Model
Sample
Development)
data
Bootstrap
6
Clinical)
Implementation

Page 12
Figure 2. Web of Knowledge records containing “umbrella review” in their title up to April 2018
0
5
10
15
20
25
30
35
40
2007
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018

Page 13
Figure 3. Focus of umbrella reviews published in Web of Knowledge –see Figure 2- up to April 2018

Page 14
Table 1. Possible conversions of some effect sizes to equivalent odds ratios (OR)
Conversion Justification
IRR to
RR
The following formula, straightforwardly derived from the definitions of incidence 
rate ratio (IRR) and risk ratio (RR), converts the former into the latter:
Fortunately, if the incidences are small enough, the average follow-up times are 
similar in exposed and non-exposed, the fraction in the left is approximately one 
and thus:
RD to
RR
The following formula, straightforwardly derived from the definitions of risk 
difference (RD) and risk ratio (RR), converts the former into the latter:
Thus, analysts might need an estimation of the probability of developing the 
disease (p) in the non-exposed.
RR to
OR
The following formula, straightforwardly derived from the definitions of risk ratio 
(RR) and odds ratio (OR), converts the former into the latter:
Fortunately, if the probabilities of developing the disease (p) are small enough, the 
fraction in the left is approximately one, and thus:
RoM to
MD
The following formula, straightforwardly derived from the definitions of ratio of 
means (RoM) and mean difference (MD), converts the former into the latter:
Thus, analysts might need an estimation of the mean (m) in controls.
MD to
Glass Δ
The following formula, straightforwardly derived from the definitions of mean 
difference (MD) and Glass Δ, converts the former into the latter:
Thus, analysts might need an estimation of the standard deviation (s) in controls.
Glass Δ to
Cohen’s d
The following formula, straightforwardly derived from the definitions of Glass Δ and 
Cohen’s d, converts the former into the latter:
Fortunately, if the variances (s2) in cases and controls are similar enough, the 
square root in the left is approximately one, and thus:
Hedge’s g to 
Cohen’s d
The following formula, straightforwardly derived from the definitions of Hedge’s g
and Cohen’s d, converts the former into the latter:
( )
( - )
average
average
exposed
non exposed
time
RR IRR
time = ´
RR IRR »
1 1
non-exposed
RR RD
p
= + ´
- 1
1
non exposed
exposed
p OR RR
p
- = ´ -
OR RR »
MD m RoM = controls ´ - ( 1)
1
controls
MD
s
D = ´
( ) ( )
( ) ( )
2 2
2 2
1 1
1 1
controls controls cases controls
controls controls cases cases
n sn s
d
n sn s
- × + - × = ´ D - × + - ×
d » D
( )
1 d g J df = ´

Page 15
Fortunately, if the sample sizes are large enough, the small-sample correction 
factor (J) is approximately one, the fraction in the left is approximately one and 
thus:
Pearson’s r to 
Cohen’s d
The following standard formula23 converts a Pearson’s r into an approximate 
Cohen’s d:
d g »
2
2
1
r d
r
× » -

Page 16
Table 2. Recommended elements in the summary tables or plots
Description Name of the factor (e.g., “Parental communication deviance”), optionally with a 
very few details such as a brief literally description of the factor, the number of 
studies, the number patients and controls, the time of follow-up, whether the 
studies are prospective, and etcetera.
Effect size The measure of effect size (e.g., “Hedge’s g”), the outcome and its confidence 
interval (e.g., “1.35 [CI=0.97-1.73]”), and the equivalent OR (e.g., “11.55”).
Features used for 
classification of 
evidence
These statistics may vary depending on the criteria used, and could be: the p￾value, the fraction of variance that is due to heterogeneity (I
2), the prediction 
interval, the confidence interval of the largest study, the results of an Egger test (or 
another method to assess potential small study reporting bias) and the results of 
an excess significance test. To simplify the table, the authors of the umbrella 
review may choose to report only the bits that are most important, e.g., the results 
of an Egger test could be simply “Yes” or “No”.
Evidence class Based on the features above.
Other statistics Some statistics not used for the classification of evidence but that are of great 
interest, though we suggest that the amount of information presented is limited to 
ensure that the table is simple enough.

Page 17
REFERENCES 
1. Ioannidis JPA, J. KM. Improving validation practices in omics research. Science. 
2011;334(6060):1230-1232.
2. Szucs D, A. IJP. Empirical assessment of published effect sizes and power in the 
recent cognitive neuroscience and psychology literature. PLoS Biol. 
2017;15(3):e2000797.
3. Ioannidis JPA. Evolution and Translation of Research Findings: From Bench to 
Where. PLoS Clin Trials. 2006;1(7):e36.
4. Ioannidis JP, Munafo MR, Fusar-Poli P, Nosek BA, David SP. Publication and other 
reporting biases in cognitive sciences: detection, prevalence, and prevention. Trends 
in cognitive sciences. 2014;18(5):235-241.
5. Ioannidis JP. Integration of evidence from multiple meta-analyses: A primer on 
umbrella reviews, treatment networks and multiple treatments meta-analyses. CMAJ. 
2009;181(8):488-493.
6. Elliott J, Kelly SE, Bai Z, et al. Optimal duration of dual antiplatelet therapy 
following percutaneous coronary intervention: protocol for an umbrella review. BMJ 
open. 2017;7(4):e015421.
7. Catala-Lopez F, Hutton B, Driver JA, et al. Cancer and central nervous system 
disorders: protocol for an umbrella review of systematic reviews and updated meta￾analyses of observational studies. Systematic reviews. 2017;6(1):69.
8. Dinsdale S, Azevedo LB, Shucksmith J, et al. Effectiveness of weight management, 
smoking cessation and alcohol reduction interventions in changing behaviors during 
pregnancy: an umbrella review protocol. JBI database of systematic reviews and 
implementation reports. 2016;14(10):29-47.
9. Jadczak AD, Makwana N, Luscombe-Marsh ND, Visvanathan R, Schultz TJ. 
Effectiveness of exercise interventions on physical function in community-dwelling 
frail older people: an umbrella review protocol. JBI database of systematic reviews 
and implementation reports. 2016;14(9):93-102.
10. Chai LK, Burrows T, May C, Brain K, Wong See D, Collins C. Effectiveness of 
family-based weight management interventions in childhood obesity: an umbrella 
review protocol. JBI database of systematic reviews and implementation reports. 
2016;14(9):32-39.
11. Thomson K, Bambra C, McNamara C, Huijts T, Todd A. The effects of public health 
policies on population health and health inequalities in European welfare states: 
protocol for an umbrella review. Systematic reviews. 2016;5:57.
12. Krause D, Roupas P. Dietary interventions as a neuroprotective therapy for the delay 
of the onset of cognitive decline in older adults: an umbrella review protocol. JBI 
database of systematic reviews and implementation reports. 2015;13(2):74-83.
13. Radua J, Ramella-Cravaro V, Ioannidis JPA, et al. What causes psychosis? An 
umbrella review of risk and protective factors. World Psychiatry. 2018;17(1):49-66.
14. Schmidt A, Cappucciati M, Radua J, et al. Improving Prognostic Accuracy in 
Subjects at Clinical High Risk for Psychosis: Systematic Review of Predictive Models 
and Meta-analytical Sequential Testing Simulation. Schizophr Bull. 2017;43(2):375-
388.
15. Fusar-Poli P, Cappucciati M, Bonoldi I, et al. Prognosis of Brief Psychotic Episodes: 
A Meta-analysis. JAMA Psychiatry. 2016;73(3):211-220.
16. Fusar-Poli P, Cappucciati M, Borgwardt S, et al. Heterogeneity of Psychosis Risk 
Within Individuals at Clinical High Risk A Meta-analytical Stratification. Jama 
Psychiatry. 2016;73(2):113-120.

Page 18
17. Siontis KC, Hernandez-Boussard T, Ioannidis JP. Overlapping meta-analyses on the 
same topic: survey of published studies. BMJ. 2013;347:f4501.
18. Ioannidis JP. The Mass Production of Redundant, Misleading, and Conflicted 
Systematic Reviews and Meta-analyses. Milbank Q. 2016;94(3):485-514.
19. Moher D, Liberati A, Tetzlaff J, Altman DG, Grp P. Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses: The PRISMA Statement. Journal of Clinical 
Epidemiology. 2009;62(10):1006-1012.
20. Stroup DF, Berlin JA, Morton SC, et al. Meta-analysis of observational studies in 
epidemiology - A proposal for reporting. Jama-Journal of the American Medical 
Association. 2000;283(15):2008-2012.
21. Shea BJ, Grimshaw JM, Wells GA, et al. Development of AMSTAR: a measurement 
tool to assess the methodological quality of systematic reviews. Bmc Medical 
Research Methodology. 2007;7.
22. Shea BJ, Bouter LM, Peterson J, et al. External Validation of a Measurement Tool to 
Assess Systematic Reviews (AMSTAR). PLoS One. 2007;2(12):5.
23. Chinn S. A simple method for converting an odds ratio to effect size for use in meta￾analysis. Stat Med. 2000;19(22):3127-3131.
24. Bellou V, Belbasis L, Tzoulaki I, Middletond LF, Ioannidis JPA, Evangelou E. 
Systematic evaluation of the associations between environmental risk factors and 
dementia: An umbrella review of systematic reviews and meta-analyses. Alzheimer’s 
& Dementia. 2016;1 - 13.
25. Bellou V, Belbasis L, Tzoulaki I, Evangelou E, Ioannidis JPA. Environmental risk 
factors and Parkinson's disease: An umbrella review of meta-analyses. Parkinsonism 
& Related Disorders. 2016;23:1-9.
26. Belbasis L, Bellou V, Evangelou E, Ioannidis JPA, Tzoulaki L. Environmental risk 
factors and multiple sclerosis: an umbrella review of systematic reviews and meta￾analyses. Lancet Neurology. 2015;14(3):263-273.
27. Schwarzer G. meta: An R package for meta-analysis. R News. 2007;7(3):40-45.
28. Viechtbauer W. Conducting meta-analyses in R with the metafor package. Journal of 
Statistical Software. 2010;36(3):1-48.
29. Radua J, Schmidt A, Borgwardt S, et al. Ventral Striatal Activation During Reward 
Processing in Psychosis: A Neurofunctional Meta-Analysis. JAMA Psychiatry. 
2015;72(12):1243-1251.
30. Viechtbauer W. Bias and efficiency of meta-analytic variance estimators in the 
random-effects model. Journal of Educational and Behavioral Statistics. 
2005;30(3):261-293.
31. Veroniki AA, Jackson D, Viechtbauer W, et al. Methods to estimate the between￾study variance and its uncertainty in meta-analysis. Res Synth Methods. 2016;7(1):55-
79.
32. Petropoulou M, Mavridis D. A comparison of 20 heterogeneity variance estimators in 
statistical synthesis of results from studies: a simulation study. Stat Med. 
2017;36(27):4266-4280.

