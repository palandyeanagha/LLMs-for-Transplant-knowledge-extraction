schizophrenia and bipolar disorder as two separate conditions. This 
separation is respected by drug companies, regulators, research 
funders, journals and bench researchers. Add that lot up, and you get 
a fundamental problem with psychiatry.
Next month, the American Psychiatric Association will release the 
long-awaited fifth version of its Diagnostic and Statistical Manual 
of Mental Disorders (DSM-5), which lists mental illnesses and their 
symptoms. Work on preparing the DSM-5 has been clouded in controversy, and the arguments over which conditions should have been 
included and which left out will rumble on for some time.
The more fundamental problem, as the News Feature explores, is 
growing doubt about the way the DSM-5 classifies mental disorders. 
Psychiatrists have long known that the illnesses of patients they see 
in the clinic cannot be broken down into discrete groups in the way 
that is taught at medical school. Symptoms overlap and flow across 
diagnostic boundaries. Patients can show the signs of two or three 
disorders at the same time. Treatments are inconsistent. Outcomes 
are unpredictable. 
Science was supposed to come to the rescue. Genetics and neuroimaging studies would, all involved hoped, reveal biological signatures 
unique to each disorder, which could be used to provide consistent 
and reliable diagnoses. Instead, it seems the opposite is true. The more 
scientists look for biomarkers for specific mental disorders, the harder 
the task becomes. Scans of the DNA and brain function of patients 
show the same stubborn refusal to group by disease type. Genetic risk 
factors and dysfunction in brain regions are shared across disorders. 
Psychiatrists joke that their patients have not 
read the textbooks. The reality is serious and 
more troubling — the textbook is wrong.
The American Psychiatric Association 
routinely points out that its DSM disease 
categories are intended only as diagnostic 
tools. It does not claim that they mark genuine biological boundaries. But the system is 
set up as if they do. That might explain why 
biomarkers and new drugs for mental illness 
remain elusive. The system should change. Funders and journals must 
encourage work that cuts across the boundaries. Researchers should be 
encouraged to investigate the causes of mental illness from the bottom 
up, as the US National Institute of Mental Health is doing. The brain 
is complicated enough. Why investigate its problems with one hand 
tied behind our backs? ■
“Patients’ 
illnesses cannot 
be broken down 
into discrete 
groups in 
the way that 
is taught at 
medical school.”
ANNOUNCEMENT
Reducing our 
irreproducibility
Over the past year, Nature has published a string of articles that highlight failures in the reliability and reproducibility of published research (collected and freely available at go.nature.com/
huhbyr). The problems arise in laboratories, but journals such as 
this one compound them when they fail to exert sufficient scrutiny 
over the results that they publish, and when they do not publish 
enough information for other researchers to assess results properly.
From next month, Nature and the Nature research journals will 
introduce editorial measures to address the problem by improving 
the consistency and quality of reporting in life-sciences articles. 
To ease the interpretation and improve the reliability of published 
results we will more systematically ensure that key methodological details are reported, and we will give more space to methods 
sections. We will examine statistics more closely and encourage 
authors to be transparent, for example by including their raw data.
Central to this initiative is a checklist intended to prompt authors 
to disclose technical and statistical information in their submissions, and to encourage referees to consider aspects important for 
research reproducibility (go.nature.com/oloeip). It was developed 
after discussions with researchers on the problems that lead to 
irreproducibility, including workshops organized last year by US 
National Institutes of Health (NIH) institutes. It also draws on published concerns about reporting standards (or the lack of them) and 
the collective experience of editors at Nature journals.
The checklist is not exhaustive. It focuses on a few experimental 
and analytical design elements that are crucial for the interpretation of research results but are often reported incompletely. For 
example, authors will need to describe methodological parameters 
that can introduce bias or influence robustness, and provide precise 
characterization of key reagents that may be subject to biological 
variability, such as cell lines and antibodies. The checklist also consolidates existing policies about data deposition and presentation.
We will also demand more precise descriptions of statistics, and 
we will commission statisticians as consultants on certain papers, 
at the editor’s discretion and at the referees’ suggestion.
We recognize that there is no single way to conduct an experimental study. Exploratory investigations cannot be done with the 
same level of statistical rigour as hypothesis-testing studies. Few 
academic laboratories have the means to perform the level of validation required, for example, to translate a finding from the laboratory to the clinic. However, that should not stand in the way of a 
full report of how a study was designed, conducted and analysed 
that will allow reviewers and readers to adequately interpret and 
build on the results.
To allow authors to describe their experimental design and 
methods in as much detail as necessary, the participating journals, including Nature, will abolish space restrictions on the 
methods section.
To further increase transparency, we will encourage authors to 
provide tables of the data behind graphs and figures. This builds 
on our established data-deposition policy for specific experiments 
and large data sets. The source data will be made available directly 
from the figure legend, for easy access. We continue to encourage authors to share detailed methods and reagent descriptions 
by depositing protocols in Protocol Exchange (www.nature.com/
protocolexchange), an open resource linked from the primary paper. 
Renewed attention to reporting and transparency is a small step. 
Much bigger underlying issues contribute to the problem, and are 
beyond the reach of journals alone. Too few biologists receive adequate training in statistics and other quantitative aspects of their 
subject. Mentoring of young scientists on matters of rigour and 
transparency is inconsistent at best. In academia, the ever increasing pressures to publish and chase funds provide little incentive to 
pursue studies and publish results that contradict or confirm previous papers. Those who document the validity or irreproducibility of 
a published piece of work seldom get a welcome from journals and 
funders, even as money and effort are wasted on false assumptions.
Tackling these issues is a long-term endeavour that will require 
the commitment of funders, institutions, researchers and publishers. It is encouraging that NIH institutes have led community 
discussions on this topic and are considering their own recommendations. We urge others to take note of these and of our initiatives, 
and do whatever they can to improve research reproducibility. ■
398 | NATURE | VOL 496 | 25 APRIL 2013
THIS WEEK EDITORIALS
© 2013 Macmillan Publishers Limited. All rights reserved

