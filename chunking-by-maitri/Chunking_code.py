# -*- coding: utf-8 -*-
"""Heather_Maitri_3_1_normalChunking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EPpSwAbO-9GGl2kNBDiyfPSKdtmv3z6m
"""

# ==============================================================================
# CELL 1: INSTALL DEPENDENCIES
# ==============================================================================
!pip install -q -U langchain-community langchain-huggingface sentence-transformers faiss-cpu langchain openpyxl

# ==============================================================================
# CELL 2: IMPORTS & CONFIGURATION
# ==============================================================================
import os
import json
import pandas as pd
from google.colab import drive
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from sentence_transformers import CrossEncoder
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. Mount Google Drive
drive.mount('/content/drive')

# 2. CONFIGURATION PATHS -- UPDATE THESE IF NEEDED
BASE_DIR = "/content/drive/MyDrive"
INPUT_TEXT_DIR = os.path.join("/content/drive/MyDrive/FINAL_FOLDER/Text Extraction/text_extracted")     # Folder with .txt files
OUTPUT_RESULTS_DIR = os.path.join("/content/drive/MyDrive/FINAL_FOLDER/Maitri/retrieval_results_new") # Folder to save results

# Path to your uploaded metadata CSV
# Update this path to where your metadata file actually lives in Drive or Colab
METADATA_FILE_PATH = "/content/drive/MyDrive/FINAL_FOLDER/Text Extraction/metadata_new_final_v1.csv"

# Create output directory
os.makedirs(OUTPUT_RESULTS_DIR, exist_ok=True)

print(f"üìÇ Input Directory: {INPUT_TEXT_DIR}")
print(f"üìÇ Output Directory: {OUTPUT_RESULTS_DIR}")
print(f"üìÑ Metadata File: {METADATA_FILE_PATH}")

# ==============================================================================
# CELL 3: LOAD METADATA
# ==============================================================================
def load_metadata_map(csv_path):
    """
    Loads the CSV and creates a dictionary for fast lookups by filename.
    """
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"‚ùå Metadata file not found at: {csv_path}")

    # Read CSV
    df = pd.read_csv(csv_path)

    # Create a dictionary where key = filename, value = row as dict
    meta_map = {}
    for _, row in df.iterrows():
        # Clean the filename to ensure it matches (strip whitespace)
        fname = str(row['filename']).strip()
        meta_map[fname] = {
            "title": row['title'],
            "doi": row['doi'],
            "year": row['year'],
            "publication": row.get('publication', 'N/A')
        }

    print(f"‚úÖ Loaded metadata for {len(meta_map)} files.")
    return meta_map

# Load the map immediately
try:
    METADATA_MAP = load_metadata_map(METADATA_FILE_PATH)
except Exception as e:
    print(f"‚ö†Ô∏è Warning: Could not load metadata ({e}). Proceeding without it.")
    METADATA_MAP = {}

# ==============================================================================
# CELL 4: ROBUST TOKEN CHUNKING LOGIC
# ==============================================================================
def split_text_by_tokens(text, filename):
    """
    Splits text into fixed-size chunks based on tokens/characters.
    Looks up metadata from the global METADATA_MAP.
    """
    # 1. Get Metadata for this file
    file_meta = METADATA_MAP.get(filename.strip(), {})

    # Defaults if metadata is missing
    doc_title = file_meta.get("title", filename)
    doc_doi = file_meta.get("doi", "Unknown DOI")
    doc_year = file_meta.get("year", "Unknown Year")

    # 2. Configure the Splitter
    # chunk_size=1000 characters is roughly 250-300 words.
    # We use a large overlap (200) to ensure context isn't lost at the cut points.
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=4000,      # Large chunk size (~1000 tokens)
        chunk_overlap=500,    # Generous overlap to preserve context
        length_function=len,
        separators=["\n\n", "\n", " ", ""] # Try to split by paragraphs first
    )

    # 3. Create Documents
    raw_chunks = text_splitter.split_text(text)
    docs = []

    for i, chunk_content in enumerate(raw_chunks):
        doc = Document(
            page_content=chunk_content,
            metadata={
                "source": filename,
                "chunk_id": i,        # specific ID for this chunk
                "title": doc_title,   # <-- From CSV
                "doi": doc_doi,       # <-- From CSV
                "year": doc_year      # <-- From CSV
            }
        )
        docs.append(doc)

    return docs

# ==============================================================================
# CELL 5: BUILD VECTOR DATABASE (One-Time Process)
# ==============================================================================
print("‚è≥ Initializing Embedding Model...")
# Using all-MiniLM-L6-v2. It has a context limit of 512 tokens.
# Note: If your chunks are 4000 chars (~1000 tokens), this model will truncate
# the end of the chunk during embedding.
# To fully capture 1000+ tokens, consider using 'BAAI/bge-large-en-v1.5' or similar.
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'}
)

print("‚è≥ Processing files and building index...")
all_chunks = []

if os.path.exists(INPUT_TEXT_DIR):
    files = [f for f in os.listdir(INPUT_TEXT_DIR) if f.endswith(".txt")]

    for filename in files:
        path = os.path.join(INPUT_TEXT_DIR, filename)
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            raw_text = f.read()

        # Split and enrich with metadata
        file_chunks = split_text_by_tokens(raw_text, filename)
        all_chunks.extend(file_chunks)

    print(f"‚úÖ Processed {len(files)} files into {len(all_chunks)} chunks.")

    print("‚è≥ Building FAISS Index...")
    vector_db = FAISS.from_documents(all_chunks, embedding_model)
    print("‚úÖ Vector Database Ready!")
else:
    print(f"‚ùå Error: Input directory '{INPUT_TEXT_DIR}' does not exist.")

# ==============================================================================
# CELL 6: QUERY & SAVE LOGIC (Updated for Top 10 Papers)
# ==============================================================================
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def process_and_save_query(user_query):
    print(f"\nüîé Processing: '{user_query}'")

    # 1. Retrieve (Broad Search - Get top 50 candidates)
    initial_docs = vector_db.similarity_search(user_query, k=50)

    # 2. Rerank (Precision Sorting)
    pairs = [[user_query, doc.page_content] for doc in initial_docs]
    scores = reranker.predict(pairs)

    # Sort by score (High to Low)
    ranked_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)

    # 3. Output 1: Top 10 Chunks (Specific Content)
    top_chunks_data = []
    for doc, score in ranked_docs[:10]:
        top_chunks_data.append({
            "score": float(score),
            "text": doc.page_content,
            "chunk_id": doc.metadata.get("chunk_id"),
            "paper_title": doc.metadata.get("title"),
            "doi": doc.metadata.get("doi"),
            "year": doc.metadata.get("year"),
            "filename": doc.metadata.get("source")
        })

    # 4. Output 2: Top 10 Unique Papers (Sources)
    top_papers_data = []
    seen_titles = set()

    for doc, score in ranked_docs:
        title = doc.metadata.get("title", "Unknown")
        # Only add if we haven't seen this paper yet
        if title not in seen_titles:
            top_papers_data.append({
                "rank": len(top_papers_data) + 1,
                "title": title,
                "doi": doc.metadata.get("doi"),
                "year": doc.metadata.get("year"),
                "best_chunk_score": float(score),
                "filename": doc.metadata.get("source")
            })
            seen_titles.add(title)

        # Stop once we have 10 papers
        if len(top_papers_data) >= 10:
            break

    # 5. Save to Drive
    safe_name = "".join([c if c.isalnum() else "_" for c in user_query])[:50]

    chunks_file = os.path.join(OUTPUT_RESULTS_DIR, f"CHUNKS_{safe_name}.json")
    papers_file = os.path.join(OUTPUT_RESULTS_DIR, f"PAPERS_{safe_name}.json")

    with open(chunks_file, 'w') as f:
        json.dump(top_chunks_data, f, indent=4)

    with open(papers_file, 'w') as f:
        json.dump(top_papers_data, f, indent=4)

    print(f"‚úÖ Results saved to:\n   - {chunks_file}\n   - {papers_file}")
    return top_chunks_data, top_papers_data

# ==============================================================================
# CELL 7: RUN IT
# ==============================================================================
query = input("Enter your research question: ")
if query:
    chunks, papers = process_and_save_query(query)

    print("\n" + "="*50)
    print(f"TOP 10 RELEVANT PAPERS FOR: '{query}'")
    print("="*50)

    for p in papers:
        print(f"#{p['rank']} [Score: {p['best_chunk_score']:.4f}]")
        print(f"   Title: {p['title']}")
        print(f"   DOI:   {p['doi']}")
        print("-" * 50)

# ==============================================================================
# CELL 7: RUN IT
# ==============================================================================
query = input("Enter your research question: ")
if query:
    chunks, papers = process_and_save_query(query)

    print("\n" + "="*50)
    print(f"TOP 10 RELEVANT PAPERS FOR: '{query}'")
    print("="*50)

    for p in papers:
        print(f"#{p['rank']} [Score: {p['best_chunk_score']:.4f}]")
        print(f"   Title: {p['title']}")
        print(f"   DOI:   {p['doi']}")
        print("-" * 50)

# ==============================================================================
# CELL 7: RUN IT
# ==============================================================================
query = input("Enter your research question: ")
if query:
    chunks, papers = process_and_save_query(query)

    print("\n" + "="*50)
    print(f"TOP 10 RELEVANT PAPERS FOR: '{query}'")
    print("="*50)

    for p in papers:
        print(f"#{p['rank']} [Score: {p['best_chunk_score']:.4f}]")
        print(f"   Title: {p['title']}")
        print(f"   DOI:   {p['doi']}")
        print("-" * 50)

# ==============================================================================
# CELL 7: RUN IT
# ==============================================================================
query = input("Enter your research question: ")
if query:
    chunks, papers = process_and_save_query(query)

    print("\n" + "="*50)
    print(f"TOP 10 RELEVANT PAPERS FOR: '{query}'")
    print("="*50)

    for p in papers:
        print(f"#{p['rank']} [Score: {p['best_chunk_score']:.4f}]")
        print(f"   Title: {p['title']}")
        print(f"   DOI:   {p['doi']}")
        print("-" * 50)

# ==============================================================================
# CELL 7: RUN IT
# ==============================================================================
query = input("Enter your research question: ")
if query:
    chunks, papers = process_and_save_query(query)

    print("\n" + "="*50)
    print(f"TOP 10 RELEVANT PAPERS FOR: '{query}'")
    print("="*50)

    for p in papers:
        print(f"#{p['rank']} [Score: {p['best_chunk_score']:.4f}]")
        print(f"   Title: {p['title']}")
        print(f"   DOI:   {p['doi']}")
        print("-" * 50)

# ==============================================================================
# CELL 7: RUN IT
# ==============================================================================
query = input("Enter your research question: ")
if query:
    chunks, papers = process_and_save_query(query)

    print("\n" + "="*50)
    print(f"TOP 10 RELEVANT PAPERS FOR: '{query}'")
    print("="*50)

    for p in papers:
        print(f"#{p['rank']} [Score: {p['best_chunk_score']:.4f}]")
        print(f"   Title: {p['title']}")
        print(f"   DOI:   {p['doi']}")
        print("-" * 50)

# ==============================================================================
# CELL 7: RUN IT
# ==============================================================================
query = input("Enter your research question: ")
if query:
    chunks, papers = process_and_save_query(query)

    print("\n" + "="*50)
    print(f"TOP 10 RELEVANT PAPERS FOR: '{query}'")
    print("="*50)

    for p in papers:
        print(f"#{p['rank']} [Score: {p['best_chunk_score']:.4f}]")
        print(f"   Title: {p['title']}")
        print(f"   DOI:   {p['doi']}")
        print("-" * 50)

# ==============================================================================
# CELL 7: RUN IT
# ==============================================================================
query = input("Enter your research question: ")
if query:
    chunks, papers = process_and_save_query(query)

    print("\n" + "="*50)
    print(f"TOP 10 RELEVANT PAPERS FOR: '{query}'")
    print("="*50)

    for p in papers:
        print(f"#{p['rank']} [Score: {p['best_chunk_score']:.4f}]")
        print(f"   Title: {p['title']}")
        print(f"   DOI:   {p['doi']}")
        print("-" * 50)

